{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3e693-b9e9-4d3a-8583-e15f040e6252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import jsonlines\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import fileinput\n",
    "import numpy as np\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud\n",
    "from textblob_de import TextBlobDE as TextBlob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "from datetime import date\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a8e91d-6e0b-42d7-b340-141f503131fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Dataset\n",
    "olafScholzJsonLines = jsonlines.open(\"OlafScholz.jl\")\n",
    "\n",
    "olafScholzTwitter = pd.read_json(\"OlafScholz.jl\", lines = True)\n",
    "print(\"Datensatzlänge: \" + str(olafScholzTwitter.info()))\n",
    "\n",
    "print(olafScholzTwitter.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed117af7-5b00-4ff1-970b-2e6eb60fa65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reply Dataframe\n",
    "# Importing Data into final Dataframe for ML\n",
    "replyDataFrame = pd.DataFrame(columns = [\"tweetid\", \"date\", \"time\", \"user\", \"text\"])\n",
    "print(replyDataFrame.head())\n",
    "iterator = 0\n",
    "\n",
    "for line in olafScholzJsonLines:\n",
    "    keyResponse = line[\"response\"]\n",
    "    data = keyResponse[\"data\"]\n",
    "    newDataRow = [None, None, None, None, None]\n",
    "    userName = line[\"account_name\"]\n",
    "    newDataRow[4] = userName\n",
    "    keyResponse = line[\"response\"]\n",
    "    data = keyResponse[\"data\"]\n",
    "\n",
    "\n",
    "    for tweet in data:\n",
    "\n",
    "        tweetTarget = 1\n",
    "       \n",
    "        tweetID = tweet[\"id\"]\n",
    "        authorID = tweet[\"author_id\"]\n",
    "        tweetDate = tweet[\"created_at\"][0:10]\n",
    "        tweetTime = tweet[\"created_at\"][11:19]\n",
    "        tweetText = tweet[\"text\"]        \n",
    "        newDataRow[0] = tweetID\n",
    "        newDataRow[1] = tweetDate\n",
    "        newDataRow[2] = tweetTime\n",
    "        newDataRow[3] = authorID\n",
    "        newDataRow[4] = tweetText\n",
    "        replyDataFrame.loc[len(replyDataFrame)] = newDataRow\n",
    "    iterator += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c9a78f-de0b-451a-bd3a-f2297741dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dir(replyDataFrame))\n",
    "print(len(replyDataFrame))\n",
    "print(replyDataFrame.shape)\n",
    "print(replyDataFrame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4692e308-20ee-4244-958c-eb55099d86b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----\")\n",
    "print(replyDataFrame[replyDataFrame[\"text\"].str.contains(\"RT @Bundeskanzler:\")].sort_values(\"date\",ascending = \"true\").head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f26ce0b-1710-4891-b1a1-cd09136caa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "print(replyDataFrame[\"text\"].str.contains(\"RT @Bundeskanzler\").head())\n",
    "print(replyDataFrame[\"text\"].str.contains(\"RT @Bundeskanzler\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aecd9a0-d7bc-4a37-98fa-b8ab6f6ac397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleansedTweetDF = replyDataFrame[replyDataFrame[\"text\"].str.contains(\"RT @Bundeskanzler:\") or replyDataFrame[\"text\"].str.contains(\"!RT @\") ]\n",
    "print(\"Size of dataframe :\\n\"+ str(replyDataFrame.count())+\"\\n----\")\n",
    "maskBundesKanz = replyDataFrame[\"text\"].str.contains(\"RT @Bundeskanzler:\")\n",
    "maskNoRT = replyDataFrame[\"text\"].str.contains(\"RT @\")==False\n",
    "print(\"Size of dataframe with RT @Bundeskanzler:\\n\" + str(replyDataFrame[maskBundesKanz].count())+\"\\n----\")\n",
    "print(\"Size of dataframe without RT @:\\n\" + str(replyDataFrame[maskNoRT].count())+\"\\n----\")\n",
    "#cleansedTweetDF contains all Tweets posted directly by Olaf before or after being Kanzler\n",
    "cleansedTweetDF = replyDataFrame[maskBundesKanz | maskNoRT]\n",
    "print(\"Size of cleansedTweetDF @:\\n\" + str(cleansedTweetDF.count())+\"\\n----\")\n",
    "print(cleansedTweetDF.head())\n",
    "#cleansedTweetDF has \"RT @Bundeskanzler:\" still in the tweet. To not have any issues with ML we removed that part of the Tweet in the following parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d67e75-e4ee-4015-bb5b-fcdf7bd123d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the Text \\\"RT @Bundeskanzler:\\\" from the text\\n\n",
    "print(\"ReplyDataframe: \\n\" + str(replyDataFrame[maskBundesKanz].count()))\n",
    "cleansedAtBundKanzDF = replyDataFrame[maskBundesKanz].copy()\n",
    "cleansedAtBundKanzDF.text = cleansedAtBundKanzDF.text.str.strip(\"RT @Bundeskanzler: \")\n",
    "print(cleansedAtBundKanzDF.head())                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb503cc-0edb-4313-9da9-fb3d54f534b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending the sliced versions to the Dataframe and resulting in the expected 4119 Tweets\n",
    "cleansedTweetDFShort = replyDataFrame[maskNoRT].append(cleansedAtBundKanzDF)\n",
    "print(cleansedTweetDFShort.count())\n",
    "print(cleansedTweetDFShort.sort_values(\"date\",ascending = False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tobias 29.05.22 and 30.05.22\n",
    "\n",
    "# Starting preparing for machine learning\n",
    "\n",
    "# Making statement text in lower case\n",
    "\n",
    "cleansedTweetDFShort\n",
    "\n",
    "#Copy for emotion analysis\n",
    "\n",
    "def clean(text):\n",
    "    cleansedTweetDFShort['text'] = cleansedTweetDFShort['text'].str.replace('@[A-Za-z0-9]+\\s?', '', regex=True)\n",
    "    text = re.sub(r'@[A-Za-z0-9]+\\s?', '', text) #Removed Mentions\n",
    "    text = re.sub(r'#', '', text) #Removed #\n",
    "    text = re.sub(r'(.)1+', r'1', text) #cleaned single letters\n",
    "    text = re.sub('((www.[^s]+)|(https?://[^s]+))','',text) #Removes links\n",
    "    text = re.sub('@','',text) #Remove @\n",
    "    text = re.sub('ä','ae',text) #Remove ä\n",
    "    text = re.sub('Ä','Ae',text) #Remove Ä\n",
    "    text = re.sub('ö','oe',text) #Remove Ä\n",
    "    text = re.sub('Ö','Oe',text) #Remove Ä\n",
    "    text = re.sub('ü','ue',text) #Remove Ä\n",
    "    text = re.sub('Ü','Ue',text) #Remove Ä\n",
    "    return text\n",
    "\n",
    "cleansedTweetDFShort['text'] = cleansedTweetDFShort['text'].apply(clean)\n",
    "\n",
    "cleansedTweetDFShort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fdbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansedTweetDFShort = cleansedTweetDFShort.astype({'text':'string'})\n",
    "print(cleansedTweetDFShort.dtypes)\n",
    "cleansedTweetDFShort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4239fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra DF for wordcloud\n",
    "\n",
    "cleansedTweetDFShort10 = cleansedTweetDFShort[:100].copy()\n",
    "cleansedTweetDFShort10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ec2fc-e88e-491c-85ca-143b97f84f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Polarity and Subjectivity\n",
    "\n",
    "# Subjectivity quantifies the amount of personal opinion and factual information contained in the text. \n",
    "# The higher subjectivity means that the text contains personal opinion rather than factual information. \n",
    "# 0 means low personal opinion and 1 a lot of personal opinion\n",
    "\n",
    "# If this code failes due to missing download, uncomment following line\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "def getS (text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Polarity lies between [-1,1], -1 defines a negative sentiment and 1 defines a positive sentiment.\n",
    "\n",
    "def getP (text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "\n",
    "cleansedTweetDFShort10['Personal Opinion (Subjectivity)'] = cleansedTweetDFShort10['text'].apply(getS)\n",
    "cleansedTweetDFShort10['Sentiment (Polarity)'] = cleansedTweetDFShort10['text'].apply(getP)\n",
    "\n",
    "cleansedTweetDFShort10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc6433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Cloud not cleaned\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "allWords = ' '.join([txt for txt in cleansedTweetDFShort10['text']])\n",
    "wordCloud = WordCloud(max_words=500, width=2000, height = 1000, \n",
    "                      max_font_size = 1000, collocations=False).generate(allWords)\n",
    "\n",
    "plt.imshow(wordCloud, interpolation = \"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e00b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining set containing all stopwords in German. (Stopwords: Word with no relevance for analysis)\n",
    "# Source for Stopwordlist: https://github.com/stopwords-iso/stopwords-de/blob/master/stopwords-de.json\n",
    "# Cleaning text from stopwords\n",
    "\n",
    "# Create copy to work with\n",
    "cleansedTweetDFShort10copy = cleansedTweetDFShort10.copy()\n",
    "\n",
    "# Making statement text in lower case\n",
    "cleansedTweetDFShort10copy['text']=cleansedTweetDFShort10copy['text'].str.lower()\n",
    "\n",
    "with open('stopwords-de.json','r') as file:\n",
    "    stopwordlistgerman = json.load(file)\n",
    "\n",
    "STOPWORDS = set(stopwordlistgerman)\n",
    "\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "cleansedTweetDFShort10copy['text'] = cleansedTweetDFShort10copy['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "cleansedTweetDFShort10copy['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning text from stopwords from nltk.corpus\n",
    "# If this code failes due to missing download, uncomment following line\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPLISTnltk = set(stopwords.words('german'))\n",
    "\n",
    "def clean_tweet(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPLISTnltk])\n",
    "\n",
    "cleansedTweetDFShort10copy['text'] = cleansedTweetDFShort10copy['text'].apply(lambda text: clean_tweet(text))\n",
    "cleansedTweetDFShort10copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58708884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning text from english stopwords\n",
    "# Source for Stopwordlist: https://countwordsfree.com/stopwords\n",
    "# Cleaning text from stopwords\n",
    "with open('stop_words_english.json','r',encoding='utf-8') as file:\n",
    "    stopwordlistenglish = json.load(file)\n",
    "\n",
    "STOPWORDS = set(stopwordlistenglish)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "cleansedTweetDFShort10copy['text'] = cleansedTweetDFShort10copy['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "cleansedTweetDFShort10copy['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctations\n",
    "\n",
    "#import string\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "cleansedTweetDFShort10copy['text']= cleansedTweetDFShort10copy['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "cleansedTweetDFShort10copy['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eedef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "porter = PorterStemmer()\n",
    "\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "cleansedTweetDFShort10copy['text']= cleansedTweetDFShort10copy['text'].apply(lambda x: stemming_on_text(x))\n",
    "cleansedTweetDFShort10copy = cleansedTweetDFShort10copy.astype({'text':'string'})\n",
    "cleansedTweetDFShort10copy['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The NLTK lemmatizer and stemmer classes\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('german')\n",
    "\n",
    "# I use the POS tagging from NLTK to retain only adjectives, verbs, adverbs \n",
    "# and nouns as a base for for lemmatization.\n",
    "def get_lemmas(tweet): \n",
    "    \n",
    "    # A dictionary to help convert Treebank tags to WordNet\n",
    "    treebank2wordnet = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r'}\n",
    "    \n",
    "    postag = ''\n",
    "    lemmas_list = []\n",
    "    \n",
    "    for word, tag in pos_tag(word_tokenize(tweet)):\n",
    "        if tag.startswith(\"JJ\")     \\\n",
    "            or tag.startswith(\"RB\") \\\n",
    "            or tag.startswith(\"VB\") \\\n",
    "            or tag.startswith(\"NN\"):\n",
    "                \n",
    "            try:\n",
    "                postag = treebank2wordnet[tag[:2]]\n",
    "            except:\n",
    "                postag = 'n'                \n",
    "                            \n",
    "            lemmas_list.append(lemmatizer.lemmatize(word.lower(), postag))    \n",
    "    \n",
    "    return lemmas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e694be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts the lemmatization process\n",
    "def get_lemmatized(tweet):\n",
    "   \n",
    "    all_tokens_string = ''\n",
    "    filtered = []\n",
    "    tokens = []\n",
    "\n",
    "    # lemmatize\n",
    "    tokens = [token for token in get_lemmas(tweet)]\n",
    "    \n",
    "    # filter\n",
    "    filtered = clean_tweet(tokens)\n",
    "\n",
    "    # join everything into a single string\n",
    "    all_tokens_string = ''.join(filtered)\n",
    "    \n",
    "    return all_tokens_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb771ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the lemmatized tweets and puts the result in an \"edited\" text column\n",
    "# for future use in this script\n",
    "\n",
    "# If this code failes due to missing download, uncomment following line\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "edited = ''\n",
    "for i, row in cleansedTweetDFShort10copy.iterrows():\n",
    "    edited = get_lemmatized(cleansedTweetDFShort10.loc[i]['text'])\n",
    "    if len(edited) > 0:\n",
    "        cleansedTweetDFShort10copy.at[i,'edited'] = edited\n",
    "    else:\n",
    "        cleansedTweetDFShort10copy.at[i,'edited'] = None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cdb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After lemmatization, some tweets may end up with the same words\n",
    "# Let's make sure that we have no duplicates\n",
    "cleansedTweetDFShort10copy.drop_duplicates(subset=['edited'], inplace=True)\n",
    "cleansedTweetDFShort10copy.dropna(inplace=True)\n",
    "cleansedTweetDFShort10copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a371fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column with the number of words in each tweet\n",
    "cleansedTweetDFShort10copy['word_count'] = cleansedTweetDFShort10copy.apply(lambda x: len(x['text'].split()),axis=1)\n",
    "t = pd.DataFrame(cleansedTweetDFShort10copy['word_count'].describe())\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current dataframe\n",
    "\n",
    "cleansedTweetDFShort10copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b3dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Cloud after cleaning\n",
    "plt.figure(figsize = (20,20))\n",
    "cleanedwords = ' '.join([txt for txt in cleansedTweetDFShort10copy['text']])\n",
    "wordCloud = WordCloud(max_words = 500, width=2000, height = 1000, max_font_size = 1000, \n",
    "                      collocations=False).generate(cleanedwords)\n",
    "\n",
    "plt.imshow(wordCloud, interpolation = \"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis with TextBlob \n",
    "\n",
    "# Create a function to compute analysis\n",
    "\n",
    "def getA(score):\n",
    "    if score < 0:\n",
    "        return 'negative'\n",
    "    elif score == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "cleansedTweetDFShort10['Analysis'] = cleansedTweetDFShort10['Sentiment (Polarity)'].apply(getA)\n",
    "cleansedTweetDFShort10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca05e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for Visualization\n",
    "\n",
    "sortedDf = cleansedTweetDFShort10.sort_values(by=['Sentiment (Polarity)'])\n",
    "\n",
    "sortedDf = sortedDf.astype({'Analysis':'string'})\n",
    "print(sortedDf.dtypes)\n",
    "sortedDf.reset_index(inplace=True, drop=True) # Reset index\n",
    "sortedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24390f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show texts with Polarity and Analysis \n",
    "\n",
    "# Problem erkannt: Er schreibt sowohl englisch als auch deutsch\n",
    "\n",
    "sortedDf\n",
    "\n",
    "for i in range (0, sortedDf.shape[0]):\n",
    "    print(sortedDf['text'][i])\n",
    "    print(sortedDf['Personal Opinion (Subjectivity)'][i])\n",
    "    print(sortedDf['Sentiment (Polarity)'][i])\n",
    "    print(sortedDf['Analysis'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Positive tweets\n",
    "\n",
    "j=1;\n",
    "\n",
    "for i in range(0, sortedDf.shape[0]):\n",
    "    if(sortedDf['Analysis'][i] == 'positive'):\n",
    "        print(str(j) + ') ' + sortedDf['text'][i])\n",
    "        print(sortedDf['Personal Opinion (Subjectivity)'][i])\n",
    "        print(sortedDf['Sentiment (Polarity)'][i])\n",
    "        print(sortedDf['Analysis'][i])\n",
    "        print()\n",
    "        j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81723a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Negative tweets\n",
    "\n",
    "j=1;\n",
    "\n",
    "for i in range(0, sortedDf.shape[0]):\n",
    "    if(sortedDf['Analysis'][i] == 'negative'):\n",
    "        print(str(j) + ') ' + sortedDf['text'][i])\n",
    "        print(sortedDf['Personal Opinion (Subjectivity)'][i])\n",
    "        print(sortedDf['Sentiment (Polarity)'][i])\n",
    "        print(sortedDf['Analysis'][i])\n",
    "        print()\n",
    "        j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot polarity and subjectivity\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "for i in range (0, sortedDf.shape[0]):\n",
    "    plt.scatter(sortedDf['Sentiment (Polarity)'][i], sortedDf['Personal Opinion (Subjectivity)'][i], color='Green')\n",
    "                \n",
    "plt.title('Sentiment Analysis')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Subjectivity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc395ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the percentage of positive tweets\n",
    "\n",
    "ptweets = sortedDf[sortedDf.Analysis ==\"positive\"] \n",
    "\n",
    "print(\"Positive Tweets in percent: \"+str(round((ptweets.shape[0]/sortedDf.shape[0])*100, 1)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the negative of positive tweets\n",
    "\n",
    "neutweets = sortedDf[sortedDf.Analysis ==\"neutral\"] \n",
    "\n",
    "print(\"Neutral Tweets in percent: \"+str(round((neutweets.shape[0]/sortedDf.shape[0])*100, 1)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d786b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the negative of positive tweets\n",
    "\n",
    "ntweets = sortedDf[sortedDf.Analysis ==\"negative\"] \n",
    "\n",
    "print(\"Negative Tweets in percent: \"+str(round((ntweets.shape[0]/sortedDf.shape[0])*100, 1)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde86c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Analysis\n",
    "\n",
    "sortedDf['Analysis'].value_counts()\n",
    "\n",
    "plt.title('Sentiment Analysis in %')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Counts')\n",
    "sortedDf['Analysis'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16079e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid repetitions in our code, here are some plotting functions which will be used often\n",
    "\n",
    "def plot_sentiment_period(df, info):\n",
    "    \n",
    "    # Using the mean values of sentiment for each period\n",
    "    df1 = df.groupby(df['datetime'].dt.to_period(info['period'])).mean()\n",
    "\n",
    "    df1.reset_index(inplace=True)\n",
    "    df1['datetime'] = pd.PeriodIndex(df1['datetime']).to_timestamp()\n",
    "    plot_df = pd.DataFrame(df1, df1.index, info['cols'])\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    ax = sns.lineplot(data=plot_df, linewidth = 3, dashes = False)\n",
    "    plt.legend(loc='best', fontsize=15)\n",
    "    plt.title(info['title'], fontsize=20)\n",
    "    plt.xlabel(info['xlab'], fontsize=15)\n",
    "    plt.ylabel(info['ylab'], fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('images/' + info['fname'])    \n",
    "    return\n",
    "\n",
    "\n",
    "def plot_fractions(props, title, fname):\n",
    "    \n",
    "    plt1 = props.plot(kind='bar', stacked=False, figsize=(16,5), colormap='Spectral') \n",
    "    plt.legend(bbox_to_anchor=(1.005, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('Online storm', fontweight='bold', fontsize=18)\n",
    "    plt.xticks(rotation=0,fontsize=14)\n",
    "    #plt.ylim(0, 0.5)\n",
    "    plt.ylabel('Fraction of Tweets', fontweight='bold', fontsize=18)\n",
    "    plt1.set_title(label=title, fontweight='bold', size=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/' + fname + '.png')\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def plot_frequency_chart(info):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    sns.set_context(\"notebook\", font_scale=1)    \n",
    "    ax = sns.barplot(x=info['x'], y=info['y'], data=info['data'], palette=(info['pal']))\n",
    "    ax.set_title(label=info['title'], fontweight='bold', size=18)\n",
    "    plt.ylabel(info['ylab'], fontsize=16)\n",
    "    plt.xlabel(info['xlab'], fontsize=16)\n",
    "    plt.xticks(rotation=info['angle'],fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('images/' + info['fname'])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting after values descending\n",
    "\n",
    "cleansedTweetDFShort10\n",
    "print(cleansedTweetDFShort10.sort_values(\"date\",ascending = False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c86ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sortedDfdiagram = cleansedTweetDFShort10.copy()\n",
    "# filter for these dates\n",
    "sortedDfdiagram.set_index('date',inplace=True)\n",
    "sortedDfdiagram=sortedDfdiagram[(sortedDfdiagram.index<='2021-11-11') & (sortedDfdiagram.index>='2021-11-08')]\n",
    "sortedDfdiagram.plot(figsize=(12,6));\n",
    "plt.ylabel('Score', fontsize=15)\n",
    "plt.xlabel('Dates', fontsize=15)\n",
    "plt.legend().set_visible(True)\n",
    "plt.title('Subjectivity and Polarity of tweets in Df', fontsize=20)\n",
    "plt.tight_layout()\n",
    "sns.despine(top=True)  \n",
    "plt.show()\n",
    "sortedDfdiagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b54719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency\n",
    "\n",
    "# We need these imports for the wordcloud representation:\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "#from matplotlib.colors import makeMappingArray\n",
    "#from palettable.colorbrewer.diverging import Spectral_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b422d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter    # Counts the most common items in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36afab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_edited_string(edited_tweets):\n",
    "    \n",
    "    edited_string = ''\n",
    "    for row in edited_tweets:\n",
    "        edited_string = edited_string + ' ' + row\n",
    "        \n",
    "    return edited_string\n",
    "\n",
    "def get_trigrams(trigrams, top_grams):\n",
    "    \n",
    "    grams_str = []\n",
    "    data = []\n",
    "\n",
    "    gram_counter = Counter(trigrams)\n",
    "    \n",
    "    for grams in gram_counter.most_common(10):\n",
    "        gram = ''\n",
    "        grams_str = grams[0]\n",
    "        grams_str_count = []\n",
    "        for n in range(0,3):\n",
    "            gram = gram + grams_str[n] + ' '\n",
    "        grams_str_count.append(gram)\n",
    "        grams_str_count.append(grams[1])\n",
    "        data.append(grams_str_count)\n",
    "        print(grams_str_count)\n",
    "\n",
    "    df = pd.DataFrame(data, columns = ['Grams', 'Count'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all the edited tweets in one single string\n",
    "joined_string = join_edited_string(cleansedTweetDFShort10copy['edited'])\n",
    "\n",
    "# Get tokens\n",
    "tokens = joined_string.split(' ')\n",
    "\n",
    "# get trigrams\n",
    "trigrams = nltk.trigrams(tokens)\n",
    "\n",
    "# plot word frequency during online storm\n",
    "word_counter = Counter(tokens)\n",
    "df_counter = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\n",
    "info = {'data': df_counter, 'x': 'freq', 'y': 'word',\n",
    "       'xlab': 'Count', 'ylab': 'Words', 'pal':'viridis',\n",
    "       'title': 'Most frequent words',\n",
    "       'fname':'word_frequency_before_onlinestorm.png',\n",
    "       'angle': 90}\n",
    "plot_frequency_chart(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14beb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot trigram frequency\n",
    "df_trigrams = get_trigrams(trigrams, 10)\n",
    "info = {'data': df_trigrams, 'x': 'Grams', 'y': 'Count',\n",
    "       'xlab': 'Trigrams', 'ylab': 'Count', 'pal':'viridis',\n",
    "       'title': 'Most frequent trigrams before online storm',\n",
    "       'fname':'trigrams_frequency_before_onlinestorm.png',\n",
    "       'angle': 40}\n",
    "plot_frequency_chart(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDATopics extraction\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_keys(topic_matrix):\n",
    "    '''\n",
    "    returns an integer list of predicted topic \n",
    "    categories for a given topic matrix\n",
    "    '''\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n",
    "\n",
    "def keys_to_counts(keys):\n",
    "    '''\n",
    "    returns a tuple of topic categories and their \n",
    "    accompanying magnitudes for a given list of keys\n",
    "    '''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)\n",
    "\n",
    "def get_top_n_words(n, n_topics, keys, document_term_matrix, tfidf_vectorizer):\n",
    "    '''\n",
    "    returns a list of n_topic strings, where each string contains the n most common \n",
    "    words in a predicted category, in order\n",
    "    '''\n",
    "    top_word_indices = []\n",
    "    for topic in range(n_topics):\n",
    "        temp_vector_sum = 0\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:, index] = 1\n",
    "            the_word = tfidf_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            try:\n",
    "                topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
    "            except:\n",
    "                pass\n",
    "        top_words.append(\", \".join(topic_words))         \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05531792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA topics\n",
    "def get_topics(edited, n_topics, n_words):\n",
    "\n",
    "    eds = edited.values\n",
    "    \n",
    "    vec = TfidfVectorizer(use_idf=True, smooth_idf=True)\n",
    "    document_term_matrix = vec.fit_transform(eds)\n",
    "    \n",
    "    model = LatentDirichletAllocation(n_components=n_topics)\n",
    "    topic_matrix = model.fit_transform(document_term_matrix)\n",
    "    \n",
    "    keys = get_keys(topic_matrix)\n",
    "    categories, counts = keys_to_counts(keys)\n",
    "    top_n_words = get_top_n_words(n_words, n_topics, keys, document_term_matrix, vec)\n",
    "\n",
    "    topics = ['Topic {}: \\n'.format(i + 1) + top_n_words[i] for i in categories]\n",
    "    data=[]\n",
    "    for i, topic in enumerate(topics):\n",
    "        tmp = []\n",
    "        tmp.append(topic)\n",
    "        tmp.append(counts[i])\n",
    "        data.append(tmp)\n",
    "    df_topics = pd.DataFrame(data, columns = ['Topics', 'Count'])\n",
    "    \n",
    "    return df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the tweets of the 6 years before the online storm\n",
    "\n",
    "\n",
    "# LDA topics\n",
    "df_topics = get_topics(cleansedTweetDFShort10copy['edited'], 5, 5)\n",
    "info = {'data': df_topics, 'x': 'Topics', 'y': 'Count',\n",
    "       'xlab': 'Topics', 'ylab': 'Count', 'pal':'viridis',\n",
    "       'title': 'LDA Topics',\n",
    "       'fname':'LDA_Topics.png',\n",
    "       'angle': 40}\n",
    "plot_frequency_chart(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment and Emotion Analysis\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "emotionDF = cleansedTweetDFShort10copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translate word using Google translator for Emotionanalysis\n",
    "\n",
    "#import googletrans\n",
    "#from googletrans import Translator\n",
    "\n",
    "#translater = Translator() #German: 'de'\n",
    "\n",
    "#print(googletrans.LANGUAGES) \n",
    "\n",
    "#Test\n",
    "#translator = Translator()\n",
    "#translation = translator.translate(\"Der Himmel ist blau und ich mag Bananen\", dest='en')\n",
    "#print(translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tryed to translate each tweets which was no good idea\n",
    "\n",
    "#emotiontest = emotionDF[:3].copy()\n",
    "#emotiontest['edited'] = emotiontest['edited'].astype(str)\n",
    "#emotiontest['translated'] = emotiontest['edited'].apply(translater.translate, src =\"de\", dest='en').apply(getattr, args=('text',))\n",
    "#emotiontest\n",
    "# Problem: Number of translations is limited\n",
    "\n",
    "# Translate the english row into german with google did not work\n",
    "\n",
    "#ncr10 = ncr[:100].copy()\n",
    "#ncr10.info()\n",
    "#ncr10['Deutsch'] = ncr10['English'].apply(translater.translate, src =\"en\", dest='de').apply(getattr, args=('text',))\n",
    "#ncr10['Deutsch']\n",
    "#ncr10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotionanalysis\n",
    "\n",
    "#import termcolor\n",
    "#import sys\n",
    "#from termcolor import colored, cprint\n",
    "\n",
    "# Importing the data from the NCR lexicon\n",
    "#ncr = pd.read_csv('NCR-lexicon.csv', sep =';')\n",
    "#ncr = ncr.astype({'English':'string'})\n",
    "#ncr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AB HIER PHILIP\n",
    "\n",
    "# Translate the english row into german by using the Google Translator for translation\n",
    "\n",
    "# Importing the data from the NCR lexicon\n",
    "lexicon = pd.read_csv('NCR-lexicon-german-cleaned.csv', header= 0,\n",
    "                        encoding= 'unicode_escape', sep =';')\n",
    "# Drop last columns\n",
    "lexicon = lexicon.iloc[: 10, :12]\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb111b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotionDf100 = emotionDF[:100].copy()\n",
    "\n",
    "# Let's create a list of the emotions\n",
    "emotions = ['Anger', 'Anticipation','Disgust','Fear', 'Joy','Sadness', 'Surprise', 'Trust']\n",
    "\n",
    "def join_edited_string(edited_tweets):\n",
    "    \n",
    "    edited_string = ''\n",
    "    for row in edited_tweets:\n",
    "        edited_string = edited_string + ' ' + row\n",
    "        \n",
    "    return edited_string\n",
    "\n",
    "# Join all the edited tweets in one single string\n",
    "joined_string = join_edited_string(emotionDf100['edited'])\n",
    "\n",
    "# Get tokens\n",
    "tokens = joined_string.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb0ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set(tokens)\n",
    "\n",
    "word_to_ind = dict((word, i) for i, word in enumerate(unique_words))\n",
    "ind_to_word = dict((i, word) for i, word in enumerate(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emotions_period(df, cols, title, fname, period = 'h' ):\n",
    "\n",
    "    df1 = emotionDf100.groupby(emotionDf100['date'].dt.to_period(period)).mean()\n",
    "\n",
    "    df1.reset_index(inplace=True)\n",
    "    df1['datetime'] = pd.PeriodIndex(df1['datetime']).to_timestamp()\n",
    "    plot_df = pd.DataFrame(df1, df1.index, cols)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    ax = sns.lineplot(data=plot_df, linewidth = 3,dashes = False)\n",
    "    plt.legend(loc='best', fontsize=15)\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.xlabel('Time (hours)', fontsize=15)\n",
    "    plt.ylabel('Z-scored Emotions', fontsize=15)\n",
    "    plt.savefig('images/'+ fname  + '.png')       \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9dc857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tweet_emotions(df, emotions, col):\n",
    "\n",
    "    df_tweets = emotionDf100.copy()\n",
    "    #df_tweets.drop(['sentiment','sentiment_intensity'], axis=1, inplace=True)\n",
    "    \n",
    "    emo_info = {'emotion':'' , 'emo_frq': defaultdict(int) }    \n",
    "\n",
    "    list_emotion_counts = []\n",
    "\n",
    "    # creating a dictionary list to hold the frequency of the words\n",
    "    # contributing to the emotions\n",
    "    for emotion in emotions:\n",
    "        emo_info = {}\n",
    "        emo_info['emotion'] = emotion\n",
    "        emo_info['emo_frq'] = defaultdict(int)\n",
    "        list_emotion_counts.append(emo_info)\n",
    "    \n",
    "    # bulding a zeros matrix to hold the emotions data\n",
    "    df_emotions = pd.DataFrame(0, index=emotionDf100.index, columns=emotions)\n",
    "\n",
    "    # stemming the word to facilitate the search in NRC\n",
    "    stemmer = SnowballStemmer(\"german\")\n",
    "    \n",
    "    # iterating in the tweets data set\n",
    "    for i, row in df_tweets.iterrows(): # for each tweet ...\n",
    "        tweet = word_tokenize(df_tweets.loc[i][col])\n",
    "        for word in tweet: # for each word ...\n",
    "            word_stemmed = stemmer.stem(word.lower())\n",
    "            # check if the word is in NRC\n",
    "            result = lexicon[lexicon.English == word_stemmed]\n",
    "            # we have a match\n",
    "            if not result.empty:\n",
    "                # update the tweet-emotions counts\n",
    "                for idx, emotion in enumerate(emotions):\n",
    "                    df_emotions.at[i, emotion] += result[emotion]\n",
    "                    \n",
    "                    # update the frequencies dictionary list\n",
    "                    if result[emotion].any():\n",
    "                        try:\n",
    "                            list_emotion_counts[idx]['emo_frq'][word_to_ind[word]] += 1\n",
    "                        except:\n",
    "                            #print(\"get_tweet_emotions - Exception\")\n",
    "                            continue\n",
    "    \n",
    "    # append the emotions matrix to the tweets data set\n",
    "    df_tweets = pd.concat([df_tweets, df_emotions], axis=1)\n",
    "\n",
    "    return df_tweets, list_emotion_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of words to highlight \n",
    "def get_words(word_list, emotions):\n",
    "    \n",
    "    words_emotion_idx = []\n",
    "    \n",
    "    for i, word in enumerate(word_list):\n",
    "        word = stemmer.stem(word.lower())\n",
    "        result = lexicon[lexicon.English == word]\n",
    "        if not result.empty:\n",
    "            for emotion in emotions:\n",
    "                if result[emotion].any() > 0:\n",
    "                    words_emotion_idx.append(i)\n",
    "                \n",
    "    return words_emotion_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7448f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_emotion_words(word_counts, n = 5):\n",
    "\n",
    "    # Here I map the numpy array \"words\" with the index and word frequency\n",
    "    words = np.zeros((len(word_counts), 2), dtype=int)\n",
    "    for i, w in enumerate(word_counts):\n",
    "        words[i][0] = w\n",
    "        words[i][1] = word_counts[w]\n",
    "\n",
    "    # From the indexes generated by the argsort function, \n",
    "    # I get the order of the top n words in the list\n",
    "    top_words_idx = np.flip(np.argsort(words[:,1])[-n:],0)\n",
    "\n",
    "    # The resulting indexes are now used as keys in the dic to get the words\n",
    "    top_words = [words[ind][0] for ind in top_words_idx]\n",
    "        \n",
    "    return words, top_words, top_words_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d151fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is now the function to display and highlight \n",
    "# the words associated to specific emotions\n",
    "def print_colored_emotions(tweets, emotions, color, on_color):\n",
    "    \n",
    "    for tweet in tweets:\n",
    "\n",
    "        word_list = word_tokenize(tweet)\n",
    "\n",
    "        word_emotion_idx = get_words(word_list, emotions)\n",
    "\n",
    "        for i, w in enumerate(word_list):\n",
    "            if i in word_emotion_idx:\n",
    "                w=colored(w, color=color, on_color=on_color)\n",
    "            print(w, end='') \n",
    "            print(' ', end='')  \n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f013d62-a297-4169-93f8-2801f40d40aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotionDf100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab0782-a54a-4df6-aba9-fd24f43f50b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Conncting words to emotions\n",
    "# We are using the NCR lexicon to associate words to emotions \n",
    "\n",
    "df_emo, list_emotion_counts = get_tweet_emotions(emotionDf100, emotions, 'edited')\n",
    "\n",
    "# Preparing for time series\n",
    "df_emo['date']= pd.to_datetime(df_emo['date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e3438-ef11-4b38-b71c-695140484c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(help(list_emotion_counts[0][\"emo_frq\"]))\n",
    "print(list_emotion_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b41cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the 10 words that contribute the most for each of the 8 emotions\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(15, 25), frameon=False) \n",
    "plt.box(False)\n",
    "plt.axis('off')\n",
    "plt.subplots_adjust(hspace = 1.6)\n",
    "counter = 0\n",
    "\n",
    "for i, emotion in enumerate(emotions): # for each emotioin\n",
    "\n",
    "    # This is the dict that holds the top 10 words \n",
    "    \n",
    "    words, top_words, top_words_indices = get_top_emotion_words(list_emotion_counts[i]['emo_frq'], 10)\n",
    "    #print(str(list_emotion_counts)+\"\\n\")\n",
    "    #print(words)\n",
    "    \n",
    "    info = {'values' : [words[ind][1] for ind in top_words_indices], \n",
    "                      'labels' : [ind_to_word[word] for word in top_words]}\n",
    "    #print(info)\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.set_context(\"notebook\", font_scale=1.25)\n",
    "    ax = fig.add_subplot(4, 2, counter+1) # plot 2 charts in each of the 4 rows\n",
    "    sns.despine()\n",
    "    ax = sns.barplot(x='labels', y='values', data=info, palette=(\"cividis\"))\n",
    "    plt.ylabel('Top words', fontsize=12)\n",
    "    ax.set_title(label=str('Emotion: ') + emotion, fontweight='bold', size=13)\n",
    "    plt.xticks(rotation=45, fontsize=14)\n",
    "    counter += 1\n",
    "\n",
    "axs.set_title(label='\\nTop 10 words for each emotion\\n', \n",
    "             fontweight='bold', size=20, pad=40)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/Top10_words_per_emotion.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650660a2-810d-401d-8959-3c615432e151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
