{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e3e693-b9e9-4d3a-8583-e15f040e6252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 13:48:45.163 INFO    numexpr.utils: NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import jsonlines\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import fileinput\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud #For issues during installing wordcloud, try: https://stackoverflow.com/questions/41409570/cant-install-wordcloud-in-python-anaconda\n",
    "from textblob_de import TextBlobDE\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "from datetime import date\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a8e91d-6e0b-42d7-b340-141f503131fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65 entries, 0 to 64\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   http_status   65 non-null     int64 \n",
      " 1   account_name  65 non-null     object\n",
      " 2   account_data  65 non-null     object\n",
      " 3   params        65 non-null     object\n",
      " 4   response      65 non-null     object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 2.7+ KB\n",
      "Datensatzlänge: None\n",
      "   http_status account_name  \\\n",
      "0          200   OlafScholz   \n",
      "1          200   OlafScholz   \n",
      "2          200   OlafScholz   \n",
      "3          200   OlafScholz   \n",
      "4          200   OlafScholz   \n",
      "\n",
      "                                        account_data  \\\n",
      "0  {'Name': 'Scholz, Olaf', 'Partei': 'SPD', 'id'...   \n",
      "1  {'Name': 'Scholz, Olaf', 'Partei': 'SPD', 'id'...   \n",
      "2  {'Name': 'Scholz, Olaf', 'Partei': 'SPD', 'id'...   \n",
      "3  {'Name': 'Scholz, Olaf', 'Partei': 'SPD', 'id'...   \n",
      "4  {'Name': 'Scholz, Olaf', 'Partei': 'SPD', 'id'...   \n",
      "\n",
      "                                              params  \\\n",
      "0  [[query, from:OlafScholz], [max_results, 100],...   \n",
      "1  [[query, from:OlafScholz], [max_results, 100],...   \n",
      "2  [[query, from:OlafScholz], [max_results, 100],...   \n",
      "3  [[query, from:OlafScholz], [max_results, 100],...   \n",
      "4  [[query, from:OlafScholz], [max_results, 100],...   \n",
      "\n",
      "                                            response  \n",
      "0  {'data': [{'context_annotations': [{'domain': ...  \n",
      "1  {'data': [{'in_reply_to_user_id': '38150247', ...  \n",
      "2  {'data': [{'conversation_id': '144576374098134...  \n",
      "3  {'data': [{'created_at': '2021-09-02T13:33:12....  \n",
      "4  {'data': [{'conversation_id': '141999599421797...  \n"
     ]
    }
   ],
   "source": [
    "#Importing Dataset\n",
    "olafScholzJsonLines = jsonlines.open(\"OlafScholz.jl\")\n",
    "\n",
    "olafScholzTwitter = pd.read_json(\"OlafScholz.jl\", lines = True)\n",
    "print(\"Datensatzlänge: \" + str(olafScholzTwitter.info()))\n",
    "\n",
    "print(olafScholzTwitter.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed117af7-5b00-4ff1-970b-2e6eb60fa65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [tweetid, date, time, user, text]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Reply Dataframe\n",
    "# Importing Data into final Dataframe for ML\n",
    "replyDataFrame = pd.DataFrame(columns = [\"tweetid\", \"date\", \"time\", \"user\", \"text\"])\n",
    "print(replyDataFrame.head())\n",
    "iterator = 0\n",
    "\n",
    "for line in olafScholzJsonLines:\n",
    "    keyResponse = line[\"response\"]\n",
    "    data = keyResponse[\"data\"]\n",
    "    newDataRow = [None, None, None, None, None]\n",
    "    userName = line[\"account_name\"]\n",
    "    newDataRow[4] = userName\n",
    "    keyResponse = line[\"response\"]\n",
    "    data = keyResponse[\"data\"]\n",
    "\n",
    "\n",
    "    for tweet in data:\n",
    "\n",
    "        tweetTarget = 1\n",
    "       \n",
    "        tweetID = tweet[\"id\"]\n",
    "        authorID = tweet[\"author_id\"]\n",
    "        tweetDate = tweet[\"created_at\"][0:10]\n",
    "        tweetTime = tweet[\"created_at\"][11:19]\n",
    "        tweetText = tweet[\"text\"]        \n",
    "        newDataRow[0] = tweetID\n",
    "        newDataRow[1] = tweetDate\n",
    "        newDataRow[2] = tweetTime\n",
    "        newDataRow[3] = authorID\n",
    "        newDataRow[4] = tweetText\n",
    "        replyDataFrame.loc[len(replyDataFrame)] = newDataRow\n",
    "    iterator += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c9a78f-de0b-451a-bd3a-f2297741dd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6419\n",
      "(6419, 5)\n",
      "               tweetid        date      time      user  \\\n",
      "0  1515588838562971650  2022-04-17  07:11:52  38150247   \n",
      "1  1514644785818058757  2022-04-14  16:40:32  38150247   \n",
      "2  1514291545742184451  2022-04-13  17:16:53  38150247   \n",
      "3  1514256817613905929  2022-04-13  14:58:53  38150247   \n",
      "4  1513955069867274245  2022-04-12  18:59:51  38150247   \n",
      "\n",
      "                                                text  \n",
      "0  RT @Bundeskanzler: Ich wünsche Ihnen und Ihren...  \n",
      "1  RT @Bundeskanzler: Meine Solidarität gilt @Kar...  \n",
      "2  RT @Bundeskanzler: Sie haben Hilfe organisiert...  \n",
      "3  RT @Bundeskanzler: Verantwortung übernehmen he...  \n",
      "4  RT @Bundeskanzler: Die Sanktionen gegen #Russl...  \n"
     ]
    }
   ],
   "source": [
    "#print(dir(replyDataFrame))\n",
    "print(len(replyDataFrame))\n",
    "print(replyDataFrame.shape)\n",
    "print(replyDataFrame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4692e308-20ee-4244-958c-eb55099d86b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "                tweetid        date      time      user  \\\n",
      "53  1493195813245313024  2022-02-14  12:09:58  38150247   \n",
      "52  1493599012691365903  2022-02-15  14:52:09  38150247   \n",
      "51  1493602211829587986  2022-02-15  15:04:51  38150247   \n",
      "50  1494003008283287560  2022-02-16  17:37:29  38150247   \n",
      "49  1494245849085493251  2022-02-17  09:42:26  38150247   \n",
      "\n",
      "                                                 text  \n",
      "53  RT @Bundeskanzler: Auf dem Weg in die #Ukraine...  \n",
      "52  RT @Bundeskanzler: Der Frieden und die Sicherh...  \n",
      "51  RT @Bundeskanzler: Ich habe Präsident #Putin i...  \n",
      "50  RT @Bundeskanzler: Die #Corona-Regeln zeigen d...  \n",
      "49  RT @Bundeskanzler: Gestern Abend habe ich mit ...  \n"
     ]
    }
   ],
   "source": [
    "print(\"----\")\n",
    "print(replyDataFrame[replyDataFrame[\"text\"].str.contains(\"RT @Bundeskanzler:\")].sort_values(\"date\",ascending = \"true\").head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f26ce0b-1710-4891-b1a1-cd09136caa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    True\n",
      "1    True\n",
      "2    True\n",
      "3    True\n",
      "4    True\n",
      "Name: text, dtype: bool\n",
      "6419\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(replyDataFrame[\"text\"].str.contains(\"RT @Bundeskanzler\").head())\n",
    "print(replyDataFrame[\"text\"].str.contains(\"RT @Bundeskanzler\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aecd9a0-d7bc-4a37-98fa-b8ab6f6ac397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataframe :\n",
      "tweetid    6419\n",
      "date       6419\n",
      "time       6419\n",
      "user       6419\n",
      "text       6419\n",
      "dtype: int64\n",
      "----\n",
      "Size of dataframe with RT @Bundeskanzler:\n",
      "tweetid    48\n",
      "date       48\n",
      "time       48\n",
      "user       48\n",
      "text       48\n",
      "dtype: int64\n",
      "----\n",
      "Size of dataframe without RT @:\n",
      "tweetid    4071\n",
      "date       4071\n",
      "time       4071\n",
      "user       4071\n",
      "text       4071\n",
      "dtype: int64\n",
      "----\n",
      "Size of cleansedTweetDF @:\n",
      "tweetid    4119\n",
      "date       4119\n",
      "time       4119\n",
      "user       4119\n",
      "text       4119\n",
      "dtype: int64\n",
      "----\n",
      "               tweetid        date      time      user  \\\n",
      "0  1515588838562971650  2022-04-17  07:11:52  38150247   \n",
      "1  1514644785818058757  2022-04-14  16:40:32  38150247   \n",
      "2  1514291545742184451  2022-04-13  17:16:53  38150247   \n",
      "3  1514256817613905929  2022-04-13  14:58:53  38150247   \n",
      "4  1513955069867274245  2022-04-12  18:59:51  38150247   \n",
      "\n",
      "                                                text  \n",
      "0  RT @Bundeskanzler: Ich wünsche Ihnen und Ihren...  \n",
      "1  RT @Bundeskanzler: Meine Solidarität gilt @Kar...  \n",
      "2  RT @Bundeskanzler: Sie haben Hilfe organisiert...  \n",
      "3  RT @Bundeskanzler: Verantwortung übernehmen he...  \n",
      "4  RT @Bundeskanzler: Die Sanktionen gegen #Russl...  \n"
     ]
    }
   ],
   "source": [
    "#cleansedTweetDF = replyDataFrame[replyDataFrame[\"text\"].str.contains(\"RT @Bundeskanzler:\") or replyDataFrame[\"text\"].str.contains(\"!RT @\") ]\n",
    "print(\"Size of dataframe :\\n\"+ str(replyDataFrame.count())+\"\\n----\")\n",
    "maskBundesKanz = replyDataFrame[\"text\"].str.contains(\"RT @Bundeskanzler:\")\n",
    "maskNoRT = replyDataFrame[\"text\"].str.contains(\"RT @\")==False\n",
    "print(\"Size of dataframe with RT @Bundeskanzler:\\n\" + str(replyDataFrame[maskBundesKanz].count())+\"\\n----\")\n",
    "print(\"Size of dataframe without RT @:\\n\" + str(replyDataFrame[maskNoRT].count())+\"\\n----\")\n",
    "#cleansedTweetDF contains all Tweets posted directly by Olaf before or after being Kanzler\n",
    "cleansedTweetDF = replyDataFrame[maskBundesKanz | maskNoRT]\n",
    "print(\"Size of cleansedTweetDF @:\\n\" + str(cleansedTweetDF.count())+\"\\n----\")\n",
    "print(cleansedTweetDF.head())\n",
    "#cleansedTweetDF has \"RT @Bundeskanzler:\" still in the tweet. To not have any issues with ML we removed that part of the Tweet in the following parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d67e75-e4ee-4015-bb5b-fcdf7bd123d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplyDataframe: \n",
      "tweetid    48\n",
      "date       48\n",
      "time       48\n",
      "user       48\n",
      "text       48\n",
      "dtype: int64\n",
      "               tweetid        date      time      user  \\\n",
      "0  1515588838562971650  2022-04-17  07:11:52  38150247   \n",
      "1  1514644785818058757  2022-04-14  16:40:32  38150247   \n",
      "2  1514291545742184451  2022-04-13  17:16:53  38150247   \n",
      "3  1514256817613905929  2022-04-13  14:58:53  38150247   \n",
      "4  1513955069867274245  2022-04-12  18:59:51  38150247   \n",
      "\n",
      "                                                text  \n",
      "0  Ich wünsche Ihnen und Ihren Familien frohe #Os...  \n",
      "1  Meine Solidarität gilt @Karl_Lauterbach: Jeder...  \n",
      "2  Sie haben Hilfe organisiert, Kräfte gebündelt ...  \n",
      "3  Verantwortung übernehmen heißt: Deutschland li...  \n",
      "4  Die Sanktionen gegen #Russland sind notwendig ...  \n"
     ]
    }
   ],
   "source": [
    "# Removing the Text \\\"RT @Bundeskanzler:\\\" from the text\\n\n",
    "print(\"ReplyDataframe: \\n\" + str(replyDataFrame[maskBundesKanz].count()))\n",
    "cleansedAtBundKanzDF = replyDataFrame[maskBundesKanz].copy()\n",
    "cleansedAtBundKanzDF.text = cleansedAtBundKanzDF.text.str.strip(\"RT @Bundeskanzler: \")\n",
    "print(cleansedAtBundKanzDF.head())                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb503cc-0edb-4313-9da9-fb3d54f534b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweetid    4119\n",
      "date       4119\n",
      "time       4119\n",
      "user       4119\n",
      "text       4119\n",
      "dtype: int64\n",
      "               tweetid        date      time      user  \\\n",
      "0  1515588838562971650  2022-04-17  07:11:52  38150247   \n",
      "1  1514644785818058757  2022-04-14  16:40:32  38150247   \n",
      "2  1514291545742184451  2022-04-13  17:16:53  38150247   \n",
      "3  1514256817613905929  2022-04-13  14:58:53  38150247   \n",
      "4  1513955069867274245  2022-04-12  18:59:51  38150247   \n",
      "\n",
      "                                                text  \n",
      "0  Ich wünsche Ihnen und Ihren Familien frohe #Os...  \n",
      "1  Meine Solidarität gilt @Karl_Lauterbach: Jeder...  \n",
      "2  Sie haben Hilfe organisiert, Kräfte gebündelt ...  \n",
      "3  Verantwortung übernehmen heißt: Deutschland li...  \n",
      "4  Die Sanktionen gegen #Russland sind notwendig ...  \n"
     ]
    }
   ],
   "source": [
    "#appending the sliced versions to the Dataframe and resulting in the expected 4119 Tweets\n",
    "cleansedTweetDFShort = replyDataFrame[maskNoRT].append(cleansedAtBundKanzDF)\n",
    "print(cleansedTweetDFShort.count())\n",
    "print(cleansedTweetDFShort.sort_values(\"date\",ascending = False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd6c646a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1508125473842335749</td>\n",
       "      <td>2022-03-27</td>\n",
       "      <td>16:55:07</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Die Saarländerinnen und Saarländer haben sich ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1504398354565976070</td>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>10:04:52</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Meine persönliche Position ist längst bekannt:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1492872048120668161</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>14:43:27</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Ich freue mich, wenn Sie und Ihr mir ab heute ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1492074452796022791</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>09:54:05</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Für die aktuelle Welle der Pandemie gibt es An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1492074451432873985</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>09:54:05</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Die Corona-Pandemie und auch die Flutkatastrop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1494245849085493251</td>\n",
       "      <td>2022-02-17</td>\n",
       "      <td>09:42:26</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Gestern Abend habe ich mit Biden zur Situation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1494003008283287560</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>17:37:29</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Die Corona-Regeln zeigen die erhoffte Wirkung....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1493602211829587986</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>15:04:51</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Ich habe Präsident Putin in Moskau verdeutlich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1493599012691365903</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>14:52:09</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Der Frieden und die Sicherheit in Europa sind ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1493195813245313024</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>12:09:58</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Auf dem Weg in die Ukraine. Heute in Kiew und ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4119 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweetid        date      time      user  \\\n",
       "12  1508125473842335749  2022-03-27  16:55:07  38150247   \n",
       "21  1504398354565976070  2022-03-17  10:04:52  38150247   \n",
       "54  1492872048120668161  2022-02-13  14:43:27  38150247   \n",
       "55  1492074452796022791  2022-02-11  09:54:05  38150247   \n",
       "56  1492074451432873985  2022-02-11  09:54:05  38150247   \n",
       "..                  ...         ...       ...       ...   \n",
       "49  1494245849085493251  2022-02-17  09:42:26  38150247   \n",
       "50  1494003008283287560  2022-02-16  17:37:29  38150247   \n",
       "51  1493602211829587986  2022-02-15  15:04:51  38150247   \n",
       "52  1493599012691365903  2022-02-15  14:52:09  38150247   \n",
       "53  1493195813245313024  2022-02-14  12:09:58  38150247   \n",
       "\n",
       "                                                 text  \n",
       "12  Die Saarländerinnen und Saarländer haben sich ...  \n",
       "21  Meine persönliche Position ist längst bekannt:...  \n",
       "54  Ich freue mich, wenn Sie und Ihr mir ab heute ...  \n",
       "55  Für die aktuelle Welle der Pandemie gibt es An...  \n",
       "56  Die Corona-Pandemie und auch die Flutkatastrop...  \n",
       "..                                                ...  \n",
       "49  Gestern Abend habe ich mit Biden zur Situation...  \n",
       "50  Die Corona-Regeln zeigen die erhoffte Wirkung....  \n",
       "51  Ich habe Präsident Putin in Moskau verdeutlich...  \n",
       "52  Der Frieden und die Sicherheit in Europa sind ...  \n",
       "53  Auf dem Weg in die Ukraine. Heute in Kiew und ...  \n",
       "\n",
       "[4119 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tobias 29.05.22 and 30.05.22\n",
    "\n",
    "# Starting preparing for machine learning\n",
    "\n",
    "# Making statement text in lower case\n",
    "\n",
    "cleansedTweetDFShort\n",
    "\n",
    "#Copy for emotion analysis\n",
    "\n",
    "emotionDF = cleansedTweetDFShort.copy()\n",
    "\n",
    "def clean(text):\n",
    "    cleansedTweetDFShort['text'] = cleansedTweetDFShort['text'].str.replace('@[A-Za-z0-9]+\\s?', '', regex=True)\n",
    "    text = re.sub(r'@[A-Za-z0-9]+\\s?', '', text) #Removed Mentions\n",
    "    text = re.sub(r'#', '', text) #Removed #\n",
    "    text = re.sub(r'(.)1+', r'1', text) #cleaned single letters\n",
    "    text = re.sub('((www.[^s]+)|(https?://[^s]+))','',text) #Removes links\n",
    "    text = re.sub('@','',text) #Remove @\n",
    "    \n",
    "    return text\n",
    "\n",
    "cleansedTweetDFShort['text'] = cleansedTweetDFShort['text'].apply(clean)\n",
    "\n",
    "cleansedTweetDFShort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78fdbd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweetid    object\n",
      "date       object\n",
      "time       object\n",
      "user       object\n",
      "text       string\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1508125473842335749</td>\n",
       "      <td>2022-03-27</td>\n",
       "      <td>16:55:07</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Die Saarländerinnen und Saarländer haben sich ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1504398354565976070</td>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>10:04:52</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Meine persönliche Position ist längst bekannt:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1492872048120668161</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>14:43:27</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Ich freue mich, wenn Sie und Ihr mir ab heute ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1492074452796022791</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>09:54:05</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Für die aktuelle Welle der Pandemie gibt es An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1492074451432873985</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>09:54:05</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Die Corona-Pandemie und auch die Flutkatastrop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweetid        date      time      user  \\\n",
       "12  1508125473842335749  2022-03-27  16:55:07  38150247   \n",
       "21  1504398354565976070  2022-03-17  10:04:52  38150247   \n",
       "54  1492872048120668161  2022-02-13  14:43:27  38150247   \n",
       "55  1492074452796022791  2022-02-11  09:54:05  38150247   \n",
       "56  1492074451432873985  2022-02-11  09:54:05  38150247   \n",
       "\n",
       "                                                 text  \n",
       "12  Die Saarländerinnen und Saarländer haben sich ...  \n",
       "21  Meine persönliche Position ist längst bekannt:...  \n",
       "54  Ich freue mich, wenn Sie und Ihr mir ab heute ...  \n",
       "55  Für die aktuelle Welle der Pandemie gibt es An...  \n",
       "56  Die Corona-Pandemie und auch die Flutkatastrop...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleansedTweetDFShort = cleansedTweetDFShort.astype({'text':'string'})\n",
    "print(cleansedTweetDFShort.dtypes)\n",
    "cleansedTweetDFShort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faa8d19f-a31b-488c-b170-d4c2fea3e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Philip 04.06.2022 Language Detection\n",
    "\n",
    "languageDF = cleansedTweetDFShort.copy()\n",
    "languageDF.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae50163d-4773-43ac-97cd-ada5c28cb374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying different text detection approaches\n",
    "#https://stackoverflow.com/questions/39142778/python-how-to-determine-the-language\n",
    "# -> https://pypi.org/project/gcld3/ -> requires https://grpc.io/docs/protoc-installation/ -> Windows Install https://www.geeksforgeeks.org/how-to-install-protocol-buffers-on-windows/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d2566f2-612f-4f6d-bf68-0f369e0153a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from textblob import TextBlob\\nfor tweet in languageDF.loc[0:1,\"text\"]:\\n    print(tweet)\\n    print(type(TextBlob(tweet)))\\n    # To get detect_language() https://stackoverflow.com/questions/69207838/textblob-detect-language-function-not-working\\n    print(\"Language: \" + str(TextBlob(tweet).detect_language()))\\n    print(\"----\")'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TextBlob Approach - Doesnt work due to Error 400 - Seems to be an issue with the Google API\n",
    "# https://www.geeksforgeeks.org/detect-an-unknown-language-using-python/\n",
    "\"\"\"from textblob import TextBlob\n",
    "for tweet in languageDF.loc[0:1,\"text\"]:\n",
    "    print(tweet)\n",
    "    print(type(TextBlob(tweet)))\n",
    "    # To get detect_language() https://stackoverflow.com/questions/69207838/textblob-detect-language-function-not-working\n",
    "    print(\"Language: \" + str(TextBlob(tweet).detect_language()))\n",
    "    print(\"----\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ded000c0-ce4c-4d71-b396-075598433b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tweetid        date      time      user  \\\n",
      "0  1508125473842335749  2022-03-27  16:55:07  38150247   \n",
      "1  1504398354565976070  2022-03-17  10:04:52  38150247   \n",
      "2  1492872048120668161  2022-02-13  14:43:27  38150247   \n",
      "3  1492074452796022791  2022-02-11  09:54:05  38150247   \n",
      "4  1492074451432873985  2022-02-11  09:54:05  38150247   \n",
      "\n",
      "                                                text Language  \n",
      "0  Die Saarländerinnen und Saarländer haben sich ...       de  \n",
      "1  Meine persönliche Position ist längst bekannt:...       de  \n",
      "2  Ich freue mich, wenn Sie und Ihr mir ab heute ...       de  \n",
      "3  Für die aktuelle Welle der Pandemie gibt es An...       de  \n",
      "4  Die Corona-Pandemie und auch die Flutkatastrop...       de  \n"
     ]
    }
   ],
   "source": [
    "#LangDetect\n",
    "from langdetect import detect\n",
    "languageDF[\"Language\"] = detect(str(languageDF.text))\n",
    "print(languageDF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10203626-f9bc-4714-a870-871e093ecfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of different Languages found: 1\n",
      "['de']\n"
     ]
    }
   ],
   "source": [
    "print(\"Amount of different Languages found: \" + str(languageDF[\"Language\"].nunique()))\n",
    "uniqueLang = languageDF[\"Language\"].unique()\n",
    "print(uniqueLang)\n",
    "# End of Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4239fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra DF for wordcloud\n",
    "\n",
    "cleansedTweetDFShort10 = cleansedTweetDFShort[:100].copy()\n",
    "cleansedTweetDFShort10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c956fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Polarity and Subjectivity\n",
    "\n",
    "# Subjectivity quantifies the amount of personal opinion and factual information contained in the text. \n",
    "# The higher subjectivity means that the text contains personal opinion rather than factual information. \n",
    "# 0 means low personal opinion and 1 a lot of personal opinion\n",
    "\n",
    "def getS (text):\n",
    "    return TextBlobDE(text).sentiment.subjectivity\n",
    "\n",
    "# Polarity lies between [-1,1], -1 defines a negative sentiment and 1 defines a positive sentiment.\n",
    "\n",
    "def getP (text):\n",
    "    return TextBlobDE(text).sentiment.polarity\n",
    "\n",
    "\n",
    "cleansedTweetDFShort10['Personal Opinion (Subjectivity)'] = cleansedTweetDFShort10['text'].apply(getS)\n",
    "cleansedTweetDFShort10['Sentiment (Polarity)'] = cleansedTweetDFShort10['text'].apply(getP)\n",
    "\n",
    "cleansedTweetDFShort10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc6433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Cloud not cleaned\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "allWords = ' '.join([txt for txt in cleansedTweetDFShort10['text']])\n",
    "wordCloud = WordCloud(max_words=500, width=2000, height = 1000, \n",
    "                      max_font_size = 1000, collocations=False).generate(allWords)\n",
    "\n",
    "plt.imshow(wordCloud, interpolation = \"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e00b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining set containing all stopwords in German. (Stopwords: Word with no relevance for analysis)\n",
    "# Source for Stopwordlist: https://github.com/stopwords-iso/stopwords-de/blob/master/stopwords-de.json\n",
    "# Cleaning text from stopwords\n",
    "\n",
    "# Create copy to work with\n",
    "cleansedTweetDFShort10copy = cleansedTweetDFShort10.copy()\n",
    "\n",
    "# Making statement text in lower case\n",
    "cleansedTweetDFShort10copy['text']=cleansedTweetDFShort10copy['text'].str.lower()\n",
    "\n",
    "with open('stopwords-de.json','r') as file:\n",
    "    stopwordlistgerman = json.load(file)\n",
    "\n",
    "STOPWORDS = set(stopwordlistgerman)\n",
    "\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "cleansedTweetDFShort10copy['text'] = cleansedTweetDFShort10copy['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "cleansedTweetDFShort10copy['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58708884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning text from english stopwords\n",
    "# Source for Stopwordlist: https://countwordsfree.com/stopwords\n",
    "# Cleaning text from stopwords\n",
    "with open('stop_words_english.json','r') as file:\n",
    "    stopwordlistenglish = json.load(file)\n",
    "\n",
    "STOPWORDS = set(stopwordlistenglish)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "cleansedTweetDFShort10copy['text'] = cleansedTweetDFShort10copy['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "cleansedTweetDFShort10copy['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctations\n",
    "\n",
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "cleansedTweetDFShort10copy['text']= cleansedTweetDFShort10copy['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "cleansedTweetDFShort10copy['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eedef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Stemming\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "porter = PorterStemmer()\n",
    "\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "cleansedTweetDFShort10copy['text']= cleansedTweetDFShort10copy['text'].apply(lambda x: stemming_on_text(x))\n",
    "cleansedTweetDFShort10copy = cleansedTweetDFShort10copy.astype({'text':'string'})\n",
    "cleansedTweetDFShort10copy['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b3dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Cloud after cleaning\n",
    "plt.figure(figsize = (20,20))\n",
    "cleanedwords = ' '.join([txt for txt in cleansedTweetDFShort10copy['text']])\n",
    "wordCloud = WordCloud(max_words = 500, width=2000, height = 1000, max_font_size = 1000, \n",
    "                      collocations=False).generate(cleanedwords)\n",
    "\n",
    "plt.imshow(wordCloud, interpolation = \"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis with TextBlobDE\n",
    "\n",
    "# Create a function to compute analysis\n",
    "\n",
    "def getA(score):\n",
    "    if score < 0:\n",
    "        return 'negative'\n",
    "    elif score == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "cleansedTweetDFShort10['Analysis'] = cleansedTweetDFShort10['Sentiment (Polarity)'].apply(getA)\n",
    "cleansedTweetDFShort10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca05e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedDf = cleansedTweetDFShort10.sort_values(by=['Sentiment (Polarity)'])\n",
    "\n",
    "#sortedDf = sortedDf.astype({'Analysis':'string'})\n",
    "print(sortedDf.dtypes)\n",
    "sortedDf.reset_index(inplace=True, drop=True) # Reset index\n",
    "sortedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24390f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show texts with Polarity and Analysis \n",
    "\n",
    "# Problem erkannt: Er schreibt sowohl englisch als auch deutsch\n",
    "\n",
    "sortedDf\n",
    "\n",
    "for i in range (0, sortedDf.shape[0]):\n",
    "    print(sortedDf['text'][i])\n",
    "    print(sortedDf['Personal Opinion (Subjectivity)'][i])\n",
    "    print(sortedDf['Sentiment (Polarity)'][i])\n",
    "    print(sortedDf['Analysis'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Positive tweets\n",
    "\n",
    "j=1;\n",
    "\n",
    "for i in range(0, sortedDf.shape[0]):\n",
    "    if(sortedDf['Analysis'][i] == 'positive'):\n",
    "        print(str(j) + ') ' + sortedDf['text'][i])\n",
    "        print(sortedDf['Personal Opinion (Subjectivity)'][i])\n",
    "        print(sortedDf['Sentiment (Polarity)'][i])\n",
    "        print(sortedDf['Analysis'][i])\n",
    "        print()\n",
    "        j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81723a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Negative tweets\n",
    "\n",
    "j=1;\n",
    "\n",
    "for i in range(0, sortedDf.shape[0]):\n",
    "    if(sortedDf['Analysis'][i] == 'negative'):\n",
    "        print(str(j) + ') ' + sortedDf['text'][i])\n",
    "        print(sortedDf['Personal Opinion (Subjectivity)'][i])\n",
    "        print(sortedDf['Sentiment (Polarity)'][i])\n",
    "        print(sortedDf['Analysis'][i])\n",
    "        print()\n",
    "        j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot polarity and subjectivity\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "for i in range (0, sortedDf.shape[0]):\n",
    "    plt.scatter(sortedDf['Sentiment (Polarity)'][i], sortedDf['Personal Opinion (Subjectivity)'][i], color='Green')\n",
    "                \n",
    "plt.title('Sentiment Analysis')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Subjectivity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc395ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the percentage of positive tweets\n",
    "\n",
    "ptweets = sortedDf[sortedDf.Analysis ==\"positive\"] \n",
    "\n",
    "print(\"Positive Tweets in percent: \"+str(round((ptweets.shape[0]/sortedDf.shape[0])*100, 1)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the negative of positive tweets\n",
    "\n",
    "neutweets = sortedDf[sortedDf.Analysis ==\"neutral\"] \n",
    "\n",
    "print(\"Neutral Tweets in percent: \"+str(round((neutweets.shape[0]/sortedDf.shape[0])*100, 1)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d786b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the negative of positive tweets\n",
    "\n",
    "ntweets = sortedDf[sortedDf.Analysis ==\"negative\"] \n",
    "\n",
    "print(\"Negative Tweets in percent: \"+str(round((ntweets.shape[0]/sortedDf.shape[0])*100, 1)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde86c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Analysis\n",
    "\n",
    "sortedDf['Analysis'].value_counts()\n",
    "\n",
    "plt.title('Sentiment Analysis in %')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Counts')\n",
    "sortedDf['Analysis'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tobias 03.06.2022\n",
    "# Sentiment and Emotion Analysis\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "emotionDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c29ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "STOPLIST = set(stopwords.words('german'))\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + \\\n",
    "[\"-\", \"...\", \"”\", \"``\", \",\", \".\", \":\", \"''\",\"#\",\"@\"]\n",
    "\n",
    "# The NLTK lemmatizer and stemmer classes\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('german')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b31ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use the POS tagging from NLTK to retain only adjectives, verbs, adverbs \n",
    "# and nouns as a base for for lemmatization.\n",
    "def get_lemmas(tweet): \n",
    "    \n",
    "    # A dictionary to help convert Treebank tags to WordNet\n",
    "    treebank2wordnet = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r'}\n",
    "    \n",
    "    postag = ''\n",
    "    lemmas_list = []\n",
    "    \n",
    "    for word, tag in pos_tag(word_tokenize(tweet)):\n",
    "        if tag.startswith(\"JJ\")     \\\n",
    "            or tag.startswith(\"RB\") \\\n",
    "            or tag.startswith(\"VB\") \\\n",
    "            or tag.startswith(\"NN\"):\n",
    "                \n",
    "            try:\n",
    "                postag = treebank2wordnet[tag[:2]]\n",
    "            except:\n",
    "                postag = 'n'                \n",
    "                            \n",
    "            lemmas_list.append(lemmatizer.lemmatize(word.lower(), postag))    \n",
    "    \n",
    "    return lemmas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to clean and filter the tokens in each tweet\n",
    "def clean_tweet(tokens):\n",
    "    \n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            if token not in STOPLIST:\n",
    "                if token[0] not in SYMBOLS:\n",
    "                    if not token.startswith('http'):\n",
    "                        if  '/' not in token:\n",
    "                            if  '-' not in token:\n",
    "                                filtered.append(token)\n",
    "                                        \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66708cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts the lemmatization process\n",
    "def get_lemmatized(tweet):\n",
    "   \n",
    "    all_tokens_string = ''\n",
    "    filtered = []\n",
    "    tokens = []\n",
    "\n",
    "    # lemmatize\n",
    "    tokens = [token for token in get_lemmas(tweet)]\n",
    "    \n",
    "    # filter\n",
    "    filtered = clean_tweet(tokens)\n",
    "\n",
    "    # join everything into a single string\n",
    "    all_tokens_string = ' '.join(filtered)\n",
    "    \n",
    "    return all_tokens_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the lemmatized tweets and puts the result in an \"edited\" text column\n",
    "# for future use in this script\n",
    "edited = ''\n",
    "for i, row in emotionDF.iterrows():\n",
    "    edited = get_lemmatized(emotionDF.loc[i]['text'])\n",
    "    if len(edited) > 0:\n",
    "        emotionDF.at[i,'edited'] = edited\n",
    "    else:\n",
    "        emotionDF.at[i,'edited'] = None     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After lemmatization, some tweets may end up with the same words\n",
    "# Let's make sure that we have no duplicates\n",
    "emotionDF.drop_duplicates(subset=['edited'], inplace=True)\n",
    "emotionDF.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de281f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using apply/lambda to create a new column with the number of words in each tweet\n",
    "emotionDF['word_count'] = emotionDF.apply(lambda x: len(x['text'].split()),axis=1)\n",
    "t = pd.DataFrame(emotionDF['word_count'].describe()).T\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8217f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotionDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "emotionDF['date']=pd.to_datetime(emotionDF['date']) \n",
    "emotionDF.sort_values('date', inplace=True, ascending=True)\n",
    "emotionDF = emotionDF.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902cbf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid repetitions in our code, here are some plotting functions \n",
    "# that will be called often ...\n",
    "\n",
    "def plot_sentiment_period(df, info):\n",
    "    \n",
    "    # Using the mean values of sentiment for each period\n",
    "    df1 = df.groupby(df['datetime'].dt.to_period(info['period'])).mean()\n",
    "\n",
    "    df1.reset_index(inplace=True)\n",
    "    df1['datetime'] = pd.PeriodIndex(df1['datetime']).to_timestamp()\n",
    "    plot_df = pd.DataFrame(df1, df1.index, info['cols'])\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    ax = sns.lineplot(data=plot_df, linewidth = 3, dashes = False)\n",
    "    plt.legend(loc='best', fontsize=15)\n",
    "    plt.title(info['title'], fontsize=20)\n",
    "    plt.xlabel(info['xlab'], fontsize=15)\n",
    "    plt.ylabel(info['ylab'], fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/' + info['fname'])    \n",
    "    return\n",
    "\n",
    "\n",
    "def plot_fractions(props, title, fname):\n",
    "    \n",
    "    plt1 = props.plot(kind='bar', stacked=False, figsize=(16,5), colormap='Spectral') \n",
    "    plt.legend(bbox_to_anchor=(1.005, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('Online storm', fontweight='bold', fontsize=18)\n",
    "    plt.xticks(rotation=0,fontsize=14)\n",
    "    #plt.ylim(0, 0.5)\n",
    "    plt.ylabel('Fraction of Tweets', fontweight='bold', fontsize=18)\n",
    "    plt1.set_title(label=title, fontweight='bold', size=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/' + fname + '.png')\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def plot_frequency_chart(info):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    sns.set_context(\"notebook\", font_scale=1)    \n",
    "    ax = sns.barplot(x=info['x'], y=info['y'], data=info['data'], palette=(info['pal']))\n",
    "    ax.set_title(label=info['title'], fontweight='bold', size=18)\n",
    "    plt.ylabel(info['ylab'], fontsize=16)\n",
    "    plt.xlabel(info['xlab'], fontsize=16)\n",
    "    plt.xticks(rotation=info['angle'],fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/' + info['fname'])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling VADER\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get VADER Compound value for sentiment intensity\n",
    "emotionDF['sentiment_intensity'] = [analyzer.polarity_scores(v)['compound'] for v in emotionDF['edited']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e926093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the sentiment category\n",
    "def get_sentiment(intensity):\n",
    "    if intensity >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif (intensity >= -0.05) and (intensity < 0.05):\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "# Using pandas apply/lambda to speed up the process\n",
    "emotionDF['sentiment'] = emotionDF.apply(lambda x: get_sentiment(x['sentiment_intensity']),axis=1)\n",
    "emotionDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translate word using Google translator for Emotionanalysis\n",
    "\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "\n",
    "translater = Translator() #German: 'de'\n",
    "\n",
    "#print(googletrans.LANGUAGES) \n",
    "\n",
    "#Test\n",
    "translator = Translator()\n",
    "translation = translator.translate(\"Der Himmel ist blau und ich mag Bananen\", dest='en')\n",
    "print(translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotiontest = emotionDF[:3].copy()\n",
    "emotiontest['edited'] = emotiontest['edited'].astype(str)\n",
    "emotiontest['translated'] = emotiontest['edited'].apply(translater.translate, src =\"de\", dest='en').apply(getattr, args=('text',))\n",
    "\n",
    "# Problem: Number of translations is limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotionanalysis\n",
    "\n",
    "import termcolor\n",
    "import sys\n",
    "from termcolor import colored, cprint\n",
    "plt.style.use('fivethirtyeight')\n",
    "# Importing the data from the NCR lexicon\n",
    "ncr = pd.read_csv('NCR-lexicon.csv', sep =';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb111b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a list of the emotions\n",
    "emotions = ['Anger', 'Anticipation','Disgust','Fear', 'Joy','Sadness', 'Surprise', 'Trust']\n",
    "# Join all the edited tweets in one single string\n",
    "joined_string = emotiontest(emotiontest['edited'])\n",
    "\n",
    "# Get tokens\n",
    "tokens = joined_string.split(' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
