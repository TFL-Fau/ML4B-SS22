{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotional Sentiment on Twitter\n",
    "## A coronavirus vaccine online firestorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Average_sentiment_during_onlinestorm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ongoing competition for a viable vaccine against coronavirus is arguably the race of the century. With its hundred millions of users, Twitter is particularly well-suited for research into sentiment and emotions running in social media. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I collected the data scraping tweets from Twitter’s application program inter-face (API), using TwitterScraper. Tweets were scraped using the search term \"Curevac\", the name of a German vaccine maker backed by Bill & Melinda Gates Foundation, and currently working on a Covid-19 vaccine. The post covers tweets from a 6-year period, from March 3, 2014 to March 18, 2020 (N = 14,991)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will find examples of some of the most common NLP (Natural Language Processing) techniques used to uncover patterns of sentiment and emotion on social media microbloguing platforms like Twitter. It is organized as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: Exploratory analysis\n",
    "- Step 2: Text processing\n",
    "- Step 3: Sentiment analysis \n",
    "- Step 4: Word frequency \n",
    "- Step 5: LDA topics extraction\n",
    "- Step 6: Emotion analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  EXPLORATORY ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scrapping the Twitter API, the retained tweets were gathered on a csv file: tweets.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's strat by importing the Python packages used for data handling (pandas), scientific computing (numpy) and data visualization (matplotlib and seaborn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import jsonlines\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import date\n",
    "\n",
    "import re # for regular expressions\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65 entries, 0 to 64\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   http_status   65 non-null     int64 \n",
      " 1   account_name  65 non-null     object\n",
      " 2   account_data  65 non-null     object\n",
      " 3   params        65 non-null     object\n",
      " 4   response      65 non-null     object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 2.7+ KB\n",
      "Datensatzlänge: None\n",
      "   http_status account_name  \\\n",
      "0          200   OlafScholz   \n",
      "1          200   OlafScholz   \n",
      "2          200   OlafScholz   \n",
      "3          200   OlafScholz   \n",
      "4          200   OlafScholz   \n",
      "\n",
      "                                        account_data  \\\n",
      "0  {'Name': 'Scholz, Olaf', 'Partei': 'SPD', 'id'...   \n",
      "1  {'Name': 'Scholz, Olaf', 'Partei': 'SPD', 'id'...   \n",
      "2  {'Name': 'Scholz, Olaf', 'Partei': 'SPD', 'id'...   \n",
      "3  {'Name': 'Scholz, Olaf', 'Partei': 'SPD', 'id'...   \n",
      "4  {'Name': 'Scholz, Olaf', 'Partei': 'SPD', 'id'...   \n",
      "\n",
      "                                              params  \\\n",
      "0  [[query, from:OlafScholz], [max_results, 100],...   \n",
      "1  [[query, from:OlafScholz], [max_results, 100],...   \n",
      "2  [[query, from:OlafScholz], [max_results, 100],...   \n",
      "3  [[query, from:OlafScholz], [max_results, 100],...   \n",
      "4  [[query, from:OlafScholz], [max_results, 100],...   \n",
      "\n",
      "                                            response  \n",
      "0  {'data': [{'context_annotations': [{'domain': ...  \n",
      "1  {'data': [{'in_reply_to_user_id': '38150247', ...  \n",
      "2  {'data': [{'conversation_id': '144576374098134...  \n",
      "3  {'data': [{'created_at': '2021-09-02T13:33:12....  \n",
      "4  {'data': [{'conversation_id': '141999599421797...  \n"
     ]
    }
   ],
   "source": [
    "#Importing Dataset\n",
    "olafScholzJsonLines = jsonlines.open(\"OlafScholz.jl\")\n",
    "\n",
    "olafScholzTwitter = pd.read_json(\"OlafScholz.jl\", lines = True)\n",
    "print(\"Datensatzlänge: \" + str(olafScholzTwitter.info()))\n",
    "\n",
    "print(olafScholzTwitter.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              datetime                                               text\n",
      "0  2022-04-17 07:11:52  RT @Bundeskanzler: Ich wünsche Ihnen und Ihren...\n",
      "1  2022-04-14 16:40:32  RT @Bundeskanzler: Meine Solidarität gilt @Kar...\n",
      "2  2022-04-13 17:16:53  RT @Bundeskanzler: Sie haben Hilfe organisiert...\n",
      "3  2022-04-13 14:58:53  RT @Bundeskanzler: Verantwortung übernehmen he...\n",
      "4  2022-04-12 18:59:51  RT @Bundeskanzler: Die Sanktionen gegen #Russl...\n"
     ]
    }
   ],
   "source": [
    "# Reply Dataframe\n",
    "# Importing Data into final Dataframe for ML\n",
    "df = pd.DataFrame(columns = [\"datetime\", \"text\"])\n",
    "iterator = 0\n",
    "\n",
    "for line in olafScholzJsonLines:\n",
    "    keyResponse = line[\"response\"]\n",
    "    data = keyResponse[\"data\"]\n",
    "    newDataRow = [None, None]\n",
    "    userName = line[\"account_name\"]\n",
    "    keyResponse = line[\"response\"]\n",
    "    data = keyResponse[\"data\"]\n",
    "\n",
    "\n",
    "    for tweet in data:\n",
    "\n",
    "        tweetTarget = 1\n",
    "       \n",
    "        tweetDate = tweet[\"created_at\"][0:10]+\" \"+tweet[\"created_at\"][11:19]\n",
    "        tweetText = tweet[\"text\"]  \n",
    "        \n",
    "        newDataRow[0] = tweetDate\n",
    "        newDataRow[1] = tweetText\n",
    "        df.loc[len(df)] = newDataRow\n",
    "    iterator += 1\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataframe :\n",
      "datetime    6419\n",
      "text        6419\n",
      "dtype: int64\n",
      "----\n",
      "Size of dataframe with RT @Bundeskanzler:\n",
      "datetime    48\n",
      "text        48\n",
      "dtype: int64\n",
      "----\n",
      "Size of dataframe without RT @:\n",
      "datetime    4071\n",
      "text        4071\n",
      "dtype: int64\n",
      "----\n",
      "Size of cleansedTweetDF @:\n",
      "datetime    4119\n",
      "text        4119\n",
      "dtype: int64\n",
      "----\n",
      "              datetime                                               text\n",
      "0  2022-04-17 07:11:52  RT @Bundeskanzler: Ich wünsche Ihnen und Ihren...\n",
      "1  2022-04-14 16:40:32  RT @Bundeskanzler: Meine Solidarität gilt @Kar...\n",
      "2  2022-04-13 17:16:53  RT @Bundeskanzler: Sie haben Hilfe organisiert...\n",
      "3  2022-04-13 14:58:53  RT @Bundeskanzler: Verantwortung übernehmen he...\n",
      "4  2022-04-12 18:59:51  RT @Bundeskanzler: Die Sanktionen gegen #Russl...\n"
     ]
    }
   ],
   "source": [
    "#cleansedTweetDF = replyDataFrame[replyDataFrame[\"text\"].str.contains(\"RT @Bundeskanzler:\") or replyDataFrame[\"text\"].str.contains(\"!RT @\") ]\n",
    "print(\"Size of dataframe :\\n\"+ str(df.count())+\"\\n----\")\n",
    "maskBundesKanz = df[\"text\"].str.contains(\"RT @Bundeskanzler:\")\n",
    "maskNoRT = df[\"text\"].str.contains(\"RT @\")==False\n",
    "print(\"Size of dataframe with RT @Bundeskanzler:\\n\" + str(df[maskBundesKanz].count())+\"\\n----\")\n",
    "print(\"Size of dataframe without RT @:\\n\" + str(df[maskNoRT].count())+\"\\n----\")\n",
    "#cleansedTweetDF contains all Tweets posted directly by Olaf before or after being Kanzler\n",
    "cleansedTweetDF = df[maskBundesKanz | maskNoRT]\n",
    "print(\"Size of cleansedTweetDF @:\\n\" + str(cleansedTweetDF.count())+\"\\n----\")\n",
    "print(df.head())\n",
    "#cleansedTweetDF has \"RT @Bundeskanzler:\" still in the tweet. To not have any issues with ML we removed that part of the Tweet in the following parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplyDataframe: \n",
      "datetime    48\n",
      "text        48\n",
      "dtype: int64\n",
      "              datetime                                               text\n",
      "0  2022-04-17 07:11:52  Ich wünsche Ihnen und Ihren Familien frohe #Os...\n",
      "1  2022-04-14 16:40:32  Meine Solidarität gilt @Karl_Lauterbach: Jeder...\n",
      "2  2022-04-13 17:16:53  Sie haben Hilfe organisiert, Kräfte gebündelt ...\n",
      "3  2022-04-13 14:58:53  Verantwortung übernehmen heißt: Deutschland li...\n",
      "4  2022-04-12 18:59:51  Die Sanktionen gegen #Russland sind notwendig ...\n"
     ]
    }
   ],
   "source": [
    "# Removing the Text \\\"RT @Bundeskanzler:\\\" from the text\\n\n",
    "print(\"ReplyDataframe: \\n\" + str(df[maskBundesKanz].count()))\n",
    "tweets = df[maskBundesKanz].copy()\n",
    "tweets.text = df.text.str.strip(\"RT @Bundeskanzler: \")\n",
    "print(tweets.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the date column ready for datetime operations\n",
    "tweets['datetime']= pd.to_datetime(tweets['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a view of the first rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime    datetime64[ns]\n",
       "text                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()\n",
    "tweets.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a plot of the tweets with the word \"CureVac\" over the past 6 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 6-year timeseries plot\n",
    "#fig = plt.figure(figsize=(15, 10))\n",
    "#ax = sns.lineplot(data=tweets.set_index(\"datetime\").groupby(pd.Grouper(freq='Y')).count())\n",
    "#plt.title('Tweets with \"CureVac\" from 2014 to 2020', fontsize=20)\n",
    "#plt.xlabel('Years', fontsize=15)\n",
    "#plt.ylabel('Tweets', fontsize=15)\n",
    "#fig.savefig(\"images/All_Tweets_2014-2020.png\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For almost 6 years, the rate of tweets went out at a regular pace, until one day, the 15th March, everything changed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digital marketing researchers call these events “online firestorms”, referring to negative word of mouth (eWOM) that suddenly attract thousands of expressions of support from other clients through social media [[1]](https://www.researchgate.net/publication/330385319_Detecting_Preventing_and_Mitigating_Online_Firestorms_in_Brand_Communities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us filter the datasets for the two distint periods (before and during online storm), we create a column to mark these stormy days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a column to filter the online storm period (from 15 and 18 March)\n",
    "#def make_onlinestorm_field():\n",
    " #   for i, row in tweets.iterrows():\n",
    "  #      if pd.to_datetime(tweets.at[i, 'datetime']) > pd.Timestamp(date(2020,3,15)):\n",
    "   #         tweets.at[i, 'onlinestorm'] = True\n",
    "    #    else:\n",
    "     #       tweets.at[i, 'onlinestorm'] = False  \n",
    "            \n",
    "#make_onlinestorm_field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting tweets during the three days online storm\n",
    "#print('In three days, tweets went over {}, all around the world.'.format(tweets[tweets['onlinestorm']]['onlinestorm'].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a few of them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a look at the distribution of the tweets, by the hour, during the online storm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "#fig = plt.figure(figsize=(15, 10))\n",
    "#ax = sns.lineplot(data=tweets[tweets['onlinestorm'] == True].set_index(\"datetime\").groupby(pd.Grouper(freq='H')).onlinestorm.count())\n",
    "#plt.title('Tweets per hour from 15 to 18 March 2020', fontsize=20)\n",
    "#plt.xlabel('Time (hours)', fontsize=15)\n",
    "#plt.ylabel('No. Tweets', fontsize=15)\n",
    "#fig.savefig(\"images/All_Tweets_Onlinestorm.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to have a first look at the content of the tweets and do some descriptive statistics. For now, I will focus only on features like hastags, mentions, urls, capital words and words in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to count tweets based on regular expressions\n",
    "def count_tweets(reg_expression, tweet):\n",
    "    tweets_list = re.findall(reg_expression, tweet)\n",
    "    return len(tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to hold these counts\n",
    "content_count = {\n",
    "    'words' : tweets['text'].apply(lambda x: count_tweets(r'\\w+', x)),\n",
    "    'mentions' : tweets['text'].apply(lambda x: count_tweets(r'@\\w+', x)),\n",
    "    'hashtags' : tweets['text'].apply(lambda x: count_tweets(r'#\\w+', x)),\n",
    "    'urls' : tweets['text'].apply(lambda x: count_tweets(r'http.?://[^\\s]+[\\s]?', x)),   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-17 07:11:52</td>\n",
       "      <td>Ich wünsche Ihnen und Ihren Familien frohe #Os...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-14 16:40:32</td>\n",
       "      <td>Meine Solidarität gilt @Karl_Lauterbach: Jeder...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-13 17:16:53</td>\n",
       "      <td>Sie haben Hilfe organisiert, Kräfte gebündelt ...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-13 14:58:53</td>\n",
       "      <td>Verantwortung übernehmen heißt: Deutschland li...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-12 18:59:51</td>\n",
       "      <td>Die Sanktionen gegen #Russland sind notwendig ...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-04-11 13:56:23</td>\n",
       "      <td>Vor der Entscheidung von Familienministerin #A...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-04-06 15:56:19</td>\n",
       "      <td>Der russische Präsident hat sich verrechnet – ...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-04-03 21:47:20</td>\n",
       "      <td>Furchtbare und grauenerregende Aufnahmen errei...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-04-03 21:47:14</td>\n",
       "      <td>Ich verlange, dass internationale Organisation...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-04-02 15:10:11</td>\n",
       "      <td>Die steigenden Energiepreise sind auch für die...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-03-28 18:57:59</td>\n",
       "      <td>Wir sind stark, weil wir geeint sind: In der @...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-03-24 18:55:22</td>\n",
       "      <td>Wir als @G7 tragen globale Verantwortung. Wir ...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-03-24 11:08:27</td>\n",
       "      <td>Genau vor einem Monat hat Präsident Putin sein...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-03-23 10:51:12</td>\n",
       "      <td>Präsident Selenskyj, die #Ukraine kann sich au...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-03-22 17:18:36</td>\n",
       "      <td>Schalte in den Weltraum: Ich habe mich gerade ...</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-03-22 16:59:43</td>\n",
       "      <td>Die Eröffnung des #Tesla-Werks mit @elonmusk i...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022-03-17 10:20:00</td>\n",
       "      <td>Ich danke @ZelenskyyUa für seine eindringliche...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022-03-16 19:52:13</td>\n",
       "      <td>Ich freue mich über den Besuch meiner finnisch...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2022-03-15 16:58:24</td>\n",
       "      <td>Ich freue mich, dass @Intel_DE bald hochmodern...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2022-03-14 17:29:29</td>\n",
       "      <td>Ich treffe mich mit Präsident @RTErdogan in ei...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2022-03-11 16:48:24</td>\n",
       "      <td>Das Leid der Opfer terroristischer Gewalt ist ...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2022-03-11 15:45:37</td>\n",
       "      <td>Das Wichtigste, das von diesem besonderen Gipf...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2022-03-10 18:19:28</td>\n",
       "      <td>Es ist Krieg in Europa. Mein Wunsch ist es, da...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2022-03-09 14:49:56</td>\n",
       "      <td>Da sind @JustinTrudeau und ich uns einig: Die ...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2022-03-08 15:13:01</td>\n",
       "      <td>Heute habe ich mit @EmmanuelMacron und Chin. S...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2022-03-08 08:36:23</td>\n",
       "      <td>Nicht nur heute, sondern Tag für Tag gilt: Vie...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2022-03-07 18:43:48</td>\n",
       "      <td>Gerade habe ich mit @POTUS, @EmmanuelMacron un...</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2022-03-04 12:36:45</td>\n",
       "      <td>Ich drücke allen im @teamdpara bei den #Paraly...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2022-03-04 11:37:32</td>\n",
       "      <td>Mein länger geplanter Besuch beim #Einsatzführ...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2022-03-03 21:09:28</td>\n",
       "      <td>Mir war es wichtig, klar zu machen: Unsere Wel...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2022-03-02 11:27:27</td>\n",
       "      <td>Danke für den herzlichen Empfang, lieber @naft...</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2022-03-02 10:26:09</td>\n",
       "      <td>Ein tief bewegender Besuch in Yad Vashem in Je...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2022-03-01 20:01:43</td>\n",
       "      <td>Die unerschütterliche Freundschaft zu Israel i...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2022-03-01 16:39:04</td>\n",
       "      <td>Der ukrainische Präsident @ZelenskyyUa hat mir...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2022-03-01 15:10:22</td>\n",
       "      <td>Ich habe mich gerade mit der @ecb-Präsidentin ...</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2022-02-27 10:49:58</td>\n",
       "      <td>Putins Krieg darf nicht alte Wunden aufreißen....</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2022-02-26 18:16:42</td>\n",
       "      <td>Der russische Überfall markiert eine Zeitenwen...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2022-02-25 17:51:19</td>\n",
       "      <td>Wir sind uns da mit unseren Partnern in der @N...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2022-02-24 18:00:55</td>\n",
       "      <td>Putin will mit seinem Angriff die Zeit zurückd...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2022-02-24 08:09:37</td>\n",
       "      <td>Der russische Angriff auf die Ukraine ist ein ...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2022-02-23 11:38:25</td>\n",
       "      <td>Viele Bürgerinnen und Bürger unseres Landes ar...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2022-02-22 20:38:43</td>\n",
       "      <td>Die Lage ist ernst. Die Friedensordnung in Eur...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2022-02-19 08:32:43</td>\n",
       "      <td>Wir können täglich an die Opfer von #Hanau eri...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2022-02-17 09:42:26</td>\n",
       "      <td>Gestern Abend habe ich mit @POTUS Biden zur Si...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2022-02-16 17:37:29</td>\n",
       "      <td>Die #Corona-Regeln zeigen die erhoffte Wirkung...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2022-02-15 15:04:51</td>\n",
       "      <td>Ich habe Präsident #Putin in Moskau verdeutlic...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2022-02-15 14:52:09</td>\n",
       "      <td>Der Frieden und die Sicherheit in Europa sind ...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2022-02-14 12:09:58</td>\n",
       "      <td>Auf dem Weg in die #Ukraine. Heute in Kiew und...</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime                                               text  \\\n",
       "0  2022-04-17 07:11:52  Ich wünsche Ihnen und Ihren Familien frohe #Os...   \n",
       "1  2022-04-14 16:40:32  Meine Solidarität gilt @Karl_Lauterbach: Jeder...   \n",
       "2  2022-04-13 17:16:53  Sie haben Hilfe organisiert, Kräfte gebündelt ...   \n",
       "3  2022-04-13 14:58:53  Verantwortung übernehmen heißt: Deutschland li...   \n",
       "4  2022-04-12 18:59:51  Die Sanktionen gegen #Russland sind notwendig ...   \n",
       "5  2022-04-11 13:56:23  Vor der Entscheidung von Familienministerin #A...   \n",
       "6  2022-04-06 15:56:19  Der russische Präsident hat sich verrechnet – ...   \n",
       "7  2022-04-03 21:47:20  Furchtbare und grauenerregende Aufnahmen errei...   \n",
       "8  2022-04-03 21:47:14  Ich verlange, dass internationale Organisation...   \n",
       "9  2022-04-02 15:10:11  Die steigenden Energiepreise sind auch für die...   \n",
       "11 2022-03-28 18:57:59  Wir sind stark, weil wir geeint sind: In der @...   \n",
       "13 2022-03-24 18:55:22  Wir als @G7 tragen globale Verantwortung. Wir ...   \n",
       "15 2022-03-24 11:08:27  Genau vor einem Monat hat Präsident Putin sein...   \n",
       "16 2022-03-23 10:51:12  Präsident Selenskyj, die #Ukraine kann sich au...   \n",
       "17 2022-03-22 17:18:36  Schalte in den Weltraum: Ich habe mich gerade ...   \n",
       "18 2022-03-22 16:59:43  Die Eröffnung des #Tesla-Werks mit @elonmusk i...   \n",
       "20 2022-03-17 10:20:00  Ich danke @ZelenskyyUa für seine eindringliche...   \n",
       "22 2022-03-16 19:52:13  Ich freue mich über den Besuch meiner finnisch...   \n",
       "23 2022-03-15 16:58:24  Ich freue mich, dass @Intel_DE bald hochmodern...   \n",
       "24 2022-03-14 17:29:29  Ich treffe mich mit Präsident @RTErdogan in ei...   \n",
       "25 2022-03-11 16:48:24  Das Leid der Opfer terroristischer Gewalt ist ...   \n",
       "26 2022-03-11 15:45:37  Das Wichtigste, das von diesem besonderen Gipf...   \n",
       "27 2022-03-10 18:19:28  Es ist Krieg in Europa. Mein Wunsch ist es, da...   \n",
       "28 2022-03-09 14:49:56  Da sind @JustinTrudeau und ich uns einig: Die ...   \n",
       "29 2022-03-08 15:13:01  Heute habe ich mit @EmmanuelMacron und Chin. S...   \n",
       "30 2022-03-08 08:36:23  Nicht nur heute, sondern Tag für Tag gilt: Vie...   \n",
       "31 2022-03-07 18:43:48  Gerade habe ich mit @POTUS, @EmmanuelMacron un...   \n",
       "32 2022-03-04 12:36:45  Ich drücke allen im @teamdpara bei den #Paraly...   \n",
       "33 2022-03-04 11:37:32  Mein länger geplanter Besuch beim #Einsatzführ...   \n",
       "34 2022-03-03 21:09:28  Mir war es wichtig, klar zu machen: Unsere Wel...   \n",
       "35 2022-03-02 11:27:27  Danke für den herzlichen Empfang, lieber @naft...   \n",
       "36 2022-03-02 10:26:09  Ein tief bewegender Besuch in Yad Vashem in Je...   \n",
       "37 2022-03-01 20:01:43  Die unerschütterliche Freundschaft zu Israel i...   \n",
       "38 2022-03-01 16:39:04  Der ukrainische Präsident @ZelenskyyUa hat mir...   \n",
       "39 2022-03-01 15:10:22  Ich habe mich gerade mit der @ecb-Präsidentin ...   \n",
       "40 2022-02-27 10:49:58  Putins Krieg darf nicht alte Wunden aufreißen....   \n",
       "41 2022-02-26 18:16:42  Der russische Überfall markiert eine Zeitenwen...   \n",
       "42 2022-02-25 17:51:19  Wir sind uns da mit unseren Partnern in der @N...   \n",
       "43 2022-02-24 18:00:55  Putin will mit seinem Angriff die Zeit zurückd...   \n",
       "44 2022-02-24 08:09:37  Der russische Angriff auf die Ukraine ist ein ...   \n",
       "46 2022-02-23 11:38:25  Viele Bürgerinnen und Bürger unseres Landes ar...   \n",
       "47 2022-02-22 20:38:43  Die Lage ist ernst. Die Friedensordnung in Eur...   \n",
       "48 2022-02-19 08:32:43  Wir können täglich an die Opfer von #Hanau eri...   \n",
       "49 2022-02-17 09:42:26  Gestern Abend habe ich mit @POTUS Biden zur Si...   \n",
       "50 2022-02-16 17:37:29  Die #Corona-Regeln zeigen die erhoffte Wirkung...   \n",
       "51 2022-02-15 15:04:51  Ich habe Präsident #Putin in Moskau verdeutlic...   \n",
       "52 2022-02-15 14:52:09  Der Frieden und die Sicherheit in Europa sind ...   \n",
       "53 2022-02-14 12:09:58  Auf dem Weg in die #Ukraine. Heute in Kiew und...   \n",
       "\n",
       "    words  mentions  hashtags  urls  \n",
       "0      21         0         1     0  \n",
       "1      15         1         0     0  \n",
       "2      20         0         0     0  \n",
       "3      14         0         1     0  \n",
       "4      18         0         1     0  \n",
       "5      16         0         1     0  \n",
       "6      18         0         0     0  \n",
       "7      14         0         1     0  \n",
       "8      16         0         0     0  \n",
       "9      16         0         0     0  \n",
       "11     20         1         0     0  \n",
       "13     19         1         0     0  \n",
       "15     18         0         0     0  \n",
       "16     17         0         1     0  \n",
       "17     19         3         0     0  \n",
       "18     18         1         1     0  \n",
       "20     17         1         1     0  \n",
       "22     17         1         0     0  \n",
       "23     18         1         0     0  \n",
       "24     21         1         0     0  \n",
       "25     18         0         0     0  \n",
       "26     18         0         0     0  \n",
       "27     20         0         0     0  \n",
       "28     17         1         0     0  \n",
       "29     16         1         1     0  \n",
       "30     20         0         0     0  \n",
       "31     18         3         0     0  \n",
       "32     21         1         1     0  \n",
       "33     15         0         2     0  \n",
       "34     19         0         0     0  \n",
       "35     15         2         0     0  \n",
       "36     19         0         0     0  \n",
       "37     17         0         0     0  \n",
       "38     18         1         0     0  \n",
       "39     19         2         1     0  \n",
       "40     17         0         0     0  \n",
       "41     18         0         0     0  \n",
       "42     19         1         0     0  \n",
       "43     19         0         0     0  \n",
       "44     19         0         0     0  \n",
       "46     18         0         0     0  \n",
       "47     20         0         0     0  \n",
       "48     21         0         1     0  \n",
       "49     21         1         1     0  \n",
       "50     18         0         1     0  \n",
       "51     18         0         1     0  \n",
       "52     21         0         0     0  \n",
       "53     22         0         1     0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([tweets, pd.DataFrame(content_count)], axis=1);df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display descriptive statistics fdor words, mentions,\n",
    "# hashtags and urls\n",
    "#for key in content_count.keys():\n",
    "    #print()\n",
    "    #print('Descriptive statistics for {}'.format(key))\n",
    "    #print(df.groupby('onlinestorm')[key].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot them \n",
    "#for key in content_count.keys():\n",
    "\n",
    "    #bins = np.arange(df[key].min(), df[key].max() + 1)\n",
    "    #g = sns.FacetGrid(df, col='onlinestorm', height=5, hue='onlinestorm', palette=\"RdYlGn\")\n",
    "    #g = g.map(sns.distplot, key, kde=False, norm_hist=True, bins=bins)\n",
    "    #plt.savefig('images/Descriptive_stats_for_' + key + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above descriptive statistics, there are no noteworthy differences in terms of mentions, hashtags or urls during the online storm.\n",
    "However, the average number of words, per tweet, increased substantially during this period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: TEXT PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next steps, I retained only the tweets in English, avoiding duplicates. These are contained in a cvs file ('tweets_en.csv') with 6,546 tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step of our analysis will look deeper into the content of these tweets. It is time to apply some of the basic NLP operations, such as cleaning, tokenizing and lemmatizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NLTK (Natural Language Toolkit), one of the most popular NLP libraries for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re  # for regular expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "STOPLIST = set(stopwords.words('german'))\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + \\\n",
    "[\"-\", \"...\", \"”\", \"``\", \",\", \".\", \":\", \"''\",\"#\",\"@\"]\n",
    "\n",
    "# The NLTK lemmatizer and stemmer classes\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use the POS tagging from NLTK to retain only adjectives, verbs, adverbs \n",
    "# and nouns as a base for for lemmatization.\n",
    "def get_lemmas(tweet): \n",
    "    \n",
    "    # A dictionary to help convert Treebank tags to WordNet\n",
    "    treebank2wordnet = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r'}\n",
    "    \n",
    "    postag = ''\n",
    "    lemmas_list = []\n",
    "    \n",
    "    for word, tag in pos_tag(word_tokenize(tweet)):\n",
    "        if tag.startswith(\"JJ\")     \\\n",
    "            or tag.startswith(\"RB\") \\\n",
    "            or tag.startswith(\"VB\") \\\n",
    "            or tag.startswith(\"NN\"):\n",
    "                \n",
    "            try:\n",
    "                postag = treebank2wordnet[tag[:2]]\n",
    "            except:\n",
    "                postag = 'n'                \n",
    "                            \n",
    "            lemmas_list.append(lemmatizer.lemmatize(word.lower(), postag))    \n",
    "    \n",
    "    return lemmas_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now pre-process the tweets, following a pipeline of tokenization, filtering, case normalization and lemma extraction, including an overall cleaning of html and other codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to clean and filter the tokens in each tweet\n",
    "def clean_tweet(tokens):\n",
    "    \n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            if token not in STOPLIST:\n",
    "                if token[0] not in SYMBOLS:\n",
    "                    if not token.startswith('http'):\n",
    "                        if  '/' not in token:\n",
    "                            if  '-' not in token:\n",
    "                                filtered.append(token)\n",
    "                                        \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to lemmatization, I apply POS (part-of-speech) tagging to make sure that only the   adjectives, verbs, adverbs and nouns are retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts the lemmatization process\n",
    "def get_lemmatized(tweet):\n",
    "   \n",
    "    all_tokens_string = ''\n",
    "    filtered = []\n",
    "    tokens = []\n",
    "\n",
    "    # lemmatize\n",
    "    tokens = [token for token in get_lemmas(tweet)]\n",
    "    \n",
    "    # filter\n",
    "    filtered = clean_tweet(tokens)\n",
    "\n",
    "    # join everything into a single string\n",
    "    all_tokens_string = ' '.join(filtered)\n",
    "    \n",
    "    return all_tokens_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the lemmatized tweets and puts the result in an \"edited\" text column\n",
    "# for future use in this script\n",
    "edited = ''\n",
    "for i, row in tweets.iterrows():\n",
    "    edited = get_lemmatized(tweets.loc[i]['text'])\n",
    "    if len(edited) > 0:\n",
    "        tweets.at[i,'edited'] = edited\n",
    "    else:\n",
    "        tweets.at[i,'edited'] = None        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After lemmatization, some tweets may end up with the same words\n",
    "# Let's make sure that we have no duplicates\n",
    "tweets.drop_duplicates(subset=['edited'], inplace=True)\n",
    "tweets.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these text processing steps, and the removal of duplicates, the final sample counts 5,508 English-language tweets, with an average of 30 words (SD 12.5, ranging from 4 to 61 words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>18.1875</td>\n",
       "      <td>1.964106</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.25</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count     mean       std   min   25%   50%    75%   max\n",
       "word_count   48.0  18.1875  1.964106  14.0  17.0  18.0  19.25  22.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using apply/lambda to create a new column with the number of words in each tweet\n",
    "tweets['word_count'] = tweets.apply(lambda x: len(x['text'].split()),axis=1)\n",
    "t = pd.DataFrame(tweets['word_count'].describe()).T\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the result of our pre-processing, showing the difference between the original tweet (column \"text\") and the lemmatized, cleaned, tweet (column \"edited\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "      <th>edited</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-17 07:11:52</td>\n",
       "      <td>Ich wünsche Ihnen und Ihren Familien frohe #Os...</td>\n",
       "      <td>wünsche familien frohe ostern paar tage kraft ...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-14 16:40:32</td>\n",
       "      <td>Meine Solidarität gilt @Karl_Lauterbach: Jeder...</td>\n",
       "      <td>solidarität gild karl_lauterbach deutschland d...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-13 17:16:53</td>\n",
       "      <td>Sie haben Hilfe organisiert, Kräfte gebündelt ...</td>\n",
       "      <td>hilfe organisiert kräfte gebündelt gezeigt be ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-13 14:58:53</td>\n",
       "      <td>Verantwortung übernehmen heißt: Deutschland li...</td>\n",
       "      <td>verantwortung übernehmen heißt deutschland lie...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-12 18:59:51</td>\n",
       "      <td>Die Sanktionen gegen #Russland sind notwendig ...</td>\n",
       "      <td>sanktionen russland notwendig wirksam großer d...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime                                               text  \\\n",
       "0 2022-04-17 07:11:52  Ich wünsche Ihnen und Ihren Familien frohe #Os...   \n",
       "1 2022-04-14 16:40:32  Meine Solidarität gilt @Karl_Lauterbach: Jeder...   \n",
       "2 2022-04-13 17:16:53  Sie haben Hilfe organisiert, Kräfte gebündelt ...   \n",
       "3 2022-04-13 14:58:53  Verantwortung übernehmen heißt: Deutschland li...   \n",
       "4 2022-04-12 18:59:51  Die Sanktionen gegen #Russland sind notwendig ...   \n",
       "\n",
       "                                              edited  word_count  \n",
       "0  wünsche familien frohe ostern paar tage kraft ...          21  \n",
       "1  solidarität gild karl_lauterbach deutschland d...          15  \n",
       "2  hilfe organisiert kräfte gebündelt gezeigt be ...          20  \n",
       "3  verantwortung übernehmen heißt deutschland lie...          14  \n",
       "4  sanktionen russland notwendig wirksam großer d...          18  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sentiment analysis -- a growing sub-field of Natural Language Processing (NLP) -- I used VADER (Valence Aware Dictionary for Sentiment Reasoning), a rule-based system that performs specially well on social media data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most useful metric is the Compound score. It is calculated by a sum of the scores of each word, normalised to output values between -1, the most extreme negative score, and +1, the most extreme positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a complete understanding of how VADER computes its Compound score you have this conference paper [[2]](https://www.researchgate.net/publication/275828927_VADER_A_Parsimonious_Rule-based_Model_for_Sentiment_Analysis_of_Social_Media_Text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the VADER analyser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the puropose of the timeseries analysis, we must make sure that the tweets are all correctly sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['datetime']=pd.to_datetime(tweets['datetime']) \n",
    "tweets.sort_values('datetime', inplace=True, ascending=True)\n",
    "tweets = tweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid repetitions in our code, here are some plotting functions \n",
    "# that will be called often ...\n",
    "\n",
    "def plot_sentiment_period(df, info):\n",
    "    \n",
    "    # Using the mean values of sentiment for each period\n",
    "    df1 = df.groupby(df['datetime'].dt.to_period(info['period'])).mean()\n",
    "\n",
    "    df1.reset_index(inplace=True)\n",
    "    df1['datetime'] = pd.PeriodIndex(df1['datetime']).to_timestamp()\n",
    "    plot_df = pd.DataFrame(df1, df1.index, info['cols'])\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    ax = sns.lineplot(data=plot_df, linewidth = 3, dashes = False)\n",
    "    plt.legend(loc='best', fontsize=15)\n",
    "    plt.title(info['title'], fontsize=20)\n",
    "    plt.xlabel(info['xlab'], fontsize=15)\n",
    "    plt.ylabel(info['ylab'], fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/' + info['fname'])    \n",
    "    return\n",
    "\n",
    "\n",
    "def plot_fractions(props, title, fname):\n",
    "    \n",
    "    plt1 = props.plot(kind='bar', stacked=False, figsize=(16,5), colormap='Spectral') \n",
    "    plt.legend(bbox_to_anchor=(1.005, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('Online storm', fontweight='bold', fontsize=18)\n",
    "    plt.xticks(rotation=0,fontsize=14)\n",
    "    #plt.ylim(0, 0.5)\n",
    "    plt.ylabel('Fraction of Tweets', fontweight='bold', fontsize=18)\n",
    "    plt1.set_title(label=title, fontweight='bold', size=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/' + fname + '.png')\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def plot_frequency_chart(info):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    sns.set_context(\"notebook\", font_scale=1)    \n",
    "    ax = sns.barplot(x=info['x'], y=info['y'], data=info['data'], palette=(info['pal']))\n",
    "    ax.set_title(label=info['title'], fontweight='bold', size=18)\n",
    "    plt.ylabel(info['ylab'], fontsize=16)\n",
    "    plt.xlabel(info['xlab'], fontsize=16)\n",
    "    plt.xticks(rotation=info['angle'],fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/' + info['fname'])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling VADER\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get VADER Compound value for sentiment intensity\n",
    "tweets['sentiment_intensity'] = [analyzer.polarity_scores(v)['compound'] for v in tweets['edited']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of VADER are the positive, negative, and neutral ratios of sentiment. The most useful metric in VADER is the Compound score. Basically, it is calculated by a sum of the scores of each word, normalized to yeld values between -1, the most extreme negative score,  and +1, the most extreme positive. \n",
    "\n",
    "From this normalized score, I will then create a categorical variable (\"sentiment\"), with an output of positive, negative and neutral ratios of sentiment, using the following thresholds:\n",
    "\n",
    "* Positive sentiment : (compound score >= 0.05).\n",
    "\n",
    "* Neutral sentiment : (compound score > -0.05) and (compound score < 0.05).\n",
    "\n",
    "* Negative sentiment : (compound score <= -0.05)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the sentiment category\n",
    "def get_sentiment(intensity):\n",
    "    if intensity >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif (intensity >= -0.05) and (intensity < 0.05):\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "# Using pandas apply/lambda to speed up the process\n",
    "tweets['sentiment'] = tweets.apply(lambda x: get_sentiment(x['sentiment_intensity']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Online Storm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The next plot gives us a clear image of the “explosion” of contradictory sentiments in this period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tweets.loc[:,['datetime','sentiment_intensity']]\n",
    "# filter for these dates\n",
    "#df.set_index('datetime',inplace=True)\n",
    "#df=df[(df.index>='2020-03-12') & (df.index<'2020-03-18')]\n",
    "#df.plot(figsize=(12,6));\n",
    "#plt.ylabel('Compoud score', fontsize=15)\n",
    "#plt.xlabel('Tweets', fontsize=15)\n",
    "#plt.legend().set_visible(False)\n",
    "#plt.title('Sentiment on tweets with CureVac (12 March to 18 March)', fontsize=20)\n",
    "#plt.tight_layout()\n",
    "#sns.despine(top=True)\n",
    "#plt.savefig('images/Sentiment_during_onlinestorm.png')   \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this one will shows us a comparison of the sentiments before and during the online strom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'onlinestorm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yl/dt_q929j1hg64nfh_lmhldhw0000gn/T/ipykernel_89898/330179998.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Values are normalized to take into account the number of tweets in each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# of the two different periods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'onlinestorm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m plot_fractions(props,'Percentage of sentiments before and during the online storm',\n\u001b[1;32m      5\u001b[0m                'Fraction_sentiments_before_and_during_onlinestorm')\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   7629\u001b[0m         \u001b[0;31m# error: Argument \"squeeze\" to \"DataFrameGroupBy\" has incompatible type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7630\u001b[0m         \u001b[0;31m# \"Union[bool, NoDefault]\"; expected \"bool\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7631\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   7632\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7633\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_grouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m    890\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    860\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'onlinestorm'"
     ]
    }
   ],
   "source": [
    "# Values are normalized to take into account the number of tweets in each \n",
    "# of the two different periods\n",
    "props = tweets.groupby('onlinestorm')['sentiment'].value_counts(normalize=True).unstack()\n",
    "plot_fractions(props,'Percentage of sentiments before and during the online storm',\n",
    "               'Fraction_sentiments_before_and_during_onlinestorm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sentiment analysis, neutral tweets usually outnumber the negative or positive ones. This is what actually happened during the 6-year period in consideration. Moreover, research has been showing that scientists tend to use neutral language while communicating among peers, particularly in social media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture clearly changed during the 3-days online storm. Sentiments became less neutral, as it is also likely that the majority of the tweets come from a wider public. The percentage of positive tweets increased, suggesting higher expectations about a viable vaccine for coronavirus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also worth paying attention to an even stronger increase in the percentage of negative sentiments during the online storm. This calls for a deeper look at the data. That is what we will do now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our text is pre-processed, it is time to examine key patterns of word frequency in tweets posted before and during the online storm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need these imports for the wordcloud representation:\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "#from matplotlib.colors import makeMappingArray\n",
    "#from palettable.colorbrewer.diverging import Spectral_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter    # Counts the most common items in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_wordcloud(tokens, title, fname):\n",
    "    \n",
    "    tokens_upper = [token.upper() for token in tokens]\n",
    "\n",
    "    cloud_mask = np.array(Image.open(\"images/cloud_mask.png\"))\n",
    "    wordcloud = WordCloud(max_font_size=100, \n",
    "                          max_words=50, width=2500, \n",
    "                          height=1750,mask=cloud_mask, \n",
    "                          background_color=\"white\").generate(\" \".join(tokens_upper))\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('images/'+ fname  + '.png')   \n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_edited_string(edited_tweets):\n",
    "    \n",
    "    edited_string = ''\n",
    "    for row in edited_tweets:\n",
    "        edited_string = edited_string + ' ' + row\n",
    "        \n",
    "    return edited_string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigrams(trigrams, top_grams):\n",
    "    \n",
    "    grams_str = []\n",
    "    data = []\n",
    "\n",
    "    gram_counter = Counter(trigrams)\n",
    "    \n",
    "    for grams in gram_counter.most_common(10):\n",
    "        gram = ''\n",
    "        grams_str = grams[0]\n",
    "        grams_str_count = []\n",
    "        for n in range(0,3):\n",
    "            gram = gram + grams_str[n] + ' '\n",
    "        grams_str_count.append(gram)\n",
    "        grams_str_count.append(grams[1])\n",
    "        data.append(grams_str_count)\n",
    "        print(grams_str_count)\n",
    "\n",
    "    df = pd.DataFrame(data, columns = ['Grams', 'Count'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets before the online storm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a look at the 20 most frequent words in tweets before the online storm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'onlinestorm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'onlinestorm'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yl/dt_q929j1hg64nfh_lmhldhw0000gn/T/ipykernel_89898/1578923912.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Filtering the tweets of the 6 years before the online storm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'onlinestorm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Join all the edited tweets in one single string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mjoined_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin_edited_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edited'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'onlinestorm'"
     ]
    }
   ],
   "source": [
    "# Filtering the tweets of the 6 years before the online storm\n",
    "df = tweets[tweets['onlinestorm'] == False]\n",
    "\n",
    "# Join all the edited tweets in one single string\n",
    "joined_string = join_edited_string(df['edited'])\n",
    "\n",
    "# Get tokens\n",
    "tokens = joined_string.split(' ')\n",
    "\n",
    "# get trigrams\n",
    "trigrams = nltk.trigrams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yl/dt_q929j1hg64nfh_lmhldhw0000gn/T/ipykernel_89898/780117303.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot word frequency during online storm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'freq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m info = {'data': df_counter, 'x': 'freq', 'y': 'word',\n\u001b[1;32m      5\u001b[0m        \u001b[0;34m'xlab'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ylab'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "# plot word frequency during online storm\n",
    "word_counter = Counter(tokens)\n",
    "df_counter = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\n",
    "info = {'data': df_counter, 'x': 'freq', 'y': 'word',\n",
    "       'xlab': 'Count', 'ylab': 'Words', 'pal':'viridis',\n",
    "       'title': 'Most frequent words before online storm',\n",
    "       'fname':'word_frequency_before_onlinestorm.png',\n",
    "       'angle': 90}\n",
    "plot_frequency_chart(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the 10 most frequent trigrams (sequences of 3 consecutive words) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trigrams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yl/dt_q929j1hg64nfh_lmhldhw0000gn/T/ipykernel_89898/609154296.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot trigram frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_trigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m info = {'data': df_trigrams, 'x': 'Grams', 'y': 'Count',\n\u001b[1;32m      4\u001b[0m        \u001b[0;34m'xlab'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Trigrams'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ylab'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m        \u001b[0;34m'title'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Most frequent trigrams before online storm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trigrams' is not defined"
     ]
    }
   ],
   "source": [
    "# plot trigram frequency\n",
    "df_trigrams = get_trigrams(trigrams, 10)\n",
    "info = {'data': df_trigrams, 'x': 'Grams', 'y': 'Count',\n",
    "       'xlab': 'Trigrams', 'ylab': 'Count', 'pal':'viridis',\n",
    "       'title': 'Most frequent trigrams before online storm',\n",
    "       'fname':'trigrams_frequency_before_onlinestorm.png',\n",
    "       'angle': 40}\n",
    "plot_frequency_chart(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the wordcloud ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_wordcloud(tokens, 'Wordcloud of most frequent words before online storm',\n",
    "                 'WordCloud_before_onlinestorm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some noteworthy features in these plots:\n",
    "\n",
    "- Along with ‘gate’ (ie., Bill Gates), the most frequent words in 6 years of tweets are ‘develop’, ‘therapeutic’, ‘deal’ and 'news'. Unsurprisingly, these were times when tweets were used mainly as public relations devices to communicate the core business of CureVac, a vaccine maker funded by the Melinda Gate Foudation.\n",
    "- Immediatly follows ‘Collaboration’, the next most frequent word, reflecting in this way the key importance of partnerships in the strategy of the company, followed by ‘new’, as a evidence of CureVac's concern with innovation.\n",
    "- The trigrams reinforce these trends, and with a stronger focus on collaboration. These are mainly about 'next generation in health care' and 'pharmaceutical deals' carried out in ‘broad strategic collaborations’.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets during the online storm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to examine what happened on those \"stormy\" three days, after the 15th March 2020 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the tweets of the 3 days of the online storm\n",
    "#df =tweets[tweets['onlinestorm']]\n",
    "\n",
    "# Join all the edited tweets in one single string\n",
    "joined_string = join_edited_string(tweets['edited'])\n",
    "\n",
    "# Get tokens\n",
    "tokens = joined_string.split(' ')\n",
    "\n",
    "# get trigrams\n",
    "trigrams = nltk.trigrams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yl/dt_q929j1hg64nfh_lmhldhw0000gn/T/ipykernel_89898/2252904825.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot word frequency during online storm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'freq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m info = {'data': df_counter, 'x': 'freq', 'y': 'word',\n\u001b[1;32m      5\u001b[0m        \u001b[0;34m'xlab'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ylab'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'inferno'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "# plot word frequency during online storm\n",
    "word_counter = Counter(tokens)\n",
    "df_counter = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\n",
    "info = {'data': df_counter, 'x': 'freq', 'y': 'word',\n",
    "       'xlab': 'Count', 'ylab': 'Words', 'pal':'inferno',\n",
    "       'title': 'Most frequent words during online storm',\n",
    "       'fname':'word_frequency_during_onlinestorm.png',\n",
    "       'angle': 90}\n",
    "plot_frequency_chart(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yl/dt_q929j1hg64nfh_lmhldhw0000gn/T/ipykernel_89898/1547669712.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot trigrams frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_trigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m info = {'data': df_trigrams, 'x': 'Grams', 'y': 'Count',\n\u001b[1;32m      4\u001b[0m        \u001b[0;34m'xlab'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Trigrams'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ylab'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'inferno'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m        \u001b[0;34m'title'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Most frequent trigrams during online storm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/yl/dt_q929j1hg64nfh_lmhldhw0000gn/T/ipykernel_89898/1246916249.py\u001b[0m in \u001b[0;36mget_trigrams\u001b[0;34m(trigrams, top_grams)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgram_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrigrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgrams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgram_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "# plot trigrams frequency\n",
    "df_trigrams = get_trigrams(trigrams, 10)\n",
    "info = {'data': df_trigrams, 'x': 'Grams', 'y': 'Count',\n",
    "       'xlab': 'Trigrams', 'ylab': 'Count', 'pal':'inferno',\n",
    "       'title': 'Most frequent trigrams during online storm',\n",
    "       'fname':'trigrams_frequency_during_onlinestorm.png',\n",
    "       'angle': 40}\n",
    "plot_frequency_chart(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yl/dt_q929j1hg64nfh_lmhldhw0000gn/T/ipykernel_89898/750753118.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m display_wordcloud(tokens, 'Wordcloud of most frequent words during online storm',\n\u001b[0m\u001b[1;32m      2\u001b[0m                  'WordCloud_during_onlinestorm')\n",
      "\u001b[0;32m/var/folders/yl/dt_q929j1hg64nfh_lmhldhw0000gn/T/ipykernel_89898/582789731.py\u001b[0m in \u001b[0;36mdisplay_wordcloud\u001b[0;34m(tokens, title, fname)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtokens_upper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcloud_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"images/cloud_mask.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     wordcloud = WordCloud(max_font_size=100, \n\u001b[1;32m      7\u001b[0m                           \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "display_wordcloud(tokens, 'Wordcloud of most frequent words during online storm',\n",
    "                 'WordCloud_during_onlinestorm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What we've seen above shows obvious differences from the main stream life of CureVac on Twitter:\n",
    "- The top word is no longer ‘gate’ but ‘trump’ (ie., Donald Trump), immediately followed by 'coronavirus'. \n",
    "- Gone are the days of collaboration for a next generation of new and innovative therapies. - - ‘Exclusive’ takes the lead, ‘collaboration’ is out of the league. \n",
    "- The most frequent trigram is ‘try buy exclusive’. These are now times for ‘exclusive large gain’. \n",
    "- ‘Buy’ becames a new key word. ‘large sum money’ and ‘offer large sum’ are now the top trigrams in the chart. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: LDA topics extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA (Latent Dirichlet Allocation) is an unsupervised machine learning technique that is increasingly popular in most text mining toolkits. You can find [here](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158) a comprehensive article on the subject, published on Medium, covering extensively the assumptions and the math behind the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied LDA in the two differernt periods (before and during the CureVac online fire-storm) to check whether the findings corroborate the trends that we have seen in our previous analysis of the word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using here Susan Li's functions to get the top words from a topic:\n",
    "\n",
    "def get_keys(topic_matrix):\n",
    "    '''\n",
    "    returns an integer list of predicted topic \n",
    "    categories for a given topic matrix\n",
    "    '''\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n",
    "\n",
    "def keys_to_counts(keys):\n",
    "    '''\n",
    "    returns a tuple of topic categories and their \n",
    "    accompanying magnitudes for a given list of keys\n",
    "    '''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)\n",
    "\n",
    "def get_top_n_words(n, n_topics, keys, document_term_matrix, tfidf_vectorizer):\n",
    "    '''\n",
    "    returns a list of n_topic strings, where each string contains the n most common \n",
    "    words in a predicted category, in order\n",
    "    '''\n",
    "    top_word_indices = []\n",
    "    for topic in range(n_topics):\n",
    "        temp_vector_sum = 0\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:, index] = 1\n",
    "            the_word = tfidf_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            try:\n",
    "                topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
    "            except:\n",
    "                pass\n",
    "        top_words.append(\", \".join(topic_words))         \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "And here is a function for topics extraction using LDA, in which I produce a dataframe with the topics and their top words to facilitate the plotting that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA topics\n",
    "def get_topics(edited, n_topics, n_words):\n",
    "\n",
    "    eds = edited.values\n",
    "    \n",
    "    vec = TfidfVectorizer(use_idf=True, smooth_idf=True)\n",
    "    document_term_matrix = vec.fit_transform(eds)\n",
    "    \n",
    "    model = LatentDirichletAllocation(n_components=n_topics)\n",
    "    topic_matrix = model.fit_transform(document_term_matrix)\n",
    "    \n",
    "    keys = get_keys(topic_matrix)\n",
    "    categories, counts = keys_to_counts(keys)\n",
    "    top_n_words = get_top_n_words(n_words, n_topics, keys, document_term_matrix, vec)\n",
    "\n",
    "    topics = ['Topic {}: \\n'.format(i + 1) + top_n_words[i] for i in categories]\n",
    "    data=[]\n",
    "    for i, topic in enumerate(topics):\n",
    "        tmp = []\n",
    "        tmp.append(topic)\n",
    "        tmp.append(counts[i])\n",
    "        data.append(tmp)\n",
    "    df_topics = pd.DataFrame(data, columns = ['Topics', 'Count'])\n",
    "    \n",
    "    return df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics before the online storm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'onlinestorm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'onlinestorm'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yl/dt_q929j1hg64nfh_lmhldhw0000gn/T/ipykernel_89898/1688715542.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Filtering the tweets of the 6 years before the online storm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'onlinestorm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# LDA topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edited'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'onlinestorm'"
     ]
    }
   ],
   "source": [
    "# Filtering the tweets of the 6 years before the online storm\n",
    "df = tweets[tweets['onlinestorm'] == False]\n",
    "\n",
    "# LDA topics\n",
    "df_topics = get_topics(df['edited'], 5, 5)\n",
    "info = {'data': df_topics, 'x': 'Topics', 'y': 'Count',\n",
    "       'xlab': 'Topics', 'ylab': 'Count', 'pal':'viridis',\n",
    "       'title': 'LDA Topics before Online Storm',\n",
    "       'fname':'LDA_Topics_before_onlinestorm.png',\n",
    "       'angle': 40}\n",
    "plot_frequency_chart(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics during the online storm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'onlinestorm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'onlinestorm'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yl/dt_q929j1hg64nfh_lmhldhw0000gn/T/ipykernel_89898/2980881263.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Filtering the tweets of the 3 days of the online storm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'onlinestorm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# LDA topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edited'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'onlinestorm'"
     ]
    }
   ],
   "source": [
    "# Filtering the tweets of the 3 days of the online storm\n",
    "df =tweets[tweets['onlinestorm']]\n",
    "\n",
    "# LDA topics\n",
    "df_topics = get_topics(df['edited'], 5, 5)\n",
    "info = {'data': df_topics, 'x': 'Topics', 'y': 'Count',\n",
    "       'xlab': 'Topics', 'ylab': 'Count', 'pal':'inferno',\n",
    "       'title': 'Main Topics during Online Storm',\n",
    "       'fname':'LDA_Topics_during_onlinestorm.png',\n",
    "       'angle': 40}\n",
    "plot_frequency_chart(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word of caution must be exercised here. LDA topics are “imaginary” (latent) topics, frequently overlapping, and a clear distinction is not always achievable. \n",
    "Nevertheless, a comparison between topics before and during the online storm leaves no doubts about contrasting trends. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a period of six years, the major topic emerging from tweets is about collaborative developments. In contrast, during the online storm, in a period of three days, the two topics that stand out are clearly about the alleged attempt of the USA president to ensure the exclusive rights for the coronavirus vaccine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Emotion analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I drew from Robert Plutchik’s wheel of basic emotions an attempt to uncover the presence of the seven lexical units for anger, fear, sadness, disgust, anticipation, joy and surprise [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Plutchiks-wheel-of-emotions.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import termcolor\n",
    "import sys\n",
    "from termcolor import colored, cprint\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic approach is to create a matrix of tweets and emotions to connect each word in the tweet to one or more emotions. I applied the National Research Council Canada (NRC) lexicon, a dictionary of 14,182 words and 10 columns rows, each corresponding to positive and negative sentiment plus eight emotions. For a deeper understanding of the NRC lexicon read this article [[4]](https://www.researchgate.net/publication/256199465_Crowdsourcing_a_Word-Emotion_Association_Lexicon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data from the NCR lexicon\n",
    "ncr = pd.read_csv('NCR-lexicon-german.csv',\n",
    "                        encoding= 'unicode_escape', sep =';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a list of the emotions\n",
    "emotions = ['Anger', 'Anticipation','Disgust','Fear', 'Joy','Sadness', 'Surprise', 'Trust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all the edited tweets in one single string\n",
    "joined_string = join_edited_string(tweets['edited'])\n",
    "\n",
    "# Get tokens\n",
    "tokens = joined_string.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build now two dictionaries with indexes and unique words, for future reference\n",
    "\n",
    "unique_words = set(tokens)\n",
    "\n",
    "word_to_ind = dict((word, i) for i, word in enumerate(unique_words))\n",
    "ind_to_word = dict((i, word) for i, word in enumerate(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emotions_period(df, cols, title, fname, period = 'h' ):\n",
    "\n",
    "    df1 = df.groupby(df['datetime'].dt.to_period(period)).mean()\n",
    "\n",
    "    df1.reset_index(inplace=True)\n",
    "    df1['datetime'] = pd.PeriodIndex(df1['datetime']).to_timestamp()\n",
    "    plot_df = pd.DataFrame(df1, df1.index, cols)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    ax = sns.lineplot(data=plot_df, linewidth = 3,dashes = False)\n",
    "    plt.legend(loc='best', fontsize=15)\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.xlabel('Time (hours)', fontsize=15)\n",
    "    plt.ylabel('Z-scored Emotions', fontsize=15)\n",
    "    plt.savefig('images/'+ fname  + '.png')       \n",
    "    \n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_emotions(df, emotions, col):\n",
    "\n",
    "    df_tweets = df.copy()\n",
    "    df_tweets.drop(['sentiment','sentiment_intensity'], axis=1, inplace=True)\n",
    "    \n",
    "    emo_info = {'emotion':'' , 'emo_frq': defaultdict(int) }    \n",
    "\n",
    "    list_emotion_counts = []\n",
    "\n",
    "    # creating a dictionary list to hold the frequency of the words\n",
    "    # contributing to the emotions\n",
    "    for emotion in emotions:\n",
    "        emo_info = {}\n",
    "        emo_info['emotion'] = emotion\n",
    "        emo_info['emo_frq'] = defaultdict(int)\n",
    "        list_emotion_counts.append(emo_info)\n",
    "    \n",
    "    # bulding a zeros matrix to hold the emotions data\n",
    "    df_emotions = pd.DataFrame(0, index=df.index, columns=emotions)\n",
    "\n",
    "    \n",
    "    # stemming the word to facilitate the search in NRC\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    # iterating in the tweets data set\n",
    "    for i, row in df_tweets.iterrows(): # for each tweet ...\n",
    "        tweet = word_tokenize(df_tweets.loc[i][col])\n",
    "        for word in tweet: # for each word ...\n",
    "            word_stemmed = stemmer.stem(word.lower())\n",
    "            # check if the word is in NRC\n",
    "            result = ncr[ncr.English == word_stemmed]\n",
    "            # we have a match\n",
    "            if not result.empty:\n",
    "                # update the tweet-emotions counts\n",
    "                for idx, emotion in enumerate(emotions):\n",
    "                    df_emotions.at[i, emotion] += result[emotion]\n",
    "                    \n",
    "                    # update the frequencies dictionary list\n",
    "                    if result[emotion].any():\n",
    "                        try:\n",
    "                            list_emotion_counts[idx]['emo_frq'][word_to_ind[word]] += 1\n",
    "                        except:\n",
    "                            continue\n",
    "    \n",
    "    # append the emotions matrix to the tweets data set\n",
    "    df_tweets = pd.concat([df_tweets, df_emotions], axis=1)\n",
    "\n",
    "    return df_tweets, list_emotion_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of words to highlight \n",
    "def get_words(word_list, emotions):\n",
    "    \n",
    "    words_emotion_idx = []\n",
    "    \n",
    "    for i, word in enumerate(word_list):\n",
    "        word = stemmer.stem(word.lower())\n",
    "        result = ncr[ncr.English == word]\n",
    "        if not result.empty:\n",
    "            for emotion in emotions:\n",
    "                if result[emotion].any() > 0:\n",
    "                    words_emotion_idx.append(i)\n",
    "                \n",
    "    return words_emotion_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def get_top_emotion_words(word_counts, n = 5):\n",
    "\n",
    "    # Here I map the numpy array \"words\" with the index and word frequency\n",
    "    words = np.zeros((len(word_counts), 2), dtype=int)\n",
    "    for i, w in enumerate(word_counts):\n",
    "        words[i][0] = w\n",
    "        words[i][1] = word_counts[w]\n",
    "\n",
    "    # From the indexes generated by the argsort function, \n",
    "    # I get the order of the top n words in the list\n",
    "    top_words_idx = np.flip(np.argsort(words[:,1])[-n:],0)\n",
    "\n",
    "    # The resulting indexes are now used as keys in the dic to get the words\n",
    "    top_words = [words[ind][0] for ind in top_words_idx]\n",
    "        \n",
    "    return words, top_words, top_words_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is now the function to display and highlight \n",
    "# the words associated to specific emotions\n",
    "def print_colored_emotions(tweets, emotions, color, on_color):\n",
    "    \n",
    "    for tweet in tweets:\n",
    "\n",
    "        word_list = word_tokenize(tweet)\n",
    "\n",
    "        word_emotion_idx = get_words(word_list, emotions)\n",
    "\n",
    "        for i, w in enumerate(word_list):\n",
    "            if i in word_emotion_idx:\n",
    "                w=colored(w, color=color, on_color=on_color)\n",
    "            print(w, end='') \n",
    "            print(' ', end='')  \n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting words to emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot convert the series to {converter}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert the series to <class 'int'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yl/dt_q929j1hg64nfh_lmhldhw0000gn/T/ipykernel_89898/2730035904.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Be patient, this will take some time ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_emo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_emotion_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tweet_emotions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'edited'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Preparing for time series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/yl/dt_q929j1hg64nfh_lmhldhw0000gn/T/ipykernel_89898/3213676171.py\u001b[0m in \u001b[0;36mget_tweet_emotions\u001b[0;34m(df, emotions, col)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;31m# update the tweet-emotions counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0mdf_emotions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0;31m# update the frequencies dictionary list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2284\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2235\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not enough indexers for scalar access (setting)!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2237\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_value\u001b[0;34m(self, index, col, value, takeable)\u001b[0m\n\u001b[1;32m   3824\u001b[0m             \u001b[0mvalidate_numeric_casting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3826\u001b[0;31m             \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3827\u001b[0m             \u001b[0;31m# Note: trying to use series._set_value breaks tests in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3828\u001b[0m             \u001b[0;31m#  tests.frame.indexing.test_indexing and tests.indexing.test_partial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# We are using the NCR lexicon to associate words to emotions \n",
    "# Be patient, this will take some time ...\n",
    "\n",
    "df_emo, list_emotion_counts = get_tweet_emotions(tweets, emotions, 'edited')\n",
    "\n",
    "# Preparing for time series\n",
    "#df_emo['datetime']= pd.to_datetime(df_emo['datetime']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better understanding of the word-emotions associations, I produce here the plots showing what are the 10 words that contributed the most for each of the 8 emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_emotion_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yl/dt_q929j1hg64nfh_lmhldhw0000gn/T/ipykernel_89898/471790996.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# This is the dict that holds the top 10 words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_words_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_emotion_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_emotion_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emo_frq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     info = {'values' : [words[ind][1] for ind in top_words_indices], \n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_emotion_counts' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the 10 words that contribute the most for each of the 8 emotions\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(15, 25), frameon=False) \n",
    "plt.box(False)\n",
    "plt.axis('off')\n",
    "plt.subplots_adjust(hspace = 1.6)\n",
    "counter = 0\n",
    "\n",
    "for i, emotion in enumerate(emotions): # for each emotioin\n",
    "\n",
    "    # This is the dict that holds the top 10 words \n",
    "    words, top_words, top_words_indices = get_top_emotion_words(list_emotion_counts[i]['emo_frq'], 10)\n",
    "    \n",
    "    info = {'values' : [words[ind][1] for ind in top_words_indices], \n",
    "                      'labels' : [ind_to_word[word] for word in top_words]}\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.set_context(\"notebook\", font_scale=1.25)\n",
    "    ax = fig.add_subplot(4, 2, counter+1) # plot 2 charts in each of the 4 rows\n",
    "    sns.despine()\n",
    "    ax = sns.barplot(x='labels', y='values', data=info, palette=(\"cividis\"))\n",
    "    plt.ylabel('Top words', fontsize=12)\n",
    "    ax.set_title(label=str('Emotion: ') + emotion, fontweight='bold', size=13)\n",
    "    plt.xticks(rotation=45, fontsize=14)\n",
    "    counter += 1\n",
    "\n",
    "axs.set_title(label='\\nTop 10 words for each emotion\\n', \n",
    "             fontweight='bold', size=20, pad=40)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/Top10_words_per_emotion.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some authors, isolated emotions might not be the best granullarity for analysis. Skillicorn (2019) and colleagues prefer to aggregate emotions into positive and negative emotions [5]. Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating negative and positive emotions\n",
    "df_emo['neg_emotions'] = df_emo['Sadness'] + df_emo['Fear'] + df_emo['Disgust'] + df_emo['Anger']\n",
    "df_emo['pos_emotions'] = df_emo['Joy'] + df_emo['Anticipation'] + df_emo['Trust'] + df_emo['Surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emo['total_neg_emotions'] = df_emo['neg_emotions'].apply(lambda x: x > 0)\n",
    "df_emo['total_pos_emotions'] = df_emo['pos_emotions'].apply(lambda x: x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use here the pandas groupby feature to obtain a normalized account of the emotions as a proportion that takes into account the number of tweets in each of the two periods (before and during the online storm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = df_emo.groupby('onlinestorm')['total_neg_emotions'].value_counts(normalize=True).unstack()\n",
    "props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that during the online storm period, negative emotions are present in 42 per cent of the tweets, whereas previously only 11% of the tweets included negative emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can spot it more clearly in the following chart ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "plot_fractions(props,'Percentage of tweets with negative emotions','Percentage_of_Tweets_with_negative_emotions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converselly, when it comes to positive emotions, we witness also an increase in the proportion of tweets with these emotions during online storm (84 per cent). But it is not such a sharp change when compared to the 61 per cent before the online storm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = df_emo.groupby('onlinestorm')['total_pos_emotions'].value_counts(normalize=True).unstack()\n",
    "props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fractions(props,'Percentage of tweets with positive emotions','Percentage_of_Tweets_with_positive_emotions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word - emotion connections in the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us have a feeling of how things work behind thew scenes, I wrote a function (print_colored_emotions) to display the words connected to negative (red) and positive (green) emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to acknowledge that I am not giving any kind of emotion score to the tweets (that would be another undertaking all together). I am just locating the word-emotion connections within the tweets, since a tweet may depict more than one emotion (or cluster of emotions) – and they usually do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some negative emotions ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_emo[df_emo['Sadness'] > 3]\n",
    "print_colored_emotions(df['text'], ['Disgust','Sadness','Anger','Fear'], 'white', 'on_red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here some positive ones ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_emo[df_emo['Anticipation'] > 4]\n",
    "print_colored_emotions(df['text'], ['Joy','Trust','Anticipation'], 'white', 'on_green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of emotions in relation to number of tweets, before and during the online storm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_emo.groupby(df_emo['onlinestorm'])[emotions].apply(lambda x:( x.sum()/x.count())*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.index = ['before_onlinestorm', 'during_onlinestorm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ =df1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.set_title(label='Comparing percentage of emotion-related words before and during online storm\\n', fontweight='bold', size=18)\n",
    "df_.reset_index().plot(\n",
    "    x=\"index\", y=[\"before_onlinestorm\", \"during_onlinestorm\"], kind=\"bar\", ax=ax\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Emotions\",fontsize = 16)\n",
    "plt.ylabel(\"Percentage of emotion-related words\",fontsize = 16)\n",
    "plt.xticks(rotation=45,fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/Percentage_emotions_before_and_during_onlinestorm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying a Z-score normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In another effort to normalize the emotion scores, I am using the Z-score, instead of the mere counts of word-emotion connections, because these are heavily affected by the number of tweets in each period in consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z-score tells us how many standard deviations an individual value is from the mean, and is calculated with following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/zscore.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the pandas’ apply function to calculate the z-score of each individual value in all the 8 columns of emotions in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zscore = df_emo.groupby(df_emo['onlinestorm'])[emotions].apply(lambda x:(x - x.mean()) / x.std())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emo = pd.concat([df_emo[['datetime','text','edited', 'onlinestorm']], df_zscore], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dynamics of emotions during the online storm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our normalized values, we can now have a more precise view of the way emotions evolved, by the hour, during the 3 days of the online storm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a mixture of all the emotions during online storm ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_emotions_period(df_emo[df_emo['onlinestorm']], emotions,\n",
    "                    'Emotions time series during online storm','Timeseries_Emotions_OnlineStorm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how each of the emotions evolved during this period ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting emotions during online storm\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(15, 25), frameon=False) \n",
    "plt.box(False)\n",
    "plt.axis('off')\n",
    "plt.subplots_adjust(hspace = 1.6)\n",
    "counter = 0\n",
    "\n",
    "df = df_emo[df_emo['onlinestorm']]\n",
    "df1 = df.groupby(df['datetime'].dt.to_period('h')).mean()\n",
    "df1.reset_index(inplace=True)\n",
    "df1['datetime'] = pd.PeriodIndex(df1['datetime']).to_timestamp()\n",
    "\n",
    "for i, emotion in enumerate(emotions): # for each emotion\n",
    "    \n",
    "    emo = []  \n",
    "    emo.append(emotion)\n",
    "    plot_df = pd.DataFrame(df1, df1.index, emo)\n",
    "    \n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.set_context(\"notebook\", font_scale=1.25)\n",
    "    ax = fig.add_subplot(4, 2, counter+1) # plot 2 charts in each of the 4 rows\n",
    "    sns.despine()\n",
    "    ax = sns.lineplot(data=plot_df, linewidth = 3,dashes = False)\n",
    "    plt.ylabel('Time by the hour', fontsize=12)\n",
    "    ax.set_title(label=str('Emotion: ') + emotion, fontweight='bold', size=13)\n",
    "    counter += 1\n",
    "\n",
    "axs.set_title(label='\\nPlot for each emotion during online storm\\n', \n",
    "             fontweight='bold', size=20, pad=40)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/Emotions_during_onlinestorm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of looking at it is by plotting contrasts of emotions, like joy and sadness ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_emotions_period(df_emo[df_emo['onlinestorm']], ['Joy', 'Sadness'],\n",
    "                    'Joy and Sadness time series during online storm','Joy_Sadness_Emotions_OnlineStorm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now trust and fear ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_emotions_period(df_emo[df_emo['onlinestorm']], ['Trust', 'Fear'],\n",
    "                    'Trust and Fear time series during online storm','Trust_Fear_Emotions_OnlineStorm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Hersausen, D., et al (2019) [Detecting, Preventing, and Mitigating Online\n",
    "Firestorms in Brand Communities](https://www.researchgate.net/publication/330385319_Detecting_Preventing_and_Mitigating_Online_Firestorms_in_Brand_Communities). *Journal of Marketing*, 83(51)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] C.J. Hutto, Eric Gilbert (2014) [VADER: A Parsimonious Rule-based Model\n",
    "for Sentiment Analysis of Social Media Text, Association for the Advancement\n",
    "of Artificial Intelligence](https://www.researchgate.net/publication/275828927_VADER_A_Parsimonious_Rule-based_Model_for_Sentiment_Analysis_of_Social_Media_Text). *Association for the Advancement\n",
    "of Artificial Intelligence.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] Plutchik, R. (2001) The Nature of Emotions. American Scientist, vol 89."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] Saif, M., Turney, P. (2013) [Crowdsourcing a Word-Emotion Association Lexicon](https://www.researchgate.net/publication/256199465_Crowdsourcing_a_Word-Emotion_Association_Lexicon), *Computational Intelligence*, 29(3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] Skillicorn, D., et al (2019) Measuring Human Emotion in Short Documents\n",
    "to Improve Social Robot and Agent Interactions, Canadian AI 2019: Advances\n",
    "in Artificial Intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
