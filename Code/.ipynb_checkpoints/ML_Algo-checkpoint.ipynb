{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a7d28f-c15e-4f62-959d-c893b9231f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#df2 = pd.read_csv(\"./Training_Dataframes/esken30ktweetswithemotions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "962b3619-d633-43bf-8b99-efcbd93829df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/swlh/tweet-sentiment-analysis-using-python-for-complete-beginners-4aeb4456040\n",
    "# Turning all Tweets into Vectors for later Machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def turnTweetsIntoVector(dataframe,max_features_IN = None):\n",
    "    dataframe.drop([\"Unnamed: 0\"],axis = 1, inplace = True)\n",
    "    keyString = dataframe.keys()\n",
    "    keyString += \"Input\"\n",
    "    dataframe.columns = keyString\n",
    "    listOfTweets = dataframe[\"editedInput\"].tolist()\n",
    "    docs = np.array(listOfTweets)\n",
    "    cv=CountVectorizer(max_features = max_features_IN, binary = True)\n",
    "    oneHotCv = cv.fit_transform(docs)\n",
    "    oneHotArray = oneHotCv.toarray()\n",
    "    oneHotArray.shape\n",
    "    oneHotDf = pd.DataFrame(oneHotArray)\n",
    "    #Naming the Columns accordingly for humans to read    \n",
    "    oneHotDf.columns = cv.get_feature_names_out()\n",
    "    #print(oneHotDf.head())\n",
    "    #print(oneHotDf.info())\n",
    "    #Normalizing Emotions    \n",
    "    longDf = pd.concat([dataframe,oneHotDf], axis = 1)\n",
    "    #dir(CountVectorizer)\n",
    "    #print(longDf.describe())\n",
    "    #longDf.head()\n",
    "    for emotion in longDf[longDf.keys()[12:20]]:\n",
    "        longDf[emotion] = np.where(longDf[emotion]>=1, 1, 0)\n",
    "    \n",
    "    # ----- To check output\n",
    "    \n",
    "    #proving, that all words in the tweet are marked appropriatly\n",
    "    #print(longDf.iloc[0][\"editedInput\"])\n",
    "    #print(longDf.iloc[0][longDf.iloc[0][\"editedInput\"].split()])\n",
    "    #Shows that keys[0:24] are tweet data, thats not part of the NLP\n",
    "    #print(longDf.keys()[:27])\n",
    "    #print(longDf.keys()[:24])\n",
    "    # -----\n",
    "\n",
    "    return longDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c2c66b7-8602-44b4-b666-a22af7e9c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is already part of old code\n",
    "import re\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+\\s?', '', text) #Removed Mentions\n",
    "    text = re.sub(r'#', '', text) #Removed #\n",
    "    text = re.sub(r'(.)1+', r'1', text) #cleaned single letters\n",
    "    text = re.sub('((www.[^s]+)|(https?://[^s]+))','',text) #Removes links\n",
    "    text = re.sub('@','',text) #Remove @\n",
    "    text = re.sub('-','',text) #Remove -\n",
    "    text = re.sub('ä','ae',text) #Remove ä\n",
    "    text = re.sub('Ä','Ae',text) #Remove Ä\n",
    "    text = re.sub('ö','oe',text) #Remove Ä\n",
    "    text = re.sub('Ö','Oe',text) #Remove Ä\n",
    "    text = re.sub('ü','ue',text) #Remove Ä\n",
    "    text = re.sub('Ü','Ue',text) #Remove Ä\n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8493cfda-0051-4df2-8cdc-c037ab39bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_toVec(sentence,goalDF):\n",
    "    #Preparing Input text\n",
    "    cleanSentence = sentence.lower()\n",
    "    cleanSentence = clean(cleanSentence)\n",
    "    cleansentene = stemming_on_text(cleanSentence)\n",
    "    \n",
    "    #Preparing EmptyDF\n",
    "    emptyList = [0]*len(goalDF.keys())\n",
    "    #print(emptyList)\n",
    "    #print(\"Len EmptyList: \" + str(len(emptyList)))\n",
    "    #emptyDF = goalDF.iloc[0:0].copy()\n",
    "    #emptyDF = emptyDF.append(emptyList)\n",
    "    #emptyDF = emptyDF.append(pd.DataFrame(emptyList, columns = goalDF.keys()), ignore_index = True)\n",
    "    emptyDF = pd.DataFrame(columns = goalDF.keys())\n",
    "    #emptyDF.append(pd.Series(), ignore_index = True)\n",
    "    emptyDF.loc[len(emptyDF)] = emptyList\n",
    "    #print(emptyDF)\n",
    "    #emptyDF = pd.DataFrame(emptyList, columns = goalDF.keys())\n",
    "    #print(\"LenEmpty DF: \" + str(len(emptyDF)) + \" Len Keys Empty DF: \" + str(len(emptyDF.keys())))\n",
    "    \n",
    "    emptyDF[\"textInput\"][0] = sentence\n",
    "    emptyDF[\"editedInput\"][0] = cleanSentence\n",
    "    #keys = list(emptyDF.keys()[0:10])\n",
    "    #print(\"---\")\n",
    "    #print(\"New LenEmpty DF: \" + str(len(emptyDF)) + \" Len Keys Empty DF: \" + str(len(emptyDF.keys())))\n",
    "    #print(emptyDF[\"textInput\"].tolist())\n",
    "    listOfWords = cleanSentence.split()\n",
    "    for word in listOfWords:\n",
    "        if word in emptyDF.columns:\n",
    "            emptyDF[word][0] = 1\n",
    "    emptyDF.fillna(0)\n",
    "    return emptyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf21197d-dffb-4e86-8c7c-103d2f51c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generateHeatMap(y_test,y_pred, emotion):\n",
    "    y_test = y_test.astype(np.int32)\n",
    "    y_pred = y_pred.astype(np.int32)\n",
    "    \"\"\"\n",
    "    print(\"len(y_test) :\" + str(len((y_test))))\n",
    "    print(\"max(y_test) :\" + str(max((y_test))))\n",
    "    print(\"----\")\n",
    "    print(\"(y_test) :\\n\" + str((y_test)))\n",
    "    print(\"(y_pred) :\\n\" + str((y_pred)))\n",
    "    print(\"----\")\n",
    "    print(\"type(y_pred) :\" + str(type(y_pred)))\n",
    "    cf_matrix[0][0]: 718\n",
    "    This is in the cf_matrix: \n",
    "    [[718  39]\n",
    "     [129  14]]\n",
    "    \"\"\"\n",
    "    cf_matrix = confusion_matrix(y_test,y_pred)\n",
    "    #print(\"type(cf_matrix[0]): \" + str(type(cf_matrix[0])))\n",
    "    #print(\"cf_matrix[0][0]: \" + str(cf_matrix[0][0]))\n",
    "    #print(\"This is in the cf_matrix: \\n\" + str(cf_matrix))\n",
    "    true_false = cf_matrix[0][0] #correctly Identified false\n",
    "    false_positive = cf_matrix[0][1] #predicted true but was false\n",
    "    false_negative = cf_matrix[1][0] #predicted false but was true\n",
    "    true_positive = cf_matrix[1][1] #correctly identified true\n",
    "    \n",
    "    sumOfAllValues = true_false + false_positive + false_negative + true_positive\n",
    "    \n",
    "    accuracy = (true_positive+true_false)/sumOfAllValues\n",
    "    precision = true_positive / (true_positive + true_false)\n",
    "    recall = true_positive / (true_positive+false_negative)\n",
    "    #f1_score = 2 / ((1/recall)+(1/precision))\n",
    "    f1_score = 2*((precision*recall)/(precision+recall))\n",
    "    \n",
    "    #https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "    print(\"----\")\n",
    "    print(\"With an Accuracy of: \"+ str(round(accuracy*100,2)) + \" % --- (Perfect at 100%, Failure at 0%)\")\n",
    "    print(\"With a Precision of: \"+ str(round(precision*100,2))+ \" % --- (Perfect at 100%, Failure at 0%)\")\n",
    "    print(\"With a Recall of: \"+ str(round(recall*100,2))+ \" % --- (Perfect at 100%, Failure at 0%)\")\n",
    "    print(\"With a f1-Score of: \"+ str(round(f1_score*100,2))+ \" % --- (Perfect at 100%, Failure at 0%)\")\n",
    "    print(\"----\")\n",
    "\n",
    "    \n",
    "    ax = sns.heatmap(cf_matrix, annot = True, cmap = \"Blues\")\n",
    "    #print(\"this is the emotion:\" + str(emotion))\n",
    "    ax.set_title(\"Seaborn Confusion Matrix for Emotion: {}\\n\\n\".format(str(emotion)))\n",
    "    ax.set_xlabel(\"\\nPredictedValues\")\n",
    "    ax.set_ylabel(\"Actual Values\")\n",
    "    \n",
    "    ax.xaxis.set_ticklabels([\"False\",\"True\"])\n",
    "    ax.yaxis.set_ticklabels([\"False\",\"True\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9570038d-f028-4283-805b-1dd96c5c2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "#print(list((keys)))\n",
    "def trainEmotionModels(dataFrame, train_sizeIN = 0.1, showHeatmap = False):\n",
    "    tweet_features = dataFrame.keys()[24:]\n",
    "    x_trainingData = dataFrame[tweet_features]\n",
    "    #print(\"x_trainingData: \\n\" + str(x_trainingData.head()))\n",
    "    keys = list(dataFrame.keys()[12:20])\n",
    "    listOfModels = []\n",
    "    iterator = 0\n",
    "    for emotion in dataFrame[keys]:\n",
    "        print(\"This is the model for the emotion '\"+ emotion[:-5] + \"': \")\n",
    "        #print(dataFrame[emotion].head())\n",
    "        y_emotion = dataFrame[emotion]\n",
    "        #print(\"y_emotion.head(): \\n\" + str(type(y_emotion)))\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_trainingData, y_emotion,train_size = train_sizeIN, test_size = 1-train_sizeIN, random_state = 4,shuffle=True)\n",
    "        #print(\"Early max(y_test) \" + str(max(y_test)))\n",
    "        #print(\"type(x_train): \"+ str(type(x_train)))\n",
    "        #print(\"type(y_train): \"+ str(type(y_train)))\n",
    "        #print(\"type(x_test): \"+ str(type(x_test)))\n",
    "        #print(\"type(y_test): \"+ str(type(y_test)))\n",
    "        emotion_Model = DecisionTreeRegressor(random_state = 1)\n",
    "        emotion_Model.fit(x_train,y_train)\n",
    "        val_predictions = emotion_Model.predict(x_test)\n",
    "        listOfModels.append(emotion_Model)\n",
    "        #print(emotion_Model.predict(x_test.head()))\n",
    "        #print(y_test.head())\n",
    "        #print(\"len(y_test.keys()): \" + str(len(y_test.keys())))\n",
    "        #print(\"Key Examples: \" + str(y_test.keys()[:-15]))\n",
    "        print(\"Mean Absolute error for {}: \".format(emotion[:-5]) + str(round(mean_absolute_error(y_test,val_predictions)*100,2))+\"%\\n\")\n",
    "        #print(\"type(x_train): \" + str(type(x_train)))\n",
    "        #print(\"type(dfSentence): \" + str(type(dfOfSentence)))\n",
    "        #print(\"Predicition for for x_train.head() for emotion {}: \".format(emotion[:-5])+ str(emotion_Model.predict(x_train.head())))\n",
    "        #print(\"Predicition for '{}' for emotion {}: \".format(myPrivateSentence,emotion[:-5])+ str(emotion_Model.predict(dfOfSentence[tweet_features])))\n",
    "        #y_emotionGoal = dataFrame[emotion]\n",
    "        if(showHeatmap):\n",
    "            #print(\"type(y_test): \" + str(type(y_test)) + \" Content: \" + str(y_test))\n",
    "            #print(\"type(val_predictions): \" + str(type(val_predictions)) + \" Content: \"+ str(val_predictions))\n",
    "            generateHeatMap(y_test,val_predictions,emotion[:-5])\n",
    "        #print(\"+++++\\n\")\n",
    "        \n",
    "    dicOfModels = {\"Wut-Model\":listOfModels[0],\"Vorfreude-Model\":listOfModels[1],\"Ekel-Model\":listOfModels[2],\"Furcht-Model\":listOfModels[3],\"Freude-Model\":listOfModels[4],\"Traurigkeit-Model\":listOfModels[5], \"Überraschungs-Model\":listOfModels[6],\"Vertrauen-Model\":listOfModels[7]}\n",
    "    return dicOfModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc99472-e31b-43a6-b51e-3841e71ede9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAttributesOfTweet(tweetNumber, df):\n",
    "    keys = list(df.keys()[12:20])\n",
    "    print(\"---\\nTweet \\n--- \\n\" + df[\"textInput\"][tweetNumber]+\"\\n--- \\nhas these Emotions according to DataInput Tweet:\")\n",
    "    for emotion in df[keys]:\n",
    "        print(str(emotion[:-5]) + \"-Value: \" +str(1==df[emotion][tweetNumber]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03b369cc-e68c-4b7b-8f5c-be663f83a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAttributesOfTweetText(tweetNumber, df):\n",
    "    keys = list(df.keys()[12:20])\n",
    "    print(\"---\\nTweet \\n--- \\n\" + df[\"textInput\"][tweetNumber]+\"\\n---\")\n",
    "    stringOfEmotion = \"\"\n",
    "    stringOfNotEmotion = \"\"\n",
    "    countEmotionsActive = 0\n",
    "    countEmotionsPassive = 0\n",
    "    for emotion in df[keys]:\n",
    "        emotionText = emotion[:-5]\n",
    "        if(emotionText == \"Ueberraschung\"):\n",
    "            emotionText=\"Überraschung\"\n",
    "        if(1==df[emotion][tweetNumber]):\n",
    "            if(len(stringOfEmotion)>0):\n",
    "                stringOfEmotion = stringOfEmotion +\", \"\n",
    "            stringOfEmotion = stringOfEmotion + emotionText\n",
    "            countEmotionsActive = countEmotionsActive + 1\n",
    "        else:\n",
    "            if(len(stringOfNotEmotion)>0):\n",
    "                stringOfNotEmotion = stringOfNotEmotion +\", \"\n",
    "            stringOfNotEmotion = stringOfNotEmotion + emotionText\n",
    "            countEmotionsPassive = countEmotionsPassive + 1\n",
    "    \n",
    "    if(countEmotionsActive > 1):\n",
    "        print(\"Der Algorithmus hat anhand des Trainings die Emotionen \" + stringOfEmotion +\" im Tweet ermittelt.\")\n",
    "    elif(countEmotionsActive == 1):\n",
    "        print(\"Der Algorithmus hat anhand des Trainings die Emotion \" + stringOfEmotion +\" im Tweet ermittelt.\")\n",
    "    else:\n",
    "        print(\"Der Algorithmus hat anhand des Trainings keine Emotionen im Tweet ermittelt.\")\n",
    "    \n",
    "    if(countEmotionsPassive > 1):\n",
    "        print(\"Dadurch sind die Emotionen \" + stringOfNotEmotion+ \" laut dem Algorithmus nicht im Tweet enthalten.\")\n",
    "    elif(countEmotionsPassive == 1):     \n",
    "        print(\"Dadurch ist die Emotion \" + stringOfNotEmotion+ \" laut dem Algorithmus nicht im Tweet enthalten.\")\n",
    "    else:\n",
    "        print(\"Dadurch sind alle Emotionen die dem Algorithmus bekannt sind im Tweet enthalten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ee4c987-f9f8-43b9-a81b-978d272a1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretOwnSentence(sentence, dicOfModels, df):\n",
    "    if(type(sentence) != str):\n",
    "        print(\"Sentence-Parameter is not a String\")\n",
    "        return\n",
    "    if(type(dicOfModels) != dict):\n",
    "        print(\"dicOfModels-Parameter is not a dictionary\")\n",
    "        return\n",
    "    if(len(dicOfModels)!=8):\n",
    "        print(\"dicOfModels Length is not 8\")\n",
    "        return\n",
    "        \n",
    "    #Transforming Sentence into Vector\n",
    "    dfOfSentence = sentence_toVec(sentence, df)\n",
    "    #print(\"----\\nThis is the shape of the df: \\n\" + str(dfOfSentence) + \"\\n----\")\n",
    "    listOfIndex = dfOfSentence.columns[(dfOfSentence == 1).all()].tolist()\n",
    "    listOfIndex.extend([\"textInput\",\"editedInput\",\"WutInput\"])\n",
    "    #print(\"----\\nInput Sentence was transformed to this vector: \\n\" +str(dfOfSentence.head()))\n",
    "    keys = list(df.keys()[12:20])\n",
    "    #Applying different Models to sentence\n",
    "    keyOfFeatures = dfOfSentence.keys()[24:]\n",
    "    features = dfOfSentence[keyOfFeatures]    \n",
    "    iterator = 0\n",
    "    for model in dicOfModels:\n",
    "        #print(str(model) + \" is the following type: \" + str(type(model)))\n",
    "        result = dicOfModels[model].predict(features)\n",
    "        #print(\"---\\nThis is the result of {}: \".format(str(model)) + str(result))\n",
    "        #print(\"result[0]: \" + str(result[0]))\n",
    "        #print(\"before writing: \" + str(dfOfSentence[dfOfSentence.keys()[12+iterator]]))\n",
    "        #dfOfSentence[12+iterator] = result[0]\n",
    "        dfOfSentence[dfOfSentence.keys()[12+iterator]][0] = result[0]\n",
    "        #print(dfOfSentence[dfOfSentence.keys()[12+iterator]][0])\n",
    "        #print(\"---\\nThis is in the dfOfSentence: \" + str(dfOfSentence[12+iterator][0]))\n",
    "        if(dfOfSentence[dfOfSentence.keys()[12+iterator]][0]>1):\n",
    "            dfOfSentence[dfOfSentence.keys()[12+iterator]][0] = 1\n",
    "        iterator = iterator + 1\n",
    "    #print(\"---\\nThis is what all the Attributes look like: \\n\" + str(dfOfSentence[dfOfSentence.keys()[12:20]]))\n",
    "    getAttributesOfTweetText(0,dfOfSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2a28f16-98d3-4731-bba5-ac35a9cf2d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://dibyendudeb.com/comparing-machine-learning-algorithms/\n",
    "\n",
    "#Importing basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Importing sklearn modules\n",
    "from sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve\n",
    "from sklearn import ensemble, linear_model, neighbors, svm, tree, neural_network\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm,model_selection, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "\n",
    "def testDifferenModels(dataFrame, number_emotion = 8, train_sizeIN = 0.66):\n",
    "    if(number_emotion>8 | number_emotion<1):\n",
    "        number_emotion = 8\n",
    "    \n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeClassifier()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('SVM', SVC()))\n",
    "    models.append((\"DTR\", DecisionTreeRegressor()))\n",
    "    \n",
    "    tweet_features = dataFrame.keys()[24:]\n",
    "    x_trainingData = dataFrame[tweet_features]\n",
    "    #print(\"x_trainingData: \\n\" + str(x_trainingData.head()))\n",
    "    lastEmotion = 12+number_emotion\n",
    "    keys = list(dataFrame.keys()[12:lastEmotion])\n",
    "    listOfModels = []\n",
    "    iterator = 0\n",
    "    for emotion in dataFrame[keys]:\n",
    "        print(emotion[:-5] + \": \")\n",
    "        #print(dataFrame[emotion].head())\n",
    "        y_emotion = dataFrame[emotion]\n",
    "        #print(\"y_emotion.head(): \\n\" + str(type(y_emotion)))\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_trainingData, y_emotion,train_size = train_sizeIN, test_size = 1-train_sizeIN, random_state = 4,shuffle=True)\n",
    "        # evaluate each model in turn\n",
    "        results = []\n",
    "        names = []\n",
    "        scoring = 'accuracy'\n",
    "        seed = 3\n",
    "        for name, model in models:\n",
    "            kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle = True)\n",
    "            cv_results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n",
    "            results.append(cv_results)\n",
    "            names.append(name)\n",
    "            msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "            print(msg)\n",
    "        # boxplot algorithm comparison\n",
    "        fig = plt.figure()\n",
    "        fig.suptitle('Comparison between different MLAs for emotion: {}'.format(str(emotion[:-5])))\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.boxplot(results)\n",
    "        ax.set_xticklabels(names)\n",
    "        plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "225412f7-cbe5-44ae-aef6-31ba4839f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare all machine learning algorithms\n",
    "def compareAllModels(dataFrame, number_emotion = 1, train_sizeIN = 0.66):\n",
    "    # Application of all Machine Learning methods\n",
    "    MLA = [\n",
    "        #GLM\n",
    "        linear_model.LogisticRegressionCV(),\n",
    "        linear_model.PassiveAggressiveClassifier(),\n",
    "        linear_model. RidgeClassifierCV(),\n",
    "        linear_model.SGDClassifier(),\n",
    "        linear_model.Perceptron(),\n",
    "\n",
    "        #Ensemble Methods\n",
    "        ensemble.AdaBoostClassifier(),\n",
    "        ensemble.BaggingClassifier(),\n",
    "        ensemble.ExtraTreesClassifier(),\n",
    "        ensemble.GradientBoostingClassifier(),\n",
    "        ensemble.RandomForestClassifier(),\n",
    "\n",
    "        #Gaussian Processes\n",
    "        #gaussian_process.GaussianProcessClassifier(),\n",
    "\n",
    "        #SVM\n",
    "        #svm.SVC(probability=True),\n",
    "        #svm.NuSVC(probability=True),\n",
    "        svm.LinearSVC(),\n",
    "\n",
    "        #Trees    \n",
    "        tree.DecisionTreeClassifier(),\n",
    "        DecisionTreeRegressor(),\n",
    "\n",
    "        #Navies Bayes\n",
    "        naive_bayes.BernoulliNB(),\n",
    "        naive_bayes.GaussianNB(),\n",
    "\n",
    "        #Nearest Neighbor\n",
    "        #neighbors.KNeighborsClassifier(),\n",
    "    ]\n",
    "    tweet_features = dataFrame.keys()[24:]\n",
    "    x_trainingData = dataFrame[tweet_features]\n",
    "    #print(\"x_trainingData: \\n\" + str(x_trainingData.head()))\n",
    "    lastEmotion = 12+number_emotion\n",
    "    keys = list(dataFrame.keys()[12:lastEmotion])\n",
    "    listOfModels = []\n",
    "    iterator = 0\n",
    "    for emotion in dataFrame[keys]:\n",
    "        print(emotion[:-5] + \": \")\n",
    "        #print(dataFrame[emotion].head())\n",
    "        y_emotion = dataFrame[emotion]\n",
    "        #print(\"y_emotion.head(): \\n\" + str(type(y_emotion)))\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_trainingData, y_emotion,train_size = train_sizeIN, test_size = 1-train_sizeIN, random_state = 4,shuffle=True)\n",
    "        MLA_columns = []\n",
    "        MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "        row_index = 0\n",
    "        for alg in MLA:  \n",
    "            #print(\"This is the Alg: \" + str(alg))\n",
    "            predicted = alg.fit(x_train, y_train).predict(x_test)\n",
    "            fp, tp, th = roc_curve(y_test, predicted)\n",
    "            MLA_name = alg.__class__.__name__\n",
    "            MLA_compare.loc[row_index,'MLA used'] = MLA_name\n",
    "            MLA_compare.loc[row_index, 'Train Accuracy'] = round(alg.score(x_train, y_train), 4)\n",
    "            MLA_compare.loc[row_index, 'Test Accuracy'] = round(alg.score(x_test, y_test), 4)\n",
    "            MLA_compare.loc[row_index, 'Precission'] = precision_score(y_test, predicted)\n",
    "            MLA_compare.loc[row_index, 'Recall'] = recall_score(y_test, predicted)\n",
    "            MLA_compare.loc[row_index, 'AUC'] = auc(fp, tp)\n",
    "\n",
    "            row_index+=1\n",
    "\n",
    "        MLA_compare.sort_values(by = ['Test Accuracy'], ascending = False, inplace = True)    \n",
    "        print(MLA_compare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b42c0fcc-6727-48d3-8c23-55386f90501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#was trying to get some other algorithems to run, but getting errors. Not in active use\n",
    "\n",
    "\n",
    "def trainEmotionModelsExtraTreesClassifier(dataFrame, train_sizeIN = 0.1, showHeatmap = False):\n",
    "    tweet_features = dataFrame.keys()[24:]\n",
    "    x_trainingData = dataFrame[tweet_features]\n",
    "    #print(\"x_trainingData: \\n\" + str(x_trainingData.head()))\n",
    "    keys = list(dataFrame.keys()[12:20])\n",
    "    listOfModels = []\n",
    "    iterator = 0\n",
    "    for emotion in dataFrame[keys]:\n",
    "        print(\"This is the model for the emotion '\"+ emotion[:-5] + \"': \")\n",
    "        #print(dataFrame[emotion].head())\n",
    "        y_emotion = dataFrame[emotion]\n",
    "        #print(\"y_emotion.head(): \\n\" + str(type(y_emotion)))\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_trainingData, y_emotion,train_size = train_sizeIN, test_size = 1-train_sizeIN, random_state = 4,shuffle=True)\n",
    "        #print(\"Early max(y_test) \" + str(max(y_test)))\n",
    "        #print(\"type(x_train): \"+ str(type(x_train)))\n",
    "        #print(\"type(y_train): \"+ str(type(y_train)))\n",
    "        #print(\"type(x_test): \"+ str(type(x_test)))\n",
    "        #print(\"type(y_test): \"+ str(type(y_test)))\n",
    "        emotion_Model = ensemble.ExtraTreesClassifier(random_state = 1)\n",
    "        emotion_Model.fit(x_train,y_train)\n",
    "        val_predictions = emotion_Model.predict(x_test)\n",
    "        listOfModels.append(emotion_Model)\n",
    "        #print(emotion_Model.predict(x_test.head()))\n",
    "        #print(y_test.head())\n",
    "        #print(\"len(y_test.keys()): \" + str(len(y_test.keys())))\n",
    "        #print(\"Key Examples: \" + str(y_test.keys()[:-15]))\n",
    "        print(\"Mean Absolute error for {}: \".format(emotion[:-5]) + str(round(mean_absolute_error(y_test,val_predictions)*100,2))+\"%\\n\")\n",
    "        #print(\"type(x_train): \" + str(type(x_train)))\n",
    "        #print(\"type(dfSentence): \" + str(type(dfOfSentence)))\n",
    "        #print(\"Predicition for for x_train.head() for emotion {}: \".format(emotion[:-5])+ str(emotion_Model.predict(x_train.head())))\n",
    "        #print(\"Predicition for '{}' for emotion {}: \".format(myPrivateSentence,emotion[:-5])+ str(emotion_Model.predict(dfOfSentence[tweet_features])))\n",
    "        #y_emotionGoal = dataFrame[emotion]\n",
    "        if(showHeatmap):\n",
    "            #print(\"type(y_test): \" + str(type(y_test)) + \" Content: \" + str(y_test))\n",
    "            #print(\"type(val_predictions): \" + str(type(val_predictions)) + \" Content: \"+ str(val_predictions))\n",
    "            generateHeatMap(y_test,val_predictions,emotion[:-5])\n",
    "        #print(\"+++++\\n\")\n",
    "        \n",
    "    dicOfModels = {\"Wut-Model\":listOfModels[0],\"Vorfreude-Model\":listOfModels[1],\"Ekel-Model\":listOfModels[2],\"Furcht-Model\":listOfModels[3],\"Freude-Model\":listOfModels[4],\"Traurigkeit-Model\":listOfModels[5], \"Überraschungs-Model\":listOfModels[6],\"Vertrauen-Model\":listOfModels[7]}\n",
    "    return dicOfModels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0ec4dd9-79df-4647-839f-0b81eb160753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_esken = pd.read_csv(\"./Training_Dataframes/esken30ktweetswithemotions.csv\")\\ndf_scholz = pd.read_csv(\"./EmotionCSV.csv\")\\n\\nsmallSample_esken = df_esken.iloc[:5000].copy()\\nprint(len(smallSample_esken))\\nprint(len(df_scholz))\\nmixedDF = pd.concat([df_scholz,smallSample_esken])\\nmixedDF.reset_index(inplace = True)\\nprint(len(mixedDF))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if you want to do it by hand - else use doItAll()\n",
    "\"\"\"\n",
    "df_esken = pd.read_csv(\"./Training_Dataframes/esken30ktweetswithemotions.csv\")\n",
    "df_scholz = pd.read_csv(\"./EmotionCSV.csv\")\n",
    "\n",
    "smallSample_esken = df_esken.iloc[:5000].copy()\n",
    "print(len(smallSample_esken))\n",
    "print(len(df_scholz))\n",
    "mixedDF = pd.concat([df_scholz,smallSample_esken])\n",
    "mixedDF.reset_index(inplace = True)\n",
    "print(len(mixedDF))\n",
    "\"\"\"\n",
    "#vectorizedDataframe_Mixed = turnTweetsIntoVector(df_scholz)\n",
    "#print(vectorizedDataframe_Mixed.keys()[0:26])\n",
    "#vectorizedDataframe_Mixed = vectorizedDataframe_Mixed.drop(columns = [\"indexInput\"])\n",
    "#print(vectorizedDataframe_Mixed.keys()[0:26])\n",
    "#giveMeMyModels = trainEmotionModels(vectorizedDataframe_Mixed,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "202d8d76-0fcc-4bad-b0eb-d4799d9bea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doItAll(path, sizeDF = 0, train_sizeIn = 0.1, plotGraph = True, ):\n",
    "    sampleDF = pd.read_csv(path)\n",
    "    if (sizeDF != 0):\n",
    "        sampleDF = sampleDF.head(sizeDF)\n",
    "    if(len(sampleDF)>5000):\n",
    "        vectorizedDataframe = turnTweetsIntoVector(sampleDF, 5000)\n",
    "    else:\n",
    "        vectorizedDataframe = turnTweetsIntoVector(sampleDF)\n",
    "    model = trainEmotionModels(vectorizedDataframe, train_sizeIn, plotGraph)\n",
    "    return model, vectorizedDataframe\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4204a93b-3ff9-47e4-9c10-c6555127484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doItAllExtraTree(path, sizeDF = 0, train_sizeIn = 0.1, plotGraph = True, ):\n",
    "    sampleDF = pd.read_csv(path)\n",
    "    if (sizeDF != 0):\n",
    "        sampleDF = sampleDF.head(sizeDF)\n",
    "    if(len(sampleDF)>5000):\n",
    "        vectorizedDataframe = turnTweetsIntoVector(sampleDF, 5000)\n",
    "    else:\n",
    "        vectorizedDataframe = turnTweetsIntoVector(sampleDF)\n",
    "    model = trainEmotionModelsExtraTreesClassifier(vectorizedDataframe, train_sizeIn, plotGraph)\n",
    "    return model, vectorizedDataframe\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9906bf5-dfb8-4b43-b198-deeb1b9543cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#myModel, vectorizedDataframe = doItAll(\"./EmotionCSV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ed8aed3-3078-442d-9b9b-18639ecebe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpretOwnSentence(\"spd sieg freude glückwunsch!\", myModel, vectorizedDataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86141d3e-7f08-447b-91c7-248f5e43aeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the model for the emotion 'Wut': \n",
      "Mean Absolute error for Wut: 17.06%\n",
      "\n",
      "This is the model for the emotion 'Vorfreude': \n",
      "Mean Absolute error for Vorfreude: 8.53%\n",
      "\n",
      "This is the model for the emotion 'Ekel': \n",
      "Mean Absolute error for Ekel: 6.76%\n",
      "\n",
      "This is the model for the emotion 'Furcht': \n",
      "Mean Absolute error for Furcht: 16.47%\n",
      "\n",
      "This is the model for the emotion 'Freude': \n",
      "Mean Absolute error for Freude: 8.24%\n",
      "\n",
      "This is the model for the emotion 'Traurigkeit': \n",
      "Mean Absolute error for Traurigkeit: 14.12%\n",
      "\n",
      "This is the model for the emotion 'Ueberraschung': \n",
      "Mean Absolute error for Ueberraschung: 4.12%\n",
      "\n",
      "This is the model for the emotion 'Vertrauen': \n",
      "Mean Absolute error for Vertrauen: 11.47%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myModel, vectorizedDataframe  = doItAll(\"./Training_Dataframes/esken30ktweetswithemotions.csv\", sizeDF = 1000, train_sizeIn = 0.66, plotGraph = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8f27d9e-d64a-4f67-b424-179d089ee5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the model for the emotion 'Wut': \n",
      "Mean Absolute error for Wut: 18.67%\n",
      "\n",
      "This is the model for the emotion 'Vorfreude': \n",
      "Mean Absolute error for Vorfreude: 20.89%\n",
      "\n",
      "This is the model for the emotion 'Ekel': \n",
      "Mean Absolute error for Ekel: 13.11%\n",
      "\n",
      "This is the model for the emotion 'Furcht': \n",
      "Mean Absolute error for Furcht: 21.0%\n",
      "\n",
      "This is the model for the emotion 'Freude': \n",
      "Mean Absolute error for Freude: 21.67%\n",
      "\n",
      "This is the model for the emotion 'Traurigkeit': \n",
      "Mean Absolute error for Traurigkeit: 15.44%\n",
      "\n",
      "This is the model for the emotion 'Ueberraschung': \n",
      "Mean Absolute error for Ueberraschung: 9.44%\n",
      "\n",
      "This is the model for the emotion 'Vertrauen': \n",
      "Mean Absolute error for Vertrauen: 33.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quickModel, quickvectorizedDataframe  = doItAll(\"./Training_Dataframes/esken30ktweetswithemotions.csv\", sizeDF = 1000, train_sizeIn = 0.1, plotGraph = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "043391d6-71ab-4e62-9bc6-23bebeb86431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wut: \n",
      "LR: 0.850000 (0.047649)\n",
      "LDA: 0.696970 (0.066391)\n",
      "KNN: 0.845455 (0.042207)\n",
      "CART: 0.866667 (0.041105)\n",
      "NB: 0.709091 (0.046847)\n",
      "SVM: 0.845455 (0.042207)\n",
      "DTR: 0.881818 (0.026068)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEVCAYAAADuAi4fAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAh7UlEQVR4nO3de5wcVZ338c+XhHAJARKJKLmCRgS8oLawLooot8AuG0B9TNQHxEvMrqgL3hDdNSr7yLqyigLGqKg8CsFVItFHIV6Wm9dM1ggk3EKIZAhIYsJVFAK/549zhlQ63dM1k+npmcr3/Xr1a6bqnKr+1alTv64+XV2tiMDMzKprh04HYGZm7eVEb2ZWcU70ZmYV50RvZlZxTvRmZhXnRG9mVnFO9ANM0pslLR7k51wt6ajBfM5OknSNpHfk/7dob0mHSbpD0iOSTpS0t6TrJD0s6bzORT08SDpJ0prcfi/pdDzbQtI8Sf/S6TiGgiGb6CW9SVJX7nD3SvqxpFd2Oq5WIuLbEXFMp+Moa7i/SDRo708CF0TEbhHxfWA2sB7YPSLeP5ixSXqrpBta1LlGUkh6cd387+f5R+TpuZK+1WJdc/Myh2xD2J8FTs/t97ttWM+gatTWETEnIj7V5ud9dm7zvQvzPtpk3lUl1teyz/THkEz0ks4EPg/8H2BvYDJwETCjg2G1JGlkp2MwpgDL66ZXRD++GTiI+/N24JTC8z4D+BtgXdkVSBLwv4ENwKnbEEt9+5UmacQ2PO+wFBH3AiuBwwuzDwdubTDvukEMbUsRMaQewB7AI8AbeqmzE+mFYG1+fB7YKZcdAXQDHwLuB+4FTgSOJx1QG4CzC+uaC3wXuBx4GPgf4MWF8rOAO3PZCuCkQtlbgV8An8vrPSfPuyGXK5fdDzwI3Ai8oLCdl5AO5j8AHwN2KKz3BtLZ1UbgLuC4XtpjNfCRHN9G4OvAzoXyvweWAQ8AvwRelOf/X+Ap4LHc5h8Cvgm8P5dPAAL4pzz93Lyd6m29uWwf4Ht5++4C3lvX5t/J2/8wKbHUetm+o0kHzoPABcC1wDuKbZX/v7Nuey4DngAez9NHkU5uevbpn3Ic4/LyU/P2vh24G7guz38bcEtu26uBKYXYApgD3JHLL8z7/QDgL8CT+bkfaLJt1wD/SuqzI/K804Ev5XlHFNrsW7200eF5u9+St2tUoey5uc0eJL27ubzJMfVI3p5HgTvz/ANyjA/k/fQPhWW+keP8UV7mqCbH89dIx+E9pGOkZzvfyubj5wFgFfC3ef4a0nFzat26tjpmmrV1ju+cwvLvJCXlDcAiYJ9W+7Fkzvoa8MX8/4gc95y6eQ8Br2RzHxtZ1wfeUbbP9CuvDtSKBiwgmA5sKjZEgzqfBH4NPBMYT0oyn8plR+Tl/xXYMe/cdcClwBjgoNyY+xUOoCeA1+f6HyAlph1z+RtISWsH4I25Qz+70FE3Ae8BRgK7sGXiORZYCuzJ5oO/Z9lLgCtzTFNJL0JvL6z3iRz7COAfSS9oDTseKdHfDEwCxpEOnnNy2Utzxzs0r+vUXH+nwrJHFdb1NuAH+f83kRLi5YWyK1utN7fV0rwPRgH7kQ7iYwtt/hfSi+8I4NPAr5ts216kg6Rn/5yR23yrRN9ke77Blgf7P5P6zsQc65eBy3LZVNJBeAkwOu/PE0nJ4YC8jz8G/LIuQfww7+PJpL42vVFsTbbvGtJBvpj8Yg78FngFfUv0XyO9aO1ISvQnF8ouAz6a98vOwCt7WU8Az83/75i3/ey8H19LemHev9C2DwKH9ay7wfq+n9t4NOl4/S3wrrrj57TcD84hvcBemPfNMfn5dit5zNxQ99xP7/sc+3pSv90J+CL5hbzEfpxMeiGa3KTNTgV+n/+vkc7cp9XNeyy34VSaJPqyfaZfeXWgV7jNAcGbgfta1LkTOL4wfSywOv9/RG7UnrOGMblhDy3UXwqcWDiAfl0o24F09vGqJs+9DJhR2Cl315U/vaNy57qd9DZ8h0KdEcBfgQML894FXFNYx8pC2a55G57VJKbVwJzC9PFsPiP7EvlFsFB+G/DqwrLFxPic3Kl3AObluLpz2TeBM1utl5T869vlI8DXC23+00LZgcBjTbbtlLr9I1IC7G+ivwU4sjD9bNKL6kg2H4T7Fcp/TE4mhf7xZ/JZfa7/ykL5d4CzGsXWZPuuISX6t5AS8v7A7bmsVKLP/eMhNvfpL5NfkPP0JcB8YGKJ46+Y6F8F3MeWffcyYG6hbS/pZV17k/r5LoV5s4D/LrTPHYWyF+bn37sw70/AwZQ7ZnpL9F8DPlMo2y3v96mt9mOJNptKOgsfSzoR+bc8/57CvP8u1B30RD8Ux+j/BOzVYnx0H9Jbtx5/yPOeXkdEPJn/fyz//WOh/DHSju6xpuefiHiKdIDtAyDpFEnLJD0g6QHgBaSzzK2WrRcRPycNNVwI/FHSfEm75+VHNdiGCYXp+wrr+XP+txhzvWIcxfaYAry/J/68DZPYsr2KMd9Jett4MOlA/yGwVtL+pCR+bYn1TgH2qSs7m3Tgb7V9pMS5c5N9vg9b7p+glzYvYQqwsBDXLaSDtBjbmrr65xfqbyC92DTcV6Rt6W0/NXMF6cTgPaQhtb44iXRm/KM8/W3gOEnj8/SHSDH/VtJySW8rud59gDX5mOhR30972xdTSO8K7i2035dJZ/Y96o9LIqLRsVrmmOnNFjkjIh4h5Zpt3o8RsZqUM15JGkK7Phf9qjCvc+PzDM0PY39Felt/Yi911pI6UY/JeV5/Ter5R9IOpLf1ayVNAb5CGjN9RkTsSRoiUWHZ6G3FEfGFiHgZacjoecAHSW8hn2iwDfcMxDawZXusIZ1h7Fl47BoRl/US/7WkoZJREXFPnj6FdHayrMR61wB31ZWNiYjj+7Fd97Ll/lHdtvbVGtIQSTG2nfN29oi6+u+qq79LRPyyxHP12je2qJhezH9MGqbra6I/lZSU7pZ0H/BfpAQ7K6/7voh4Z0TsQzoLvkjSc0usdy0wKR8TPer7aW/buIZ0Fr5Xoe12j4iDSm/ZZq2OmVZtvUXOkDQaeAbbdswVXU9K6K8gDSUX572SzYn+0fx318Kyzyr8X7rP9MWQS/QR8SBpbPfCfB30rpJ2lHScpM/kapcBH5M0XtJeuX6vl5618DJJJ+czyn8mdc5fk8YVg3z1g6TTSGf0pUh6uaRDJe1I2sF/AZ7M7za+A/ybpDH5BeXMbdyGd0uaKGkc6ez58jz/K8CcHIckjZb0d5LG5PI/ksbQi64lvbj1dM5rSGeaNxTeKfW23t8CD0n6sKRdJI2Q9AJJL+/Hdv0/4KDC/nkvWx4YfTWP1O5TAHIfmtGi/kckHZTr7yHpDSWf64/AREmjStY/mzSktrpJ+Q6Sdi48dpI0ATiS9MH4wfnxYuDfyVffSHqDpIl5HRtJffrJ+pU38BtSv/1QPgaPAE4AFpTZmEhXpCwGzpO0u6QdJD1H0qvLLF+3rlbHTKu2vhQ4TdLBknYiXdH3m17auq+uI50MrY2Ih/K8G/K8PUgnsETEOtKLy1vycfE20nBpj772mVKGXKIHiIj/JO3Ej5GS7BpS4vl+rnIO0EW6iuUm0pUy52zDU15J+qB1I+kStZMj4omIWAGcR9pJfySNIf6iD+vdnZQQN5LeNv6JdCUNpMT5KOlDyhtIHfHibdiGS0kH1ar8OAcgIrpIH+pekONYSRoH7PFp0ovmA5I+kOddS/psoyfR30A6A3n67Wdv680H5QmkpHMX6Wzsq6QO3ycRsZ70gfi5pPabRt/2Qb3zSVdcLJb0MOkF/dBenn8hKWkukPQQ6R3dcSWf6+ekK1Xuk7S+VeWIWBsRvV1DPYs0lNHzuJPUX5dFxOJ85n5fRNwHfAF4kaQXAC8HfiPpEdK2vy8i7ioRz+PAP5C2dz3pEudTIuLWVssWnEIacum5Iuy7pM9F+qO3Y6bXto6InwH/QroS7F5Scp1Z5kklTVb6Ps/kXqpdSxqSKu6/ZaQP9JcWhl8hHTcfJPXng9j8DqDldvRXz2Vy2y1Jc0kfPr2l07GYmbXDkDyjNzOzgeNEb2ZWcdv90I2ZWdX5jN7MrOKc6M3MKs6J3sys4pzozcwqzonezKzinOjNzCrOid7MrOKc6M3MKs6J3sys4pzozcwqzonezKzinOjNzCrOid7MrOKc6M3MKm5kpwNoZK+99oqpU6d2Ogwzs2Fj6dKl6yNifKOyUole0nTSb22OAL4aEefWlY8l/Xbjc0g/gP22iLg5l60GHib9GPGmiKi1er6pU6fS1dVVJjQzMwMk/aFZWctEL2kEcCFwNNANLJG0KP9wdo+zST9QfJKk5+f6RxbKX5N/5NnMzAZZmTH6Q4CVEbEq/yr8AmBGXZ0DgZ8B5F+Inypp7wGN1MzM+qVMop8ArClMd+d5Rb8HTgaQdAgwBZiYywJYLGmppNnNnkTSbEldkrrWrVtXNn4zM2uhTKJXg3n1PzR7LjBW0jLgPcDvgE257LCIeClwHPBuSYc3epKImB8RtYiojR/f8PMEMzPrhzIfxnYDkwrTE4G1xQoR8RBwGoAkAXflBxGxNv+9X9JC0lDQddscuZmZlVLmjH4JME3SvpJGATOBRcUKkvbMZQDvAK6LiIckjZY0JtcZDRwD3Dxw4ZuZWSstz+gjYpOk04GrSZdXXhwRyyXNyeXzgAOASyQ9CawA3p4X3xtYmE7yGQlcGhFXDfxmmJlZM4qoH27vvFqtFr6O3sysPElLm31PaUh+M9as0/K70D4ZiidNZuBEb9ZQs6QtyQndhh3f1MzMrOKc6M3MKs6J3sys4pzozcwqzonezKzinOjNzCrOid7MrOIqfR19X7/04uujzWxbDcUv21U60TdqPH/hxczaaSh+2c5DN2ZmFedEb2ZWcU70ZmYV50RvZlZxTvRmZhXnRG9mVnGVvrzSOmcoXktsw4f7z8Byore2GIrXEtvw4f4zsEoN3UiaLuk2SSslndWgfKykhZJulPRbSS8ou6yZmbVXy0QvaQRwIXAccCAwS9KBddXOBpZFxIuAU4Dz+7CsmZm1UZkz+kOAlRGxKiIeBxYAM+rqHAj8DCAibgWmStq75LJmZtZGZRL9BGBNYbo7zyv6PXAygKRDgCnAxJLLmplZG5VJ9I0+/q7/NORcYKykZcB7gN8Bm0oum55Emi2pS1LXunXrSoRlZmZllLnqphuYVJieCKwtVoiIh4DTAJSui7orP3ZttWxhHfOB+QC1Ws0fq9ugGDduHBs3buzTMn259G/s2LFs2LChr2GZDagyiX4JME3SvsA9wEzgTcUKkvYE/pzH4d8BXBcRD0lquaxZJ23cuLGtl+v153pws4HWMtFHxCZJpwNXAyOAiyNiuaQ5uXwecABwiaQngRXA23tbtj2bYmY2uPr6jrBT7wY1FL98UKvVoqurqy3r9hcuOmuotX+74xlq2zvcDbX2bGc8fV23pKURUWtU5nvdmJlVnBO9mVnFVSLRjxs3DkmlHkDpupIYN25ch7duaOtL27v9rZ77z+CoxE3N2nnlhK+a6J2vWrFt4f4zOCpxRm9mZs050ZuZVZwTvZlZxVVijN7MrBPi47vD3D3at+4B4kRvZtZP+sRD7f3C1NyBWZeHbszMKs6J3sys4pzozcwqzonezKzinOjNzCrOid7MrOJ8eaWZ2TZo1/10xo4dO2DrcqI3M+unPv4wSMd+NMVDN2ZmFedEb2ZWcU70ZmYVV2qMXtJ04HxgBPDViDi3rnwP4FvA5LzOz0bE13PZauBh4ElgU7MfrzXrhHbelOrp9Zt1WMtEL2kEcCFwNNANLJG0KCJWFKq9G1gRESdIGg/cJunbEfF4Ln9NRKwf6ODNtlU7b0oFA3tjKrP+KjN0cwiwMiJW5cS9AJhRVyeAMUrXGe0GbAA2DWikZmbWL2US/QRgTWG6O88rugA4AFgL3AS8LyKeymUBLJa0VNLsZk8iabakLkld69atK70BZmbWuzKJvtG3Aerf6x4LLAP2AQ4GLpDUMzh5WES8FDgOeLekwxs9SUTMj4haRNTGjx9fJnYzsyFHUsNHq7J2KpPou4FJhemJpDP3otOAKyJZCdwFPB8gItbmv/cDC0lDQWZmlRQRfX60W5lEvwSYJmlfSaOAmcCiujp3A0cCSNob2B9YJWm0pDF5/mjgGODmgQrezMxaa3nVTURsknQ6cDXp8sqLI2K5pDm5fB7wKeAbkm4iDfV8OCLWS9oPWJjfmowELo2Iq9q0LWZm1oA6de+F3tRqtejq6iq/QBuvg07rf7C96x/O2t320Nb2b/f9Rzp5f5NhYZj3n6FE0tJm31OqRKJv58HkA7V3wz1RDvf1D3du/4HTW6L33Sttu9fOqx4G8lazZv3lRG/btb6e7W1PZ4hWHZVJ9MPh5v9mZp1QiUQ/XG7+b2bWCb5NsZlZxTnRm5lVnBO9mVnFOdGbmVWcE72ZWcU50ZuZVZwTvZlZxVXiOnrrrCreQqC3bWpW5u9n9E8V+89Q40Rv26SqtxAYDjFWQVX7z1DjoRszs4pzojczqzgnejOzinOiNzOrOCd6M7OKK5XoJU2XdJuklZLOalC+h6QfSPq9pOWSTiu7rJmZtVfLRC9pBHAhcBxwIDBL0oF11d4NrIiIFwNHAOdJGlVyWTMza6MyZ/SHACsjYlVEPA4sAGbU1QlgjNI3H3YDNgCbSi5rDUjq88Osh/uPFZVJ9BOANYXp7jyv6ALgAGAtcBPwvoh4quSyAEiaLalLUte6detKhl9dEdHw0arMDNx/bEtlEn2jl/r6XnEssAzYBzgYuEDS7iWXTTMj5kdELSJq48ePLxGWmZmVUSbRdwOTCtMTSWfuRacBV0SyErgLeH7JZc3MrI3KJPolwDRJ+0oaBcwEFtXVuRs4EkDS3sD+wKqSy5qZWRu1vKlZRGySdDpwNTACuDgilkuak8vnAZ8CviHpJtJwzYcjYj1Ao2XbsylmZtaIhuKHMLVaLbq6utqy7uF+9zvHb9tiuLf/cI+/nSQtjYhaozJ/M9bMrOKc6M3MKs6J3sys4pzozcwqzonezKzinOjNzCrOid7MrOKc6M3MKs6J3sys4pzozcwqruW9bsz6o7cfsmhW5q+2Ww/3n4HlRG9t4YPOtoX7z8Dy0I2ZWcU50ZuZVZwTvZlZxTnRm5lVnBO9mVnFOdGbmVVcpS+vbHa9ra/DNbPtSakzeknTJd0maaWksxqUf1DSsvy4WdKTksblstWSbspl7fkh2CYiok8PM7MqanlGL2kEcCFwNNANLJG0KCJW9NSJiP8A/iPXPwE4IyI2FFbzmohYP6CRm5lZKWXO6A8BVkbEqoh4HFgAzOil/izgsoEIzszMtl2ZRD8BWFOY7s7ztiJpV2A68L3C7AAWS1oqaXZ/AzUzs/4p82Fso08umw1onwD8om7Y5rCIWCvpmcBPJN0aEddt9STpRWA2wOTJk0uEVQ3jxo1j48aNfVqmtxs+1Rs7diwbNmxoXdHMKqvMGX03MKkwPRFY26TuTOqGbSJibf57P7CQNBS0lYiYHxG1iKiNHz++RFjVsHHjxj5/aNyXR19fRMysesok+iXANEn7ShpFSuaL6itJ2gN4NXBlYd5oSWN6/geOAW4eiMDNzKyclkM3EbFJ0unA1cAI4OKIWC5pTi6fl6ueBCyOiEcLi+8NLMxDDSOBSyPiqoHcADMz652G4vXjtVoturoG9ZL7jpHU1mv4271+G17cH6pL0tKIqDUq8y0QzMwqzonebBgbN24ckko/gD7VHzduXIe30AZCpe91Y1Z1PVdttUtfLuW1octn9GZmFedEb2ZWcU70ZmYV50RvZlZxTvRmZhXnRG9mVnFO9GZmFedEb2ZWcU70ZmYV50RvZlZxTvRmZhXnRG9mVnFO9GZmFee7V3ZYfHx3mLtHe9dvZts1J/oO0yceav8vTM1t2+rNbBjw0I2ZWcU50ZuZVVypoRtJ04HzgRHAVyPi3LryDwJvLqzzAGB8RGxotayZ9Z8/47Ey1Gp8WNII4HbgaKAbWALMiogVTeqfAJwREa/t67I9arVadHV19XVbhiVJ7R+jb+P6rbPcf6yHpKURUWtUVmbo5hBgZUSsiojHgQXAjF7qzwIu6+eyZmY2wMok+gnAmsJ0d563FUm7AtOB7/Vj2dmSuiR1rVu3rkRYZmZWRplE3+hn4Ju9lzsB+EVEbOjrshExPyJqEVEbP358ibDMzKyMMom+G5hUmJ4IrG1Sdyabh236uqyZmbVBmUS/BJgmaV9Jo0jJfFF9JUl7AK8GruzrsmZm1j4tL6+MiE2STgeuJl0ieXFELJc0J5fPy1VPAhZHxKOtlh3ojTAzs+ZaXl7ZCb68cvis3zrL/cd6bOvllWZmNow50ZuZVZzvXmk2zEmNrmIeGGPHjm3bum3wONGbDWN9HT/3mPv2yUM3ZmYV50RvZlZxTvRmZhXnRG9mVnFO9GZmFedEb2ZWcU70ZmYV50RvZlZxTvRmZhXnRG9mVnFO9GZmFedEb2ZWcU70ZmYV50RvZlZxTvRmZhVXKtFLmi7pNkkrJZ3VpM4RkpZJWi7p2sL81ZJuymXbxw/BmpkNIS1/eETSCOBC4GigG1giaVFErCjU2RO4CJgeEXdLembdal4TEesHLmwzMyurzBn9IcDKiFgVEY8DC4AZdXXeBFwREXcDRMT9AxummZn1V5lEPwFYU5juzvOKngeMlXSNpKWSTimUBbA4z5/d7EkkzZbUJalr3bp1ZeOvBElte/g3P82szG/GNvrl4fofnRwJvAw4EtgF+JWkX0fE7cBhEbE2D+f8RNKtEXHdViuMmA/MB6jVatvNj1r6Nz/NrN3KnNF3A5MK0xOBtQ3qXBURj+ax+OuAFwNExNr8935gIWkoyMzMBkmZRL8EmCZpX0mjgJnAoro6VwKvkjRS0q7AocAtkkZLGgMgaTRwDHDzwIVvZmattBy6iYhNkk4HrgZGABdHxHJJc3L5vIi4RdJVwI3AU8BXI+JmSfsBCyX1PNelEXFVuzbGzMy2pqE43lur1aKry5fcN+IxetsW7j/VJWlpRNQalfmbsWZmFedEb2ZWcU70ZmYV50RvZlZxTvRmZhXnRG9mVnFO9GZmFedEb2ZWcU70ZmYV50RvZlZxTvRmZhVX5n70ZjbM5BsJ9qnM98CpLid6swpy0rYiD92YmVWcE72ZWcU50ZuZVZwTvZlZxTnRm5lVnBO9mVnFlUr0kqZLuk3SSklnNalzhKRlkpZLurYvy5qZWfu0vI5e0gjgQuBooBtYImlRRKwo1NkTuAiYHhF3S3pm2WXNzKy9ypzRHwKsjIhVEfE4sACYUVfnTcAVEXE3QETc34dlzcysjcok+gnAmsJ0d55X9DxgrKRrJC2VdEofljUzszYqcwuERjfGqP9+9UjgZcCRwC7AryT9uuSy6Umk2cBsgMmTJ5cIy8zMyihzRt8NTCpMTwTWNqhzVUQ8GhHrgeuAF5dcFoCImB8RtYiojR8/vmz8ZmbWQplEvwSYJmlfSaOAmcCiujpXAq+SNFLSrsChwC0llzUzszZqOXQTEZsknQ5cDYwALo6I5ZLm5PJ5EXGLpKuAG4GngK9GxM0AjZZt07aYmVkDGoq3M63VatHV1dXpMIYkSb4FrZltRdLSiKg1KvP96Ico/3CEmQ0UJ/ohyknbzAaK73VjZlZxTvRmZhXnRG9mVnFO9GZmFedEb2ZWcU70ZmYV50RvZlZxTvRmZhU3JG+BIGkd8Ic2rX4vYH2b1j0YHH9nOf7OGs7xtzv2KRHR8Na/QzLRt5Okrmb3gxgOHH9nOf7OGs7xdzJ2D92YmVWcE72ZWcVtj4l+fqcD2EaOv7Mcf2cN5/g7Fvt2N0ZvZra92R7P6M3MtiuVTvSSHmkwb66keyQtk7RC0qxOxNZIiXjvkHSFpAPr6rxEUkg6dvCi3SrORwr/H59jnZzj/7OkZzapG5LOK0x/QNLcQQs8PeezJC2QdGfuEz+S9Lxcdoakv0jao1D/CEkPSvqdpFslfVbSC/M+WiZpg6S78v8/HcxtKcTYtF3r+tStkr4kqaO5QNJHJS2XdGOO68eSPl1X52BJt+T/V0u6vq58maSbBzPuwnM/mZ9/uaTfSzpT0g6Sji30i0ck3Zb/v6RRP2pXfJVO9L34XEQcDMwAvixpxw7H08rnIuLgiJgGXA78XFLxetlZwA35b0dJOhL4IjA9Iu7Os9cD72+yyF+BkyXtNRjx1VP6ua6FwDUR8ZyIOBA4G9g7V5lF+pH7k+oWvT4iXgK8BPh7YPe8jw4GFgEfzNNHDcZ2NNCqXXuOgQOBFwKvHqzA6kl6BakNXxoRLwKOAs4F3lhXdSZwaWF6jKRJeR0HDEasvXgs7++DgKOB44GPR8TVhX7RBbw5T5+Sl9uiH0k6rB3Bba+JHoCIuAP4MzC207GUFRGXA4uBN8HTier1wFuBYyTt3KnYJL0K+ArwdxFxZ6HoYuCNksY1WGwT6UOqMwYhxEZeAzwREfN6ZkTEsoi4XtJzgN2Aj9HkRTQiHgOWARMGIda+KNuuo4CdgY1tj6i5ZwPrI+KvABGxPiKuBR6QdGih3v8CFhSmv8PmF4NZwGWDEWwrEXE/MBs4Xb39JuiWy7S1H23XiV7SS4E78o4ZTv4HeH7+/zDgrpxYryGdSXTCTsCVwIkRcWtd2SOkZP++JsteCLy5ODwyiF4ALG1S1pM8rgf2Lw4/9ZA0FpgGXNe2CPuvt3Y9Q9Iy4F7g9ohYNpiB1VkMTJJ0u6SLJPW8u7iMdBaPpL8B/pRPznp8Fzg5/38C8IPBCriViFhFyq9b9ZlG2t2PttdEf4ak24DfAHM7HEt/FM8SZrH5LGcBnRu+eQL4JfD2JuVfAE6VtHt9QUQ8BFwCvLd94fXLTGBBRDwFXAG8oVD2Kkk3AvcBP4yI+zoRYG9atGvP0M0zgdGSZg5mbEUR8QjwMtJZ8DrgcklvJfXn1+fPD2ay9Rn7BmBjjv0W0rvzoaTM2fyg9KPtNdF/LiL2J73tu6STwx399BLgFkkjgNcB/yppNWls/DhJYzoQ01Okt9Yvl3R2fWFEPEAaX/2nJst/nvQiMbpN8TWznJRktiDpRaQzrJ/ktp3Jli+i1+fx5BcC/yjp4PaH2i+fp5d2jYgngKuAwwcxpkZxPBkR10TEx4HTgddFxBpgNenzg9eRhmrqXU565zIkhm16SNoPeBJoNVowKP1oe030AETEFaQPSE7tdCxlSXodcAypYx8F/D4iJkXE1IiYAnwPOLETsUXEn0kfqr1ZUqMz+/8E3gWMbLDsBtKB3OwdQbv8HNhJ0jt7Zkh6OXA+MDe369SI2AeYIGlKceGIuB34NPDhwQy6rFbtmseQ/xa4s1H5YJC0v6RphVkHs/mmhpcBnwPujIjuBosvBD4DXN3WIPsgXygxD7ggSn5Rqd39qOqJfldJ3YXHmQ3qfBI4s9OXl2XN4j0jX5J1B/AW4LURsY50hrmwbh3fI39Q2wk5sUwHPiZpRl3ZelK8OzVZ/DzSHf4GTT4QTwKOVrq8cjlpOO8Itm7bheQx4zrzgMMl7dvGULdFo3btGaO/mfTCe9FgB1WwG/BNpUtbbyRdCTQ3l/0XcBBbfgj7tIh4OCL+PSIeH5RIm9ul5/JK4Kekzx0+0cd1tK0f+ZuxZmYVNxTOYs3MrI2c6M3MKs6J3sys4pzozcwqzonezKzinOjNzCrOid7MrOKc6M3MKu7/A3kZT8x/fhrnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testDifferenModels(quickvectorizedDataframe, number_emotion = 1, train_sizeIN = 0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c549ed6d-f370-411b-b15d-469f818d2a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wut: \n",
      "                       MLA used  Train Accuracy  Test Accuracy  Precission  \\\n",
      "6             BaggingClassifier          0.9818         0.8794    1.000000   \n",
      "8    GradientBoostingClassifier          0.9394         0.8735    1.000000   \n",
      "7          ExtraTreesClassifier          1.0000         0.8706    1.000000   \n",
      "5            AdaBoostClassifier          0.9652         0.8676    0.777778   \n",
      "11       DecisionTreeClassifier          1.0000         0.8529    0.638889   \n",
      "1   PassiveAggressiveClassifier          1.0000         0.8471    0.700000   \n",
      "4                    Perceptron          1.0000         0.8441    0.684211   \n",
      "9        RandomForestClassifier          1.0000         0.8441    1.000000   \n",
      "0          LogisticRegressionCV          1.0000         0.8412    0.666667   \n",
      "2             RidgeClassifierCV          1.0000         0.8412    0.650000   \n",
      "10                    LinearSVC          1.0000         0.8412    0.650000   \n",
      "3                 SGDClassifier          1.0000         0.8382    0.586207   \n",
      "13                  BernoulliNB          0.8455         0.8206    0.000000   \n",
      "14                   GaussianNB          0.9667         0.7118    0.243243   \n",
      "12        DecisionTreeRegressor          1.0000         0.0690    0.684211   \n",
      "\n",
      "      Recall       AUC  \n",
      "6   0.316667  0.658333  \n",
      "8   0.283333  0.641667  \n",
      "7   0.266667  0.633333  \n",
      "5   0.350000  0.664286  \n",
      "11  0.383333  0.668452  \n",
      "1   0.233333  0.605952  \n",
      "4   0.216667  0.597619  \n",
      "9   0.116667  0.558333  \n",
      "0   0.200000  0.589286  \n",
      "2   0.216667  0.595833  \n",
      "10  0.216667  0.595833  \n",
      "3   0.283333  0.620238  \n",
      "13  0.000000  0.498214  \n",
      "14  0.300000  0.550000  \n",
      "12  0.433333  0.695238  \n"
     ]
    }
   ],
   "source": [
    "compareAllModels(vectorizedDataframe, number_emotion = 1, train_sizeIN = 0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6db37d9b-b207-446d-b5ce-3b1c77720792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quickModel, quickvectorizedDataframe  = doItAllExtraTree(\"./Training_Dataframes/esken30ktweetswithemotions.csv\", sizeDF = 1000, train_sizeIn = 0.1, plotGraph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4e24fd3-b223-4e47-b8fe-af0d81f795a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Tweet \n",
      "--- \n",
      "gruen umwelt klima streik frauen krach kriegsieg freude glückwunsch unglück freiheit gerechtigkeit!\n",
      "---\n",
      "Der Algorithmus hat anhand des Trainings die Emotionen Wut, Furcht, Freude, Vertrauen im Tweet  ermittelt.\n",
      "Dadurch sind die Emotionen Vorfreude, Ekel, Traurigkeit, Überraschung laut dem Algorithmus nicht im Tweet enthalten.\n"
     ]
    }
   ],
   "source": [
    "#Testing Model for fun!\n",
    "#print(type(df_esken[\"text\"][0:5]))\n",
    "#for tweet in df_esken[\"text\"][0:5]:\n",
    "#    interpretOwnSentence(tweet,myModel,vectorizedDataframe)\n",
    "interpretOwnSentence(\"gruen umwelt klima streik frauen krach kriegsieg freude glückwunsch unglück freiheit gerechtigkeit!\", myModel, vectorizedDataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "baa335b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model for streamlit application\n",
    "\n",
    "#import pickle\n",
    "\n",
    "#filename = 'finalized_model.sav'\n",
    "#pickle.dump(myModel, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "920f06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "#Testing Model for fun!\n",
    "#print(type(df_esken[\"text\"][0:5]))\n",
    "#for tweet in df_esken[\"text\"][0:5]:\n",
    "#    interpretOwnSentence(tweet,myModel,vectorizedDataframe)\n",
    "#interpretOwnSentence(\"gruen umwelt klima streik frauen krach kriegsieg freude glückwunsch unglück freiheit gerechtigkeit!\", loaded_model, vectorizedDataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
