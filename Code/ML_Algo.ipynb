{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7d28f-c15e-4f62-959d-c893b9231f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#df2 = pd.read_csv(\"./Training_Dataframes/esken30ktweetswithemotions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b3619-d633-43bf-8b99-efcbd93829df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/swlh/tweet-sentiment-analysis-using-python-for-complete-beginners-4aeb4456040\n",
    "# Turning all Tweets into Vectors for later Machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def turnTweetsIntoVector(dataframe,max_features_IN = None):\n",
    "    dataframe.drop([\"Unnamed: 0\"],axis = 1, inplace = True)\n",
    "    keyString = dataframe.keys()\n",
    "    keyString += \"Input\"\n",
    "    dataframe.columns = keyString\n",
    "    listOfTweets = dataframe[\"editedInput\"].tolist()\n",
    "    docs = np.array(listOfTweets)\n",
    "    cv=CountVectorizer(max_features = max_features_IN, binary = True)\n",
    "    oneHotCv = cv.fit_transform(docs)\n",
    "    oneHotArray = oneHotCv.toarray()\n",
    "    oneHotArray.shape\n",
    "    oneHotDf = pd.DataFrame(oneHotArray)\n",
    "    #Naming the Columns accordingly for humans to read    \n",
    "    oneHotDf.columns = cv.get_feature_names_out()\n",
    "    #print(oneHotDf.head())\n",
    "    #print(oneHotDf.info())\n",
    "    #Normalizing Emotions    \n",
    "    longDf = pd.concat([dataframe,oneHotDf], axis = 1)\n",
    "    #dir(CountVectorizer)\n",
    "    #print(longDf.describe())\n",
    "    #longDf.head()\n",
    "    for emotion in longDf[longDf.keys()[12:20]]:\n",
    "        longDf[emotion] = np.where(longDf[emotion]>=1, 1, 0)\n",
    "    \n",
    "    # ----- To check output\n",
    "    \n",
    "    #proving, that all words in the tweet are marked appropriatly\n",
    "    #print(longDf.iloc[0][\"editedInput\"])\n",
    "    #print(longDf.iloc[0][longDf.iloc[0][\"editedInput\"].split()])\n",
    "    #Shows that keys[0:24] are tweet data, thats not part of the NLP\n",
    "    #print(longDf.keys()[:27])\n",
    "    #print(longDf.keys()[:24])\n",
    "    # -----\n",
    "\n",
    "    return longDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c66b7-8602-44b4-b666-a22af7e9c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is already part of old code\n",
    "import re\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+\\s?', '', text) #Removed Mentions\n",
    "    text = re.sub(r'#', '', text) #Removed #\n",
    "    text = re.sub(r'(.)1+', r'1', text) #cleaned single letters\n",
    "    text = re.sub('((www.[^s]+)|(https?://[^s]+))','',text) #Removes links\n",
    "    text = re.sub('@','',text) #Remove @\n",
    "    text = re.sub('-','',text) #Remove -\n",
    "    text = re.sub('ä','ae',text) #Remove ä\n",
    "    text = re.sub('Ä','Ae',text) #Remove Ä\n",
    "    text = re.sub('ö','oe',text) #Remove Ä\n",
    "    text = re.sub('Ö','Oe',text) #Remove Ä\n",
    "    text = re.sub('ü','ue',text) #Remove Ä\n",
    "    text = re.sub('Ü','Ue',text) #Remove Ä\n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493cfda-0051-4df2-8cdc-c037ab39bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_toVec(sentence,goalDF):\n",
    "    #Preparing Input text\n",
    "    cleanSentence = sentence.lower()\n",
    "    cleanSentence = clean(cleanSentence)\n",
    "    cleansentene = stemming_on_text(cleanSentence)\n",
    "    \n",
    "    #Preparing EmptyDF\n",
    "    emptyList = [0]*len(goalDF.keys())\n",
    "    #print(emptyList)\n",
    "    #print(\"Len EmptyList: \" + str(len(emptyList)))\n",
    "    #emptyDF = goalDF.iloc[0:0].copy()\n",
    "    #emptyDF = emptyDF.append(emptyList)\n",
    "    #emptyDF = emptyDF.append(pd.DataFrame(emptyList, columns = goalDF.keys()), ignore_index = True)\n",
    "    emptyDF = pd.DataFrame(columns = goalDF.keys())\n",
    "    #emptyDF.append(pd.Series(), ignore_index = True)\n",
    "    emptyDF.loc[len(emptyDF)] = emptyList\n",
    "    #print(emptyDF)\n",
    "    #emptyDF = pd.DataFrame(emptyList, columns = goalDF.keys())\n",
    "    #print(\"LenEmpty DF: \" + str(len(emptyDF)) + \" Len Keys Empty DF: \" + str(len(emptyDF.keys())))\n",
    "    \n",
    "    emptyDF[\"textInput\"][0] = sentence\n",
    "    emptyDF[\"editedInput\"][0] = cleanSentence\n",
    "    #keys = list(emptyDF.keys()[0:10])\n",
    "    #print(\"---\")\n",
    "    #print(\"New LenEmpty DF: \" + str(len(emptyDF)) + \" Len Keys Empty DF: \" + str(len(emptyDF.keys())))\n",
    "    #print(emptyDF[\"textInput\"].tolist())\n",
    "    listOfWords = cleanSentence.split()\n",
    "    for word in listOfWords:\n",
    "        if word in emptyDF.columns:\n",
    "            emptyDF[word][0] = 1\n",
    "    emptyDF.fillna(0)\n",
    "    return emptyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21197d-dffb-4e86-8c7c-103d2f51c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generateHeatMap(y_test,y_pred, emotion):\n",
    "    y_test = y_test.astype(np.int32)\n",
    "    y_pred = y_pred.astype(np.int32)\n",
    "    \"\"\"\n",
    "    print(\"len(y_test) :\" + str(len((y_test))))\n",
    "    print(\"max(y_test) :\" + str(max((y_test))))\n",
    "    print(\"----\")\n",
    "    print(\"(y_test) :\\n\" + str((y_test)))\n",
    "    print(\"(y_pred) :\\n\" + str((y_pred)))\n",
    "    print(\"----\")\n",
    "    print(\"type(y_pred) :\" + str(type(y_pred)))\n",
    "    cf_matrix[0][0]: 718\n",
    "    This is in the cf_matrix: \n",
    "    [[718  39]\n",
    "     [129  14]]\n",
    "    \"\"\"\n",
    "    cf_matrix = confusion_matrix(y_test,y_pred)\n",
    "    #print(\"type(cf_matrix[0]): \" + str(type(cf_matrix[0])))\n",
    "    #print(\"cf_matrix[0][0]: \" + str(cf_matrix[0][0]))\n",
    "    #print(\"This is in the cf_matrix: \\n\" + str(cf_matrix))\n",
    "    true_false = cf_matrix[0][0] #correctly Identified false (true negative)\n",
    "    false_positive = cf_matrix[0][1] #predicted true but was false (false positive)\n",
    "    false_negative = cf_matrix[1][0] #predicted false but was true (false negative)\n",
    "    true_positive = cf_matrix[1][1] #correctly identified true (true positive)\n",
    "    \n",
    "    sumOfAllValues = true_false + false_positive + false_negative + true_positive\n",
    "    \n",
    "    accuracy = (true_positive+true_false)/sumOfAllValues\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive+false_negative)\n",
    "    #f1_score = 2 / ((1/recall)+(1/precision))\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    #https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "    print(\"----\")\n",
    "    print(\"With an Accuracy of: \"+ str(round(accuracy*100,2)) + \" % --- (Perfect at 100%, Failure at 0%)\")\n",
    "    print(\"With a Precision of: \"+ str(round(precision*100,2))+ \" % --- (Perfect at 100%, Failure at 0%)\")\n",
    "    print(\"With a Recall of: \"+ str(round(recall*100,2))+ \" % --- (Perfect at 100%, Failure at 0%)\")\n",
    "    print(\"With a f1-Score of: \"+ str(round(f1_score*100,2))+ \" % --- (Perfect at 100%, Failure at 0%)\")\n",
    "    print(\"----\")\n",
    "\n",
    "    \n",
    "    ax = sns.heatmap(cf_matrix, annot = True, cmap = \"Blues\")\n",
    "    #print(\"this is the emotion:\" + str(emotion))\n",
    "    ax.set_title(\"Seaborn Confusion Matrix for Emotion: {}\\n\\n\".format(str(emotion)))\n",
    "    ax.set_xlabel(\"\\nPredictedValues\")\n",
    "    ax.set_ylabel(\"Actual Values\")\n",
    "    \n",
    "    ax.xaxis.set_ticklabels([\"False\",\"True\"])\n",
    "    ax.yaxis.set_ticklabels([\"False\",\"True\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9570038d-f028-4283-805b-1dd96c5c2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "#print(list((keys)))\n",
    "def trainEmotionModels(dataFrame, train_sizeIN = 0.1, showHeatmap = False):\n",
    "    tweet_features = dataFrame.keys()[24:]\n",
    "    x_trainingData = dataFrame[tweet_features]\n",
    "    #print(\"x_trainingData: \\n\" + str(x_trainingData.head()))\n",
    "    keys = list(dataFrame.keys()[12:20])\n",
    "    listOfModels = []\n",
    "    iterator = 0\n",
    "    for emotion in dataFrame[keys]:#keys if you want to check all emotions\n",
    "        print(\"This is the model for the emotion '\"+ emotion[:-5] + \"': \")\n",
    "        #print(dataFrame[emotion].head())\n",
    "        y_emotion = dataFrame[emotion]\n",
    "        #print(\"y_emotion.head(): \\n\" + str(type(y_emotion)))\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_trainingData, y_emotion,train_size = train_sizeIN, test_size = 1-train_sizeIN, random_state = 4,shuffle=True)\n",
    "        #print(\"Early max(y_test) \" + str(max(y_test)))\n",
    "        #print(\"type(x_train): \"+ str(type(x_train)))\n",
    "        #print(\"type(y_train): \"+ str(type(y_train)))\n",
    "        #print(\"type(x_test): \"+ str(type(x_test)))\n",
    "        #print(\"type(y_test): \"+ str(type(y_test)))\n",
    "        emotion_Model = DecisionTreeRegressor(random_state = 1)\n",
    "        emotion_Model.fit(x_train,y_train)\n",
    "        val_predictions = emotion_Model.predict(x_test)\n",
    "        listOfModels.append(emotion_Model)\n",
    "        #print(emotion_Model.predict(x_test.head()))\n",
    "        #print(y_test.head())\n",
    "        #print(\"len(y_test.keys()): \" + str(len(y_test.keys())))\n",
    "        #print(\"Key Examples: \" + str(y_test.keys()[:-15]))\n",
    "        print(\"Mean Absolute error for {}: \".format(emotion[:-5]) + str(round(mean_absolute_error(y_test,val_predictions)*100,2))+\"%\\n\")\n",
    "        #print(\"type(x_train): \" + str(type(x_train)))\n",
    "        #print(\"type(dfSentence): \" + str(type(dfOfSentence)))\n",
    "        #print(\"Predicition for for x_train.head() for emotion {}: \".format(emotion[:-5])+ str(emotion_Model.predict(x_train.head())))\n",
    "        #print(\"Predicition for '{}' for emotion {}: \".format(myPrivateSentence,emotion[:-5])+ str(emotion_Model.predict(dfOfSentence[tweet_features])))\n",
    "        #y_emotionGoal = dataFrame[emotion]\n",
    "        if(showHeatmap):\n",
    "            #print(\"type(y_test): \" + str(type(y_test)) + \" Content: \" + str(y_test))\n",
    "            #print(\"type(val_predictions): \" + str(type(val_predictions)) + \" Content: \"+ str(val_predictions))\n",
    "            generateHeatMap(y_test,val_predictions,emotion[:-5])\n",
    "        #print(\"+++++\\n\")\n",
    "        \n",
    "    dicOfModels = {\"Wut-Model\":listOfModels[0],\"Vorfreude-Model\":listOfModels[1],\"Ekel-Model\":listOfModels[2],\"Furcht-Model\":listOfModels[3],\"Freude-Model\":listOfModels[4],\"Traurigkeit-Model\":listOfModels[5], \"Überraschungs-Model\":listOfModels[6],\"Vertrauen-Model\":listOfModels[7]}\n",
    "    return dicOfModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc99472-e31b-43a6-b51e-3841e71ede9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAttributesOfTweet(tweetNumber, df):\n",
    "    keys = list(df.keys()[12:20])\n",
    "    print(\"---\\nTweet \\n--- \\n\" + df[\"textInput\"][tweetNumber]+\"\\n--- \\nhas these Emotions according to DataInput Tweet:\")\n",
    "    for emotion in df[keys]:\n",
    "        print(str(emotion[:-5]) + \"-Value: \" +str(1==df[emotion][tweetNumber]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b369cc-e68c-4b7b-8f5c-be663f83a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAttributesOfTweetText(tweetNumber, df):\n",
    "    keys = list(df.keys()[12:20])\n",
    "    print(\"---\\nTweet \\n--- \\n\" + df[\"textInput\"][tweetNumber]+\"\\n---\")\n",
    "    stringOfEmotion = \"\"\n",
    "    stringOfNotEmotion = \"\"\n",
    "    countEmotionsActive = 0\n",
    "    countEmotionsPassive = 0\n",
    "    for emotion in df[keys]:\n",
    "        emotionText = emotion[:-5]\n",
    "        if(emotionText == \"Ueberraschung\"):\n",
    "            emotionText=\"Überraschung\"\n",
    "        if(1==df[emotion][tweetNumber]):\n",
    "            if(len(stringOfEmotion)>0):\n",
    "                stringOfEmotion = stringOfEmotion +\", \"\n",
    "            stringOfEmotion = stringOfEmotion + emotionText\n",
    "            countEmotionsActive = countEmotionsActive + 1\n",
    "        else:\n",
    "            if(len(stringOfNotEmotion)>0):\n",
    "                stringOfNotEmotion = stringOfNotEmotion +\", \"\n",
    "            stringOfNotEmotion = stringOfNotEmotion + emotionText\n",
    "            countEmotionsPassive = countEmotionsPassive + 1\n",
    "    \n",
    "    if(countEmotionsActive > 1):\n",
    "        print(\"Der Algorithmus hat anhand des Trainings die Emotionen \" + stringOfEmotion +\" im Tweet ermittelt.\")\n",
    "    elif(countEmotionsActive == 1):\n",
    "        print(\"Der Algorithmus hat anhand des Trainings die Emotion \" + stringOfEmotion +\" im Tweet ermittelt.\")\n",
    "    else:\n",
    "        print(\"Der Algorithmus hat anhand des Trainings keine Emotionen im Tweet ermittelt.\")\n",
    "    \n",
    "    if(countEmotionsPassive > 1):\n",
    "        print(\"Dadurch sind die Emotionen \" + stringOfNotEmotion+ \" laut dem Algorithmus nicht im Tweet enthalten.\")\n",
    "    elif(countEmotionsPassive == 1):     \n",
    "        print(\"Dadurch ist die Emotion \" + stringOfNotEmotion+ \" laut dem Algorithmus nicht im Tweet enthalten.\")\n",
    "    else:\n",
    "        print(\"Dadurch sind alle Emotionen die dem Algorithmus bekannt sind im Tweet enthalten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee4c987-f9f8-43b9-a81b-978d272a1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretOwnSentence(sentence, dicOfModels, df):\n",
    "    if(type(sentence) != str):\n",
    "        print(\"Sentence-Parameter is not a String\")\n",
    "        return\n",
    "    if(type(dicOfModels) != dict):\n",
    "        print(\"dicOfModels-Parameter is not a dictionary\")\n",
    "        return\n",
    "    if(len(dicOfModels)!=8):\n",
    "        print(\"dicOfModels Length is not 8\")\n",
    "        return\n",
    "        \n",
    "    #Transforming Sentence into Vector\n",
    "    dfOfSentence = sentence_toVec(sentence, df)\n",
    "    #print(\"----\\nThis is the shape of the df: \\n\" + str(dfOfSentence) + \"\\n----\")\n",
    "    listOfIndex = dfOfSentence.columns[(dfOfSentence == 1).all()].tolist()\n",
    "    listOfIndex.extend([\"textInput\",\"editedInput\",\"WutInput\"])\n",
    "    #print(\"----\\nInput Sentence was transformed to this vector: \\n\" +str(dfOfSentence.head()))\n",
    "    keys = list(df.keys()[12:20])\n",
    "    #Applying different Models to sentence\n",
    "    keyOfFeatures = dfOfSentence.keys()[24:]\n",
    "    features = dfOfSentence[keyOfFeatures]    \n",
    "    iterator = 0\n",
    "    for model in dicOfModels:\n",
    "        #print(str(model) + \" is the following type: \" + str(type(model)))\n",
    "        result = dicOfModels[model].predict(features)\n",
    "        #print(\"---\\nThis is the result of {}: \".format(str(model)) + str(result))\n",
    "        #print(\"result[0]: \" + str(result[0]))\n",
    "        #print(\"before writing: \" + str(dfOfSentence[dfOfSentence.keys()[12+iterator]]))\n",
    "        #dfOfSentence[12+iterator] = result[0]\n",
    "        dfOfSentence[dfOfSentence.keys()[12+iterator]][0] = result[0]\n",
    "        #print(dfOfSentence[dfOfSentence.keys()[12+iterator]][0])\n",
    "        #print(\"---\\nThis is in the dfOfSentence: \" + str(dfOfSentence[12+iterator][0]))\n",
    "        if(dfOfSentence[dfOfSentence.keys()[12+iterator]][0]>1):\n",
    "            dfOfSentence[dfOfSentence.keys()[12+iterator]][0] = 1\n",
    "        iterator = iterator + 1\n",
    "    #print(\"---\\nThis is what all the Attributes look like: \\n\" + str(dfOfSentence[dfOfSentence.keys()[12:20]]))\n",
    "    getAttributesOfTweetText(0,dfOfSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a28f16-98d3-4731-bba5-ac35a9cf2d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://dibyendudeb.com/comparing-machine-learning-algorithms/\n",
    "\n",
    "#Importing basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Importing sklearn modules\n",
    "from sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve\n",
    "from sklearn import ensemble, linear_model, neighbors, svm, tree, neural_network\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm,model_selection, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "\n",
    "def testDifferenModels(dataFrame, number_emotion = 8, train_sizeIN = 0.66):\n",
    "    if(number_emotion>8 | number_emotion<1):\n",
    "        number_emotion = 8\n",
    "    \n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeClassifier()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('SVM', SVC()))\n",
    "    models.append((\"DTR\", DecisionTreeRegressor()))\n",
    "    \n",
    "    tweet_features = dataFrame.keys()[24:]\n",
    "    x_trainingData = dataFrame[tweet_features]\n",
    "    #print(\"x_trainingData: \\n\" + str(x_trainingData.head()))\n",
    "    lastEmotion = 12+number_emotion\n",
    "    keys = list(dataFrame.keys()[12:lastEmotion])\n",
    "    listOfModels = []\n",
    "    iterator = 0\n",
    "    for emotion in dataFrame[keys]:\n",
    "        print(emotion[:-5] + \": \")\n",
    "        #print(dataFrame[emotion].head())\n",
    "        y_emotion = dataFrame[emotion]\n",
    "        #print(\"y_emotion.head(): \\n\" + str(type(y_emotion)))\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_trainingData, y_emotion,train_size = train_sizeIN, test_size = 1-train_sizeIN, random_state = 4,shuffle=True)\n",
    "        # evaluate each model in turn\n",
    "        results = []\n",
    "        names = []\n",
    "        scoring = 'accuracy'\n",
    "        seed = 3\n",
    "        for name, model in models:\n",
    "            kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle = True)\n",
    "            cv_results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n",
    "            results.append(cv_results)\n",
    "            names.append(name)\n",
    "            msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "            print(msg)\n",
    "        # boxplot algorithm comparison\n",
    "        fig = plt.figure()\n",
    "        fig.suptitle('Comparison between different MLAs for emotion: {}'.format(str(emotion[:-5])))\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.boxplot(results)\n",
    "        ax.set_xticklabels(names)\n",
    "        plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225412f7-cbe5-44ae-aef6-31ba4839f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare all machine learning algorithms\n",
    "def compareAllModels(dataFrame, number_emotion = 1, train_sizeIN = 0.66):\n",
    "    # Application of all Machine Learning methods\n",
    "    MLA = [\n",
    "        #GLM\n",
    "        linear_model.LogisticRegressionCV(),\n",
    "        linear_model.PassiveAggressiveClassifier(),\n",
    "        linear_model. RidgeClassifierCV(),\n",
    "        linear_model.SGDClassifier(),\n",
    "        linear_model.Perceptron(),\n",
    "\n",
    "        #Ensemble Methods\n",
    "        ensemble.AdaBoostClassifier(),\n",
    "        ensemble.BaggingClassifier(),\n",
    "        ensemble.ExtraTreesClassifier(),\n",
    "        ensemble.GradientBoostingClassifier(),\n",
    "        ensemble.RandomForestClassifier(),\n",
    "\n",
    "        #Gaussian Processes\n",
    "        #gaussian_process.GaussianProcessClassifier(),\n",
    "\n",
    "        #SVM\n",
    "        #svm.SVC(probability=True),\n",
    "        #svm.NuSVC(probability=True),\n",
    "        svm.LinearSVC(),\n",
    "\n",
    "        #Trees    \n",
    "        tree.DecisionTreeClassifier(),\n",
    "        DecisionTreeRegressor(),\n",
    "\n",
    "        #Navies Bayes\n",
    "        naive_bayes.BernoulliNB(),\n",
    "        naive_bayes.GaussianNB(),\n",
    "\n",
    "        #Nearest Neighbor\n",
    "        #neighbors.KNeighborsClassifier(),\n",
    "    ]\n",
    "    tweet_features = dataFrame.keys()[24:]\n",
    "    x_trainingData = dataFrame[tweet_features]\n",
    "    #print(\"x_trainingData: \\n\" + str(x_trainingData.head()))\n",
    "    lastEmotion = 12+number_emotion\n",
    "    keys = list(dataFrame.keys()[12:lastEmotion])\n",
    "    listOfModels = []\n",
    "    iterator = 0\n",
    "    for emotion in dataFrame[keys]:\n",
    "        print(emotion[:-5] + \": \")\n",
    "        #print(dataFrame[emotion].head())\n",
    "        y_emotion = dataFrame[emotion]\n",
    "        #print(\"y_emotion.head(): \\n\" + str(type(y_emotion)))\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_trainingData, y_emotion,train_size = train_sizeIN, test_size = 1-train_sizeIN, random_state = 4,shuffle=True)\n",
    "        MLA_columns = []\n",
    "        MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "        row_index = 0\n",
    "        for alg in MLA:  \n",
    "            #print(\"This is the Alg: \" + str(alg))\n",
    "            predicted = alg.fit(x_train, y_train).predict(x_test)\n",
    "            fp, tp, th = roc_curve(y_test, predicted)\n",
    "            MLA_name = alg.__class__.__name__\n",
    "            MLA_compare.loc[row_index,'MLA used'] = MLA_name\n",
    "            MLA_compare.loc[row_index, 'Train Accuracy'] = round(alg.score(x_train, y_train), 4)\n",
    "            MLA_compare.loc[row_index, 'Test Accuracy'] = round(alg.score(x_test, y_test), 4)\n",
    "            MLA_compare.loc[row_index, 'Precission'] = precision_score(y_test, predicted)\n",
    "            MLA_compare.loc[row_index, 'Recall'] = recall_score(y_test, predicted)\n",
    "            MLA_compare.loc[row_index, 'AUC'] = auc(fp, tp)\n",
    "\n",
    "            row_index+=1\n",
    "\n",
    "        MLA_compare.sort_values(by = ['Test Accuracy'], ascending = False, inplace = True)    \n",
    "        print(MLA_compare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c0fcc-6727-48d3-8c23-55386f90501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#was trying to get some other algorithems to run, but getting errors. Not in active use\n",
    "\n",
    "\n",
    "def trainEmotionModelsExtraTreesClassifier(dataFrame, train_sizeIN = 0.1, showHeatmap = False):\n",
    "    tweet_features = dataFrame.keys()[24:]\n",
    "    x_trainingData = dataFrame[tweet_features]\n",
    "    #print(\"x_trainingData: \\n\" + str(x_trainingData.head()))\n",
    "    keys = list(dataFrame.keys()[12:20])\n",
    "    listOfModels = []\n",
    "    iterator = 0\n",
    "    for emotion in dataFrame[keys]:\n",
    "        print(\"This is the model for the emotion '\"+ emotion[:-5] + \"': \")\n",
    "        #print(dataFrame[emotion].head())\n",
    "        y_emotion = dataFrame[emotion]\n",
    "        #print(\"y_emotion.head(): \\n\" + str(type(y_emotion)))\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_trainingData, y_emotion,train_size = train_sizeIN, test_size = 1-train_sizeIN, random_state = 4,shuffle=True)\n",
    "        #print(\"Early max(y_test) \" + str(max(y_test)))\n",
    "        #print(\"type(x_train): \"+ str(type(x_train)))\n",
    "        #print(\"type(y_train): \"+ str(type(y_train)))\n",
    "        #print(\"type(x_test): \"+ str(type(x_test)))\n",
    "        #print(\"type(y_test): \"+ str(type(y_test)))\n",
    "        emotion_Model = ensemble.ExtraTreesClassifier(random_state = 1)\n",
    "        emotion_Model.fit(x_train,y_train)\n",
    "        val_predictions = emotion_Model.predict(x_test)\n",
    "        listOfModels.append(emotion_Model)\n",
    "        #print(emotion_Model.predict(x_test.head()))\n",
    "        #print(y_test.head())\n",
    "        #print(\"len(y_test.keys()): \" + str(len(y_test.keys())))\n",
    "        #print(\"Key Examples: \" + str(y_test.keys()[:-15]))\n",
    "        print(\"Mean Absolute error for {}: \".format(emotion[:-5]) + str(round(mean_absolute_error(y_test,val_predictions)*100,2))+\"%\\n\")\n",
    "        #print(\"type(x_train): \" + str(type(x_train)))\n",
    "        #print(\"type(dfSentence): \" + str(type(dfOfSentence)))\n",
    "        #print(\"Predicition for for x_train.head() for emotion {}: \".format(emotion[:-5])+ str(emotion_Model.predict(x_train.head())))\n",
    "        #print(\"Predicition for '{}' for emotion {}: \".format(myPrivateSentence,emotion[:-5])+ str(emotion_Model.predict(dfOfSentence[tweet_features])))\n",
    "        #y_emotionGoal = dataFrame[emotion]\n",
    "        if(showHeatmap):\n",
    "            #print(\"type(y_test): \" + str(type(y_test)) + \" Content: \" + str(y_test))\n",
    "            #print(\"type(val_predictions): \" + str(type(val_predictions)) + \" Content: \"+ str(val_predictions))\n",
    "            generateHeatMap(y_test,val_predictions,emotion[:-5])\n",
    "        #print(\"+++++\\n\")\n",
    "        \n",
    "    dicOfModels = {\"Wut-Model\":listOfModels[0],\"Vorfreude-Model\":listOfModels[1],\"Ekel-Model\":listOfModels[2],\"Furcht-Model\":listOfModels[3],\"Freude-Model\":listOfModels[4],\"Traurigkeit-Model\":listOfModels[5], \"Überraschungs-Model\":listOfModels[6],\"Vertrauen-Model\":listOfModels[7]}\n",
    "    return dicOfModels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec4dd9-79df-4647-839f-0b81eb160753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to do it by hand - else use doItAll()\n",
    "\"\"\"\n",
    "df_esken = pd.read_csv(\"./Training_Dataframes/esken30ktweetswithemotions.csv\")\n",
    "df_scholz = pd.read_csv(\"./EmotionCSV.csv\")\n",
    "\n",
    "smallSample_esken = df_esken.iloc[:5000].copy()\n",
    "print(len(smallSample_esken))\n",
    "print(len(df_scholz))\n",
    "mixedDF = pd.concat([df_scholz,smallSample_esken])\n",
    "mixedDF.reset_index(inplace = True)\n",
    "print(len(mixedDF))\n",
    "\"\"\"\n",
    "#vectorizedDataframe_Mixed = turnTweetsIntoVector(df_scholz)\n",
    "#print(vectorizedDataframe_Mixed.keys()[0:26])\n",
    "#vectorizedDataframe_Mixed = vectorizedDataframe_Mixed.drop(columns = [\"indexInput\"])\n",
    "#print(vectorizedDataframe_Mixed.keys()[0:26])\n",
    "#giveMeMyModels = trainEmotionModels(vectorizedDataframe_Mixed,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d8d76-0fcc-4bad-b0eb-d4799d9bea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doItAll(path, sizeDF = 0, train_sizeIn = 0.1, plotGraph = True, ):\n",
    "    sampleDF = pd.read_csv(path)\n",
    "    if (sizeDF != 0):\n",
    "        sampleDF = sampleDF.head(sizeDF)\n",
    "    if(len(sampleDF)>5000):\n",
    "        vectorizedDataframe = turnTweetsIntoVector(sampleDF, 5000)\n",
    "    else:\n",
    "        vectorizedDataframe = turnTweetsIntoVector(sampleDF)\n",
    "    model = trainEmotionModels(vectorizedDataframe, train_sizeIn, plotGraph)\n",
    "    return model, vectorizedDataframe\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204a93b-3ff9-47e4-9c10-c6555127484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doItAllExtraTree(path, sizeDF = 0, train_sizeIn = 0.1, plotGraph = True, ):\n",
    "    sampleDF = pd.read_csv(path)\n",
    "    if (sizeDF != 0):\n",
    "        sampleDF = sampleDF.head(sizeDF)\n",
    "    if(len(sampleDF)>5000):\n",
    "        vectorizedDataframe = turnTweetsIntoVector(sampleDF, 5000)\n",
    "    else:\n",
    "        vectorizedDataframe = turnTweetsIntoVector(sampleDF)\n",
    "    model = trainEmotionModelsExtraTreesClassifier(vectorizedDataframe, train_sizeIn, plotGraph)\n",
    "    return model, vectorizedDataframe\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9906bf5-dfb8-4b43-b198-deeb1b9543cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#myModel, vectorizedDataframe = doItAll(\"./EmotionCSV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8aed3-3078-442d-9b9b-18639ecebe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpretOwnSentence(\"spd sieg freude glückwunsch!\", myModel, vectorizedDataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86141d3e-7f08-447b-91c7-248f5e43aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel, vectorizedDataframe  = doItAll(\"./Training_Dataframes/esken30ktweetswithemotions.csv\", sizeDF = 5000, train_sizeIn = 0.66, plotGraph = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f27d9e-d64a-4f67-b424-179d089ee5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "quickModel, quickvectorizedDataframe  = doItAll(\"./Training_Dataframes/esken30ktweetswithemotions.csv\", sizeDF = 1000, train_sizeIn = 0.1, plotGraph = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043391d6-71ab-4e62-9bc6-23bebeb86431",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDifferenModels(quickvectorizedDataframe, number_emotion = 1, train_sizeIN = 0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c549ed6d-f370-411b-b15d-469f818d2a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "compareAllModels(vectorizedDataframe, number_emotion = 1, train_sizeIN = 0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db37d9b-b207-446d-b5ce-3b1c77720792",
   "metadata": {},
   "outputs": [],
   "source": [
    "quickModel, quickvectorizedDataframe  = doItAllExtraTree(\"./Training_Dataframes/esken30ktweetswithemotions.csv\", sizeDF = 1000, train_sizeIn = 0.1, plotGraph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e24fd3-b223-4e47-b8fe-af0d81f795a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Model for fun!\n",
    "#print(type(df_esken[\"text\"][0:5]))\n",
    "#for tweet in df_esken[\"text\"][0:5]:\n",
    "#    interpretOwnSentence(tweet,myModel,vectorizedDataframe)\n",
    "interpretOwnSentence(\"gruen umwelt klima streik frauen krach kriegsieg freude glückwunsch unglück freiheit gerechtigkeit!\", myModel, vectorizedDataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa335b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model for streamlit application\n",
    "\n",
    "#import pickle\n",
    "\n",
    "#filename = 'finalized_model.sav'\n",
    "#pickle.dump(myModel, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "#Testing Model for fun!\n",
    "#print(type(df_esken[\"text\"][0:5]))\n",
    "#for tweet in df_esken[\"text\"][0:5]:\n",
    "#    interpretOwnSentence(tweet,myModel,vectorizedDataframe)\n",
    "#interpretOwnSentence(\"gruen umwelt klima streik frauen krach kriegsieg freude glückwunsch unglück freiheit gerechtigkeit!\", loaded_model, vectorizedDataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
