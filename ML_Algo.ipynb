{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a7d28f-c15e-4f62-959d-c893b9231f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"EmotionCSV.csv\")\n",
    "#df2 = pd.read_csv(\"./Training_Dataframes/esken30ktweetswithemotions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e55b663-8741-4886-8b77-e53efbdc949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(df))\n",
    "#print(df.head())\n",
    "df.drop([\"Unnamed: 0\"],axis = 1, inplace = True)\n",
    "#print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34847d55-9b27-4f59-993c-87cbbb94e9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweetidInput', 'datetimeInput', 'dateInput', 'timeInput', 'userInput',\n",
      "       'textInput', 'languageInput', 'editedInput', 'textforttbInput',\n",
      "       'lematizedInput', 'AnalysisInput', 'word_countInput', 'WutInput',\n",
      "       'VorfreudeInput', 'EkelInput', 'FurchtInput', 'FreudeInput',\n",
      "       'TraurigkeitInput', 'UeberraschungInput', 'VertrauenInput',\n",
      "       'neg_emotionsInput', 'pos_emotionsInput', 'total_neg_emotionsInput',\n",
      "       'total_pos_emotionsInput'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#print(df.head())\n",
    "#print(df.keys())\n",
    "keyString = df.keys()\n",
    "keyString += \"Input\"\n",
    "df.columns = keyString\n",
    "#print(df.keys())\n",
    "print(keyString[0:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "528d2c40-c348-467b-8c40-204f7a8c5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfTweets = df[\"editedInput\"].tolist()\n",
    "docs = np.array(listOfTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962b3619-d633-43bf-8b99-efcbd93829df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tweetidInput   userInput  word_countInput     WutInput  VorfreudeInput  \\\n",
      "count  4.012000e+03      4012.0      4012.000000  4012.000000     4012.000000   \n",
      "mean   8.509599e+17  38150247.0        10.102193     0.094467        0.188933   \n",
      "std    4.648605e+17         0.0         5.526349     0.366646        0.453466   \n",
      "min    2.043751e+09  38150247.0         0.000000     0.000000        0.000000   \n",
      "25%    4.679688e+17  38150247.0         6.000000     0.000000        0.000000   \n",
      "50%    8.915403e+17  38150247.0         9.000000     0.000000        0.000000   \n",
      "75%    1.316796e+18  38150247.0        15.000000     0.000000        0.000000   \n",
      "max    1.515589e+18  38150247.0        29.000000     4.000000        4.000000   \n",
      "\n",
      "         EkelInput  FurchtInput  FreudeInput  TraurigkeitInput  \\\n",
      "count  4012.000000  4012.000000  4012.000000       4012.000000   \n",
      "mean      0.052343     0.124128     0.229561          0.103938   \n",
      "std       0.250160     0.413010     0.554570          0.362014   \n",
      "min       0.000000     0.000000     0.000000          0.000000   \n",
      "25%       0.000000     0.000000     0.000000          0.000000   \n",
      "50%       0.000000     0.000000     0.000000          0.000000   \n",
      "75%       0.000000     0.000000     0.000000          0.000000   \n",
      "max       3.000000     4.000000     5.000000          4.000000   \n",
      "\n",
      "       UeberraschungInput  ...   zwietracht      zwingen     zwischen  \\\n",
      "count         4012.000000  ...  4012.000000  4012.000000  4012.000000   \n",
      "mean             0.110419  ...     0.000499     0.000249     0.000249   \n",
      "std              0.344521  ...     0.022324     0.015788     0.015788   \n",
      "min              0.000000  ...     0.000000     0.000000     0.000000   \n",
      "25%              0.000000  ...     0.000000     0.000000     0.000000   \n",
      "50%              0.000000  ...     0.000000     0.000000     0.000000   \n",
      "75%              0.000000  ...     0.000000     0.000000     0.000000   \n",
      "max              4.000000  ...     1.000000     1.000000     1.000000   \n",
      "\n",
      "       zwischenfall  zwischenlandung  zwischenschritt  zwischenzeit  \\\n",
      "count   4012.000000      4012.000000      4012.000000   4012.000000   \n",
      "mean       0.000499         0.000249         0.000249      0.000249   \n",
      "std        0.022324         0.015788         0.015788      0.015788   \n",
      "min        0.000000         0.000000         0.000000      0.000000   \n",
      "25%        0.000000         0.000000         0.000000      0.000000   \n",
      "50%        0.000000         0.000000         0.000000      0.000000   \n",
      "75%        0.000000         0.000000         0.000000      0.000000   \n",
      "max        1.000000         1.000000         1.000000      1.000000   \n",
      "\n",
      "       zwischenzeitlich      zynisch      zypries  \n",
      "count       4012.000000  4012.000000  4012.000000  \n",
      "mean           0.000249     0.000249     0.000249  \n",
      "std            0.015788     0.015788     0.015788  \n",
      "min            0.000000     0.000000     0.000000  \n",
      "25%            0.000000     0.000000     0.000000  \n",
      "50%            0.000000     0.000000     0.000000  \n",
      "75%            0.000000     0.000000     0.000000  \n",
      "max            1.000000     1.000000     1.000000  \n",
      "\n",
      "[8 rows x 12467 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetidInput</th>\n",
       "      <th>datetimeInput</th>\n",
       "      <th>dateInput</th>\n",
       "      <th>timeInput</th>\n",
       "      <th>userInput</th>\n",
       "      <th>textInput</th>\n",
       "      <th>languageInput</th>\n",
       "      <th>editedInput</th>\n",
       "      <th>textforttbInput</th>\n",
       "      <th>lematizedInput</th>\n",
       "      <th>...</th>\n",
       "      <th>zwietracht</th>\n",
       "      <th>zwingen</th>\n",
       "      <th>zwischen</th>\n",
       "      <th>zwischenfall</th>\n",
       "      <th>zwischenlandung</th>\n",
       "      <th>zwischenschritt</th>\n",
       "      <th>zwischenzeit</th>\n",
       "      <th>zwischenzeitlich</th>\n",
       "      <th>zynisch</th>\n",
       "      <th>zypries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1508125473842335749</td>\n",
       "      <td>2022-03-27 16:55:07</td>\n",
       "      <td>2022-03-27</td>\n",
       "      <td>16:55:07</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Die Saarländerinnen und Saarländer haben sich ...</td>\n",
       "      <td>de</td>\n",
       "      <td>saarlaenderinnen saarlaender klar wechsel spit...</td>\n",
       "      <td>Die Saarländerinnen und Saarländer haben sich ...</td>\n",
       "      <td>['saarlaenderinnen', 'saarlaender', 'klar', 'w...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1504398354565976070</td>\n",
       "      <td>2022-03-17 10:04:52</td>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>10:04:52</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Meine persönliche Position ist längst bekannt:...</td>\n",
       "      <td>de</td>\n",
       "      <td>persoenliche position laengst bekannt zeitlich...</td>\n",
       "      <td>Meine persönliche Position ist längst bekannt:...</td>\n",
       "      <td>['persoenliche', 'position', 'laengst', 'bekan...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1492872048120668161</td>\n",
       "      <td>2022-02-13 14:43:27</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>14:43:27</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Ich freue mich, wenn Sie und Ihr mir ab heute ...</td>\n",
       "      <td>de</td>\n",
       "      <td>freue mich folgt alles regierungsarbeit betrif...</td>\n",
       "      <td>Ich freue mich, wenn Sie und Ihr mir ab heute ...</td>\n",
       "      <td>['freue', 'mich', 'folgt', 'alles', 'regierung...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1492074452796022791</td>\n",
       "      <td>2022-02-11 09:54:05</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>09:54:05</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Für die aktuelle Welle der Pandemie gibt es An...</td>\n",
       "      <td>de</td>\n",
       "      <td>aktuelle welle pandemie anlass zuversicht wiss...</td>\n",
       "      <td>Für die aktuelle Welle der Pandemie gibt es An...</td>\n",
       "      <td>['aktuelle', 'pandemie', 'anlass', 'zuversicht...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1492074451432873985</td>\n",
       "      <td>2022-02-11 09:54:05</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>09:54:05</td>\n",
       "      <td>38150247</td>\n",
       "      <td>Die #Corona-Pandemie und auch die Flutkatastro...</td>\n",
       "      <td>de</td>\n",
       "      <td>coronapandemie flutkatastrophe sommer gezeigt ...</td>\n",
       "      <td>Die Corona-Pandemie und auch die Flutkatastrop...</td>\n",
       "      <td>['coronapandemie', 'flutkatastrophe', 'sommer'...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12478 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          tweetidInput        datetimeInput   dateInput timeInput  userInput  \\\n",
       "0  1508125473842335749  2022-03-27 16:55:07  2022-03-27  16:55:07   38150247   \n",
       "1  1504398354565976070  2022-03-17 10:04:52  2022-03-17  10:04:52   38150247   \n",
       "2  1492872048120668161  2022-02-13 14:43:27  2022-02-13  14:43:27   38150247   \n",
       "3  1492074452796022791  2022-02-11 09:54:05  2022-02-11  09:54:05   38150247   \n",
       "4  1492074451432873985  2022-02-11 09:54:05  2022-02-11  09:54:05   38150247   \n",
       "\n",
       "                                           textInput languageInput  \\\n",
       "0  Die Saarländerinnen und Saarländer haben sich ...            de   \n",
       "1  Meine persönliche Position ist längst bekannt:...            de   \n",
       "2  Ich freue mich, wenn Sie und Ihr mir ab heute ...            de   \n",
       "3  Für die aktuelle Welle der Pandemie gibt es An...            de   \n",
       "4  Die #Corona-Pandemie und auch die Flutkatastro...            de   \n",
       "\n",
       "                                         editedInput  \\\n",
       "0  saarlaenderinnen saarlaender klar wechsel spit...   \n",
       "1  persoenliche position laengst bekannt zeitlich...   \n",
       "2  freue mich folgt alles regierungsarbeit betrif...   \n",
       "3  aktuelle welle pandemie anlass zuversicht wiss...   \n",
       "4  coronapandemie flutkatastrophe sommer gezeigt ...   \n",
       "\n",
       "                                     textforttbInput  \\\n",
       "0  Die Saarländerinnen und Saarländer haben sich ...   \n",
       "1  Meine persönliche Position ist längst bekannt:...   \n",
       "2  Ich freue mich, wenn Sie und Ihr mir ab heute ...   \n",
       "3  Für die aktuelle Welle der Pandemie gibt es An...   \n",
       "4  Die Corona-Pandemie und auch die Flutkatastrop...   \n",
       "\n",
       "                                      lematizedInput  ... zwietracht  zwingen  \\\n",
       "0  ['saarlaenderinnen', 'saarlaender', 'klar', 'w...  ...          0        0   \n",
       "1  ['persoenliche', 'position', 'laengst', 'bekan...  ...          0        0   \n",
       "2  ['freue', 'mich', 'folgt', 'alles', 'regierung...  ...          0        0   \n",
       "3  ['aktuelle', 'pandemie', 'anlass', 'zuversicht...  ...          0        0   \n",
       "4  ['coronapandemie', 'flutkatastrophe', 'sommer'...  ...          0        0   \n",
       "\n",
       "   zwischen  zwischenfall  zwischenlandung  zwischenschritt  zwischenzeit  \\\n",
       "0         0             0                0                0             0   \n",
       "1         0             0                0                0             0   \n",
       "2         0             0                0                0             0   \n",
       "3         0             0                0                0             0   \n",
       "4         0             0                0                0             0   \n",
       "\n",
       "   zwischenzeitlich  zynisch  zypries  \n",
       "0                 0        0        0  \n",
       "1                 0        0        0  \n",
       "2                 0        0        0  \n",
       "3                 0        0        0  \n",
       "4                 0        0        0  \n",
       "\n",
       "[5 rows x 12478 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://medium.com/swlh/tweet-sentiment-analysis-using-python-for-complete-beginners-4aeb4456040\n",
    "# Turning all Tweets into Vectors for later Machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(binary = True)\n",
    "oneHotCv = cv.fit_transform(docs)\n",
    "oneHotArray = oneHotCv.toarray()\n",
    "oneHotArray.shape\n",
    "oneHotDf = pd.DataFrame(oneHotArray)\n",
    "#Naming the Columns accordingly for humans to read\n",
    "oneHotDf.columns = cv.get_feature_names_out()\n",
    "#print(oneHotDf.head())\n",
    "#print(oneHotDf.info())\n",
    "longDf = pd.concat([df,oneHotDf], axis = 1)\n",
    "#dir(CountVectorizer)\n",
    "print(longDf.describe())\n",
    "longDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d65065d-bbb2-4ca9-b3d3-e6a13dba3bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saarlaenderinnen saarlaender klar wechsel spitze landes entschieden sicher gestalten du  herzlichen glueckwunsch ueberzeugenden wahlsieg ltwsaar22 saarland\n",
      "saarlaenderinnen    1\n",
      "saarlaender         1\n",
      "klar                1\n",
      "wechsel             1\n",
      "spitze              1\n",
      "landes              1\n",
      "entschieden         1\n",
      "sicher              1\n",
      "gestalten           1\n",
      "du                  1\n",
      "herzlichen          1\n",
      "glueckwunsch        1\n",
      "ueberzeugenden      1\n",
      "wahlsieg            1\n",
      "ltwsaar22           1\n",
      "saarland            1\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#proving, that all words in the tweet are marked appropriatly\n",
    "print(longDf.iloc[0][\"editedInput\"])\n",
    "print(longDf.iloc[0][longDf.iloc[0][\"editedInput\"].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7588806a-1f6d-456a-bc65-ca4cf2f3bdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweetidInput', 'datetimeInput', 'dateInput', 'timeInput', 'userInput',\n",
      "       'textInput', 'languageInput', 'editedInput', 'textforttbInput',\n",
      "       'lematizedInput', 'AnalysisInput', 'word_countInput', 'WutInput',\n",
      "       'VorfreudeInput', 'EkelInput', 'FurchtInput', 'FreudeInput',\n",
      "       'TraurigkeitInput', 'UeberraschungInput', 'VertrauenInput',\n",
      "       'neg_emotionsInput', 'pos_emotionsInput', 'total_neg_emotionsInput',\n",
      "       'total_pos_emotionsInput', '000', '0230', '0800016'],\n",
      "      dtype='object')\n",
      "Index(['tweetidInput', 'datetimeInput', 'dateInput', 'timeInput', 'userInput',\n",
      "       'textInput', 'languageInput', 'editedInput', 'textforttbInput',\n",
      "       'lematizedInput', 'AnalysisInput', 'word_countInput', 'WutInput',\n",
      "       'VorfreudeInput', 'EkelInput', 'FurchtInput', 'FreudeInput',\n",
      "       'TraurigkeitInput', 'UeberraschungInput', 'VertrauenInput',\n",
      "       'neg_emotionsInput', 'pos_emotionsInput', 'total_neg_emotionsInput',\n",
      "       'total_pos_emotionsInput'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(longDf.keys()[:27])\n",
    "print(longDf.keys()[:24])\n",
    "#Shows that keys[0:24] are tweet data, thats not part of the NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c2c66b7-8602-44b4-b666-a22af7e9c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is already part of old code\n",
    "import re\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+\\s?', '', text) #Removed Mentions\n",
    "    text = re.sub(r'#', '', text) #Removed #\n",
    "    text = re.sub(r'(.)1+', r'1', text) #cleaned single letters\n",
    "    text = re.sub('((www.[^s]+)|(https?://[^s]+))','',text) #Removes links\n",
    "    text = re.sub('@','',text) #Remove @\n",
    "    text = re.sub('-','',text) #Remove -\n",
    "    text = re.sub('ä','ae',text) #Remove ä\n",
    "    text = re.sub('Ä','Ae',text) #Remove Ä\n",
    "    text = re.sub('ö','oe',text) #Remove Ä\n",
    "    text = re.sub('Ö','Oe',text) #Remove Ä\n",
    "    text = re.sub('ü','ue',text) #Remove Ä\n",
    "    text = re.sub('Ü','Ue',text) #Remove Ä\n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8493cfda-0051-4df2-8cdc-c037ab39bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_toVec(sentence,goalDF):\n",
    "    #Preparing Input text\n",
    "    cleanSentence = sentence.lower()\n",
    "    cleanSentence = clean(cleanSentence)\n",
    "    cleansentene = stemming_on_text(cleanSentence)\n",
    "    \n",
    "    #Preparing EmptyDF\n",
    "    emptyList = [0]*len(goalDF.keys())\n",
    "    #print(emptyList)\n",
    "    #print(\"Len EmptyList: \" + str(len(emptyList)))\n",
    "    #emptyDF = goalDF.iloc[0:0].copy()\n",
    "    #emptyDF = emptyDF.append(emptyList)\n",
    "    #emptyDF = emptyDF.append(pd.DataFrame(emptyList, columns = goalDF.keys()), ignore_index = True)\n",
    "    emptyDF = pd.DataFrame(columns = goalDF.keys())\n",
    "    #emptyDF.append(pd.Series(), ignore_index = True)\n",
    "    emptyDF.loc[len(emptyDF)] = emptyList\n",
    "    #print(emptyDF)\n",
    "    #emptyDF = pd.DataFrame(emptyList, columns = goalDF.keys())\n",
    "    #print(\"LenEmpty DF: \" + str(len(emptyDF)) + \" Len Keys Empty DF: \" + str(len(emptyDF.keys())))\n",
    "    \n",
    "    emptyDF[\"textInput\"][0] = sentence\n",
    "    emptyDF[\"editedInput\"][0] = cleanSentence\n",
    "    #keys = list(emptyDF.keys()[0:10])\n",
    "    #print(\"---\")\n",
    "    #print(\"New LenEmpty DF: \" + str(len(emptyDF)) + \" Len Keys Empty DF: \" + str(len(emptyDF.keys())))\n",
    "    #print(emptyDF[\"textInput\"].tolist())\n",
    "    listOfWords = cleanSentence.split()\n",
    "    for word in listOfWords:\n",
    "        if word in emptyDF.columns:\n",
    "            emptyDF[word][0] = 1\n",
    "    emptyDF.fillna(0)\n",
    "    return emptyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf21197d-dffb-4e86-8c7c-103d2f51c079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000\n"
     ]
    }
   ],
   "source": [
    "myPrivateSentence = \"Wir sind falsch\"    \n",
    "print(longDf.keys()[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9570038d-f028-4283-805b-1dd96c5c2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "#print(list((keys)))\n",
    "def trainEmotionModels(dataFrame):\n",
    "    tweet_features = longDf.keys()[24:]\n",
    "    X_trainingData = longDf[tweet_features]\n",
    "    keys = list(longDf.keys()[12:20])\n",
    "    listOfModels = []\n",
    "    iterator = 0\n",
    "    for emotion in longDf[keys]:\n",
    "        print(emotion[:-5] + \": \")\n",
    "        #print(longDf[emotion].head())\n",
    "        y_emotion = longDf[emotion]\n",
    "        train_X, val_X, train_y, val_y = train_test_split(X_trainingData, y_emotion,test_size = 0.33, random_state = 1,shuffle=True)\n",
    "        emotion_Model = DecisionTreeRegressor(random_state = 1)\n",
    "        emotion_Model.fit(X_trainingData,y_emotion)\n",
    "        val_predictions = emotion_Model.predict(val_X)\n",
    "        listOfModels.append(emotion_Model)\n",
    "        #print(emotion_Model.predict(val_X.head()))\n",
    "        \n",
    "        print(\"Mean Absolute error for {}: \".format(emotion[:-5]) + str(round(mean_absolute_error(val_y,val_predictions)*100,2))+\"%\\n\")\n",
    "        #print(\"type(train_X): \" + str(type(train_X)))\n",
    "        #print(\"type(dfSentence): \" + str(type(dfOfSentence)))\n",
    "        #print(\"Predicition for for train_X.head() for emotion {}: \".format(emotion[:-5])+ str(emotion_Model.predict(train_X.head())))\n",
    "        #print(\"Predicition for '{}' for emotion {}: \".format(myPrivateSentence,emotion[:-5])+ str(emotion_Model.predict(dfOfSentence[tweet_features])))\n",
    "        #y_emotionGoal = longDf[emotion]\n",
    "        \n",
    "        \n",
    "    dicOfModels = {\"Wut-Model\":listOfModels[0],\"Vorfreude-Model\":listOfModels[1],\"Ekel-Model\":listOfModels[2],\"Furcht-Model\":listOfModels[3],\"Freude-Model\":listOfModels[4],\"Traurigkeit-Model\":listOfModels[5], \"Ueberraschungs-Model\":listOfModels[6],\"Vetrauen-Model\":listOfModels[7]}\n",
    "    return dicOfModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccc99472-e31b-43a6-b51e-3841e71ede9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAttributesOfTweet(tweetNumber, df):\n",
    "    keys = list(df.keys()[12:20])\n",
    "    print(\"---\\nTweet \\n--- \\n{}\\n--- \\nhas these Emotions according to DataInput Tweet:\")\n",
    "    for emotion in df[keys]:\n",
    "        print(str(emotion[:-5]) + \"-Value: \" +str(df[emotion][tweetNumber]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ee4c987-f9f8-43b9-a81b-978d272a1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretOwnSentence(sentence, dicOfModels, df):\n",
    "    if(type(sentence) != str):\n",
    "        print(\"Sentence-Parameter is not a String\")\n",
    "        return\n",
    "    if(type(dicOfModels) != dict):\n",
    "        print(\"dicOfModels-Parameter is not a dictionary\")\n",
    "        return\n",
    "    if(len(dicOfModels)!=8):\n",
    "        print(\"dicOfModels Length is not 8\")\n",
    "        return\n",
    "        \n",
    "    #Transforming Sentence into Vector\n",
    "    dfOfSentence = sentence_toVec(myPrivateSentence, df)\n",
    "    #print(\"----\\nThis is the shape of the df: \\n\" + str(dfOfSentence) + \"\\n----\")\n",
    "    listOfIndex = dfOfSentence.columns[(dfOfSentence == 1).all()].tolist()\n",
    "    listOfIndex.extend([\"textInput\",\"editedInput\",\"WutInput\"])\n",
    "    #print(\"----\\nInput Sentence was transformed to this vector: \\n\" +str(dfOfSentence.head()))\n",
    "    \n",
    "    \n",
    "    keys = list(df.keys()[12:20])\n",
    "    #Applying different Models to sentence\n",
    "    keyOfFeatures = dfOfSentence.keys()[24:]\n",
    "    features = dfOfSentence[keyOfFeatures]\n",
    "    iterator = 0\n",
    "    for model in dicOfModels:\n",
    "        #print(str(model) + \" is the following type: \" + str(type(model)))\n",
    "        result = dicOfModels[model].predict(features)\n",
    "        #print(\"---\\nThis is the result of {}: \".format(str(model)) + str(result))\n",
    "        #print(\"result[0]: \" + str(result[0]))\n",
    "        print(\"before writing: \" + str(dfOfSentence[dfOfSentence.keys()[12+iterator]]))\n",
    "        dfOfSentence[12+iterator] = result[0]\n",
    "        print(\"---\\nThis is in the dfOfSentence: \" + str(dfOfSentence[12+iterator][0]))\n",
    "        iterator = iterator + 1\n",
    "    print(\"---\\nThis is what all the Attributes look like: \\n\" + str(dfOfSentence[dfOfSentence.keys()[12:20]]))\n",
    "    getAttributesOfTweet(0,dfOfSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4bde8-4767-4ffd-b183-306d2420608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "giveMeMyModels = trainEmotionModels(longDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6d13c536-c0a0-4b0d-90be-1e18b725d976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before writing: 0    0\n",
      "Name: WutInput, dtype: object\n",
      "---\n",
      "This is in the dfOfSentence: 1.0\n",
      "before writing: 0    0\n",
      "Name: VorfreudeInput, dtype: object\n",
      "---\n",
      "This is in the dfOfSentence: 0.0\n",
      "before writing: 0    0\n",
      "Name: EkelInput, dtype: object\n",
      "---\n",
      "This is in the dfOfSentence: 1.0\n",
      "before writing: 0    0\n",
      "Name: FurchtInput, dtype: object\n",
      "---\n",
      "This is in the dfOfSentence: 0.0\n",
      "before writing: 0    0\n",
      "Name: FreudeInput, dtype: object\n",
      "---\n",
      "This is in the dfOfSentence: 0.0\n",
      "before writing: 0    0\n",
      "Name: TraurigkeitInput, dtype: object\n",
      "---\n",
      "This is in the dfOfSentence: 0.0\n",
      "before writing: 0    0\n",
      "Name: UeberraschungInput, dtype: object\n",
      "---\n",
      "This is in the dfOfSentence: 0.0\n",
      "before writing: 0    0\n",
      "Name: VertrauenInput, dtype: object\n",
      "---\n",
      "This is in the dfOfSentence: 0.0\n",
      "---\n",
      "This is what all the Attributes look like: \n",
      "  WutInput VorfreudeInput EkelInput FurchtInput FreudeInput TraurigkeitInput  \\\n",
      "0        0              0         0           0           0                0   \n",
      "\n",
      "  UeberraschungInput VertrauenInput  \n",
      "0                  0              0  \n",
      "---\n",
      "Tweet \n",
      "--- \n",
      "{}\n",
      "--- \n",
      "has these Emotions according to DataInput Tweet:\n",
      "Wut-Value: 0\n",
      "Vorfreude-Value: 0\n",
      "Ekel-Value: 0\n",
      "Furcht-Value: 0\n",
      "Freude-Value: 0\n",
      "Traurigkeit-Value: 0\n",
      "Ueberraschung-Value: 0\n",
      "Vertrauen-Value: 0\n"
     ]
    }
   ],
   "source": [
    "interpretOwnSentence(\"Wir sind falsch\", giveMeMyModels, longDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a083f9-aaf7-467b-af1a-cc14872a8219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
