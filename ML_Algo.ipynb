{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7d28f-c15e-4f62-959d-c893b9231f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"EmotionCSV.csv\")\n",
    "#df2 = pd.read_csv(\"./Training_Dataframes/esken30ktweetswithemotions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55b663-8741-4886-8b77-e53efbdc949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(df))\n",
    "#print(df.head())\n",
    "df.drop([\"Unnamed: 0\"],axis = 1, inplace = True)\n",
    "#print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34847d55-9b27-4f59-993c-87cbbb94e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.head())\n",
    "print(df.keys())\n",
    "keyString = df.keys()\n",
    "keyString += \"Input\"\n",
    "    \n",
    "df.columns = keyString\n",
    "print(df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb650c-085c-4dd4-a3c5-45fd3c1ebb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d2c40-c348-467b-8c40-204f7a8c5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfTweets = df[\"editedInput\"].tolist()\n",
    "docs = np.array(listOfTweets)\n",
    "\n",
    "\"\"\"\n",
    "#Getting amount of individual words\n",
    "longListofWords = \" \".join(docs).split()\n",
    "#print(longListofWords[0])\n",
    "unique = set(longListofWords)\n",
    "countOfUniqueWords = len(unique)\n",
    "print(countOfUniqueWords)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b3619-d633-43bf-8b99-efcbd93829df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/swlh/tweet-sentiment-analysis-using-python-for-complete-beginners-4aeb4456040\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#dir(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f24c1-64d0-4e14-a7a9-ad94b8f4086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(binary = True)\n",
    "#help(cv.fit_transform)\n",
    "oneHotCv = cv.fit_transform(docs)\n",
    "#print(type(oneHotCv))\n",
    "#print(dir(oneHotCv))\n",
    "#print(cv.get_feature_names())\n",
    "oneHotArray = oneHotCv.toarray()\n",
    "oneHotArray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f2ad3-07b9-42a5-83b4-636fa90c2b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "numberZero = 0\n",
    "numberOne = 1\n",
    "print(\"Length of Tweet {} according to textforttb: \".format(numberZero)+str(len(df[\"edited\"][numberZero].split()))) \n",
    "print(\"Length of Tweet {} according to oneHotArray: \".format(numberZero)+str(sum(oneHotArray[numberZero])))\n",
    "print(max(oneHotArray[0]))\n",
    "print(df[\"edited\"][numberZero].split())\n",
    "print(oneHotArray[numberZero])\n",
    "# All Unique Words are represented in the oneHotArray. Only \"Und\" ist removed, which results in a length -1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a09d9-4736-4eb7-b9bc-ee51dda86548",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHotDf = pd.DataFrame(oneHotArray)\n",
    "oneHotDf.columns = cv.get_feature_names_out()\n",
    "print(oneHotDf.head())\n",
    "print(oneHotDf.info())\n",
    "#print(oneHotDf[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267fe82-a0bc-4d6b-9981-0735625570fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "longDF = pd.concat([df,oneHotDf], axis = 1)\n",
    "#print(type(longDF))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4dcb4e-3c7f-4edc-ab87-ae226a18ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.info())\n",
    "print(longDF.info())\n",
    "print(str(longDF.count()))\n",
    "#dir(longDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65065d-bbb2-4ca9-b3d3-e6a13dba3bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(longDF.iloc[0][\"editedInput\"])\n",
    "print(longDF.iloc[0][longDF.iloc[0][\"editedInput\"].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7588806a-1f6d-456a-bc65-ca4cf2f3bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(longDF.keys()[:27])\n",
    "print(longDF.keys()[:24])\n",
    "#Shows that keys[0:25] are tweet data, thats not part of the NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c66b7-8602-44b4-b666-a22af7e9c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is already part of old code\n",
    "import re\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+\\s?', '', text) #Removed Mentions\n",
    "    text = re.sub(r'#', '', text) #Removed #\n",
    "    text = re.sub(r'(.)1+', r'1', text) #cleaned single letters\n",
    "    text = re.sub('((www.[^s]+)|(https?://[^s]+))','',text) #Removes links\n",
    "    text = re.sub('@','',text) #Remove @\n",
    "    text = re.sub('-','',text) #Remove -\n",
    "    text = re.sub('ä','ae',text) #Remove ä\n",
    "    text = re.sub('Ä','Ae',text) #Remove Ä\n",
    "    text = re.sub('ö','oe',text) #Remove Ä\n",
    "    text = re.sub('Ö','Oe',text) #Remove Ä\n",
    "    text = re.sub('ü','ue',text) #Remove Ä\n",
    "    text = re.sub('Ü','Ue',text) #Remove Ä\n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493cfda-0051-4df2-8cdc-c037ab39bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_toVec(sentence,goalDF):\n",
    "    #Preparing Input text\n",
    "    cleanSentence = sentence.lower()\n",
    "    cleanSentence = clean(cleanSentence)\n",
    "    cleansentene = stemming_on_text(cleanSentence)\n",
    "    \n",
    "    #Preparing EmptyDF\n",
    "    emptyList = [0]*len(goalDF.keys())\n",
    "    #print(emptyList)\n",
    "    #print(\"Len EmptyList: \" + str(len(emptyList)))\n",
    "    #emptyDF = goalDF.iloc[0:0].copy()\n",
    "    #emptyDF = emptyDF.append(emptyList)\n",
    "    #emptyDF = emptyDF.append(pd.DataFrame(emptyList, columns = goalDF.keys()), ignore_index = True)\n",
    "    emptyDF = pd.DataFrame(columns = goalDF.keys())\n",
    "    #emptyDF.append(pd.Series(), ignore_index = True)\n",
    "    emptyDF.loc[len(emptyDF)] = emptyList\n",
    "    #print(emptyDF)\n",
    "    #emptyDF = pd.DataFrame(emptyList, columns = goalDF.keys())\n",
    "    print(\"LenEmpty DF: \" + str(len(emptyDF)) + \" Len Keys Empty DF: \" + str(len(emptyDF.keys())))\n",
    "    \n",
    "    emptyDF[\"textInput\"][0] = sentence\n",
    "    emptyDF[\"editedInput\"][0] = cleanSentence\n",
    "    #keys = list(emptyDF.keys()[0:10])\n",
    "    #print(\"---\")\n",
    "    print(\"New LenEmpty DF: \" + str(len(emptyDF)) + \" Len Keys Empty DF: \" + str(len(emptyDF.keys())))\n",
    "    #print(emptyDF[\"textInput\"].tolist())\n",
    "    listOfWords = cleanSentence.split()\n",
    "    for word in listOfWords:\n",
    "        if word in emptyDF.columns:\n",
    "            emptyDF[word][0] = 1\n",
    "    return emptyDF\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916bcb1-57fe-47ad-8a70-0ac2e7432d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "longDF.keys()[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21197d-dffb-4e86-8c7c-103d2f51c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOfSentence = sentence_toVec(\"Wir sind heute leider alle sehr sauer. Daher lasst uns alle feiern! Zusammen sind wir stark aber auch bischen wütend\", longDF)\n",
    "#print(dfOfSentence.loc[:,[\"textInput\",\"editedInput\",\"WutInput\"]])\n",
    "listOfIndex = dfOfSentence.columns[(dfOfSentence == 1).all()].tolist()\n",
    "#print(\"ListofIndex\" + str(listOfIndex))\n",
    "\n",
    "listOfIndex.extend([\"textInput\",\"editedInput\",\"WutInput\"])\n",
    "#print(\"This is a new List: \" + str(listOfIndex))\n",
    "print(dfOfSentence.loc[:,listOfIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9eafd-0715-438d-89d0-c121100cf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emotion_Model = DecisionTreeRegressor(random_state = 1)\n",
    "#help(emotion_Model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0358b0-12df-41eb-baea-cb4b9e3d9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_features = longDF.keys()[24:]\n",
    "\"\"\"print(df.keys())\n",
    "print(str(longDF[\"text\"].head())+\"\\n\")\n",
    "print(longDF[\"time\"].head())\n",
    "print(longDF[\"date\"].head())\n",
    "print(longDF.keys()[5:].tolist().index(\"time\")+1)\n",
    "#tweet_features = longDF.keys()[10620:10630]\n",
    "print(longDF[longDF.columns[10620:10630]])\"\"\"\n",
    "X_trainingData = longDF[tweet_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ba3fc-a916-429d-a78b-d4b0e488c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train_X, val_X, train_y, val_y = train_test_split(X_trainingData, y_emotionWut, random_state = 0)\n",
    "emotion_Model = DecisionTreeRegressor(random_state = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c4c2c-1ff1-47f6-b3a0-1cc25abd6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "#val_predictions = emotion_Model.predict(val_X)\n",
    "#print(emotion_Model.predict(val_X.head()))\n",
    "#print(mean_absolute_error(val_y,val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792eca8-e00c-4a97-ad1c-d92ba1ce9efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(train_test_split)\n",
    "#Parameters train_test_split(test_size,train_size,random_state,shuffle)\n",
    "#help(DecisionTreeRegressor)\n",
    "#dir(DecisionTreeRegressor)\n",
    "#Parameters DecisionTreeRegressor(criterion,splitter,max_depth,min_sample_split,min_sample_leaf,min_weight_fraction_leaf,max_features,random_state,max_leaf_nodes,min_impurity_decrease)\n",
    "# criterion : {\"squared_error\", \"friedman_mse\", \"absolute_error\",\"poisson\"}, default=\"squared_error\"\n",
    "#help(emotion_Model.score)\n",
    "#dir(longDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9570038d-f028-4283-805b-1dd96c5c2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(longDF.keys()[12:20])\n",
    "#print(list((keys)))\n",
    "for emotion in longDF[keys]:\n",
    "    print(emotion[:-5] + \": \")\n",
    "    #print(longDF[emotion].head())\n",
    "    y_emotion = longDF[emotion]\n",
    "    train_X, val_X, train_y, val_y = train_test_split(X_trainingData, y_emotion,test_size = 0.33, random_state = 1,shuffle=True)\n",
    "    emotion_Model = DecisionTreeRegressor(random_state = 1)\n",
    "    emotion_Model.fit(X_trainingData,y_emotion)\n",
    "    val_predictions = emotion_Model.predict(val_X)\n",
    "    #print(emotion_Model.predict(val_X.head()))\n",
    "    print(\"Mean Absolute error for {}: \".format(emotion[:-5]) + str(round(mean_absolute_error(val_y,val_predictions)*100,2))+\"%\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"type(train_X): \" + str(type(train_X)))\n",
    "    print(\"type(dfSentence): \" + str(type(dfOfSentence)))\n",
    "    \n",
    "    print(\"Predicition for 'Ich bin sehr Sauer' for train_X.head(): \".format(emotion[:-5])+ str(emotion_Model.predict(train_X.head())))\n",
    "    print(\"Predicition for 'Ich bin sehr Sauer' for emotion {}: \".format(emotion[:-5])+ str(emotion_Model.predict(dfOfSentence[tweet_features])))\n",
    "    #y_emotionGoal = longDF[emotion]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc99472-e31b-43a6-b51e-3841e71ede9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
