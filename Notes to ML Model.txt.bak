Limitations:
- This Model was trained on a Limited amount of Tweets and vocabulary of certain politicians. This leads to the Model being limited in the vocabulary and the context that the politician used the tweets in.
- As a further result of being trained on a limited amount of Tweets, certain words might have been used in a limited amount of context. An example for this is "herzlichen" which normaly would be associated positively, but is flagged as "Freude", "Traurig", "Vertrauen". This can result in certain posts having the emotion Sadness, although its a positive tweet
- Comparison of differen ML-Models is on a very limited Dataset of 1000 tweets due to lack of computation power. This results in the quality of the different model maybe varying when having a different size of data set
- This results in the Model best being used to analyse the tweet of the politician that the model was trained on. That offers a higher chance of the politician using the same vocabulary in a similiar context.

Further reaseach:
- Add further politicians for higher veriety and more data
- Training a model on normal text, to increase the vocabulary and context to allow higher quality of emotion recognition
- Improving the initial emotion recognition input for higher quality / more precise training dataset
- Training the model on bigger tweet dataset than 5000 Tweets
- Test if CART Model is realy the supperior ML-Model on a bigger Dataset than the currently implemented DTR Model


